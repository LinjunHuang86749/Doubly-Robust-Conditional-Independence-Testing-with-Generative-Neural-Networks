{"cells":[{"cell_type":"code","source":["# Uninstall the current version of scikit-learn\n","!pip uninstall -y scikit-learn\n","\n","# Install scikit-learn version 1.3.2\n","!pip install scikit-learn==1.3.2\n","\n","# Verify the installation\n","import sklearn\n","print(sklearn.__version__)"],"metadata":{"id":"bCCFjyfyxxSU","outputId":"628f0759-d2eb-4290-a32b-8a76084c2a6a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731957237867,"user_tz":360,"elapsed":6623,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: scikit-learn 1.5.2\n","Uninstalling scikit-learn-1.5.2:\n","  Successfully uninstalled scikit-learn-1.5.2\n","Collecting scikit-learn==1.3.2\n","  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (1.26.4)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (1.13.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (3.5.0)\n","Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-learn\n","Successfully installed scikit-learn-1.3.2\n","1.3.2\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4aVcP_s6-znC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731957252009,"user_tz":360,"elapsed":14145,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"}},"outputId":"be86a148-632b-4abf-f2f2-732175705893"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-333d46bccd83>:302: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.x, self.y = torch.load(filepath)\n","<ipython-input-2-333d46bccd83>:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.x, self.y = torch.load(filepath)\n","<ipython-input-2-333d46bccd83>:512: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  G_image.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_image.pth'))\n","<ipython-input-2-333d46bccd83>:516: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  G_label.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_label.pth'))\n","10000it [00:01, 5339.48it/s]\n"]}],"source":["latent_space_dim = 5\n","\n","from sklearn.decomposition import PCA\n","import torch\n","import torch.nn as nn\n","from torch.optim import SGD\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.distributions as TD\n","from zmq import device\n","import torch.optim as optim\n","from datetime import datetime\n","import functools\n","from tqdm import tqdm\n","\n","set_seed = 42\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# Utilites related to Sinkhorn computations and training for TensorFlow 2.0\n","import tensorflow as tf\n","import logging\n","import tensorflow_probability as tfp\n","from sklearn.metrics.pairwise import rbf_kernel\n","from scipy.stats import rankdata, ks_2samp, wilcoxon\n","from sklearn.model_selection import KFold\n","from datetime import datetime\n","import decimal\n","import torch\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","import gc # Garbage Collector\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","logging.getLogger('tensorflow').disabled = True\n","tf.keras.backend.set_floatx('float32')\n","\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","class CharacteristicFunction:\n","    '''\n","    class to construct a function that represents the characteristic function\n","    '''\n","\n","    def __init__(self, size, x_dims, z_dims, test_size):\n","        self.n_samples = size\n","        self.hidden_dims = 20 # default: 20\n","        self.test_size = test_size\n","\n","        self.input_dim = z_dims + x_dims\n","        self.z_dims = z_dims\n","        self.x_dims = x_dims\n","        self.input_shape1x = [self.x_dims, self.hidden_dims]\n","        self.input_shape1z = [self.z_dims, self.hidden_dims]\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, 1]\n","\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = tf.sqrt(2.0 / (input_shape[0]))\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def update(self):\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","\n","    def call(self, x, z):\n","        # inputs are concatenations of z and v\n","        x = tf.reshape(tensor=x, shape=[self.test_size, -1, self.x_dims])\n","        z = tf.reshape(tensor=z, shape=[self.test_size, -1, self.z_dims])\n","        # we asssume parameter b for z to be 0\n","        h1 = tf.nn.sigmoid(tf.matmul(x, self.w1x) + self.b1)\n","        out = tf.nn.sigmoid(tf.matmul(h1, self.w2))\n","        return out\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","#\n","# test statistics for DGCIT\n","#\n","\n","\n","def t_and_sigma(psy_x_i, psy_y_i, phi_x_i, phi_y_i):\n","    b, n = psy_x_i.shape\n","    x_mtx = phi_x_i - psy_x_i\n","    y_mtx = phi_y_i - psy_y_i\n","    matrix = tf.reshape(x_mtx[None, :, :] * y_mtx[:, None, :], [-1, n])\n","    t_b = tf.reduce_sum(matrix, axis=1) / tf.cast(n, tf.float64)\n","    t_b = tf.expand_dims(t_b, axis=1)\n","\n","    crit_matrix = matrix - t_b\n","    std_b = tf.sqrt(tf.reduce_sum(crit_matrix**2, axis=1) / tf.cast(n-1, tf.float64))\n","    return t_b, std_b\n","\n","\n","def test_statistics(psy_x_i, psy_y_i, phi_x_i, phi_y_i, t_b, std_b, j):\n","    b, n = psy_x_i.shape\n","    x_mtx = phi_x_i - psy_x_i\n","    y_mtx = phi_y_i - psy_y_i\n","    matrix = tf.reshape(x_mtx[None, :, :] * y_mtx[:, None, :], [-1, n])\n","    crit_matrix = matrix - t_b\n","    test_stat = tf.reduce_max(tf.abs(tf.sqrt(tf.cast(n, tf.float64)) * tf.squeeze(t_b) / std_b))\n","\n","    sig = tf.reduce_sum(crit_matrix[None, :, :] * crit_matrix[:, None, :], axis=2)\n","    coef = std_b[None, :] * std_b[:, None] * tf.cast(n-1, tf.float64)\n","    sig_xy = sig / coef\n","\n","    eigenvalues, eigenvectors = tf.linalg.eigh(sig_xy)\n","    base = tf.zeros_like(eigenvectors)\n","    eig_vals = tf.sqrt(eigenvalues + 1e-12)\n","    lamda = tf.linalg.set_diag(base, eig_vals)\n","    sig_sqrt = tf.matmul(tf.matmul(eigenvectors, lamda), tf.linalg.inv(eigenvectors))\n","\n","    z_dist = tfp.distributions.Normal(0.0, scale=1.0)\n","    z_samples = z_dist.sample([b*b, j])\n","    z_samples = tf.cast(z_samples, tf.float64)\n","    vals = tf.matmul(sig_sqrt, z_samples)\n","    t_j = tf.reduce_max(vals, axis=0)\n","    return test_stat, t_j\n","\n","#\n","# Training algorithm for DGCIT\n","#\n","\n","def dgcit(x, y, z, generator_x, generator_y, test_size=500, z_dim=100,\n","      x_dims=1, y_dims=1, M=100, k=1,\n","      b=30, j=1000):\n","\n","    noise_dimension_image = 50\n","    noise_dimension_label = 1\n","    input_noise_type = \"normal\"\n","\n","    psy_x_all = []\n","    phi_x_all = []\n","    psy_y_all = []\n","    phi_y_all = []\n","    test_samples = b\n","\n","    psy_x_b = []\n","    phi_x_b = []\n","    psy_y_b = []\n","    phi_y_b = []\n","\n","    x_samples = []\n","    y_samples = []\n","    z_input = []\n","    x_input = []\n","    y_input = []\n","\n","    test_xyz = DatasetSelect_GAN(x, y, z, 1)\n","    testing_dataset = torch.utils.data.DataLoader(test_xyz, batch_size=1, shuffle=False)\n","\n","    G_image = generator_x.eval()\n","    G_label = generator_y.eval()\n","\n","    for test_x, test_y, test_z, Z_fake in testing_dataset:\n","\n","        Z_test_repeat = test_z.repeat(M,1).to(device).detach()\n","\n","        # Generate fake data\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension_label, input_noise_type, input_var = 1.0/3.0).to(device)\n","        with torch.no_grad():\n","            gen_y = G_label(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device).detach()\n","\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension_image, input_noise_type, input_var = 1.0/3.0).to(device)\n","        with torch.no_grad():\n","            gen_x = G_image(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device).detach()\n","\n","        gen_x_all = gen_x.reshape(M, x_dims).cpu().detach().numpy()\n","        gen_y_all = gen_y.reshape(M, y_dims).cpu().detach().numpy()\n","\n","        fake_x = tf.convert_to_tensor(gen_x_all)\n","        fake_y = tf.convert_to_tensor(gen_y_all)\n","\n","        test_z = tf.convert_to_tensor(test_z.cpu().detach().numpy())\n","        test_x = tf.convert_to_tensor(test_x.cpu().detach().numpy())\n","        test_y = tf.convert_to_tensor(test_y.cpu().detach().numpy())\n","\n","        test_z = tf.reshape(test_z, (1, z_dim))\n","        test_y = tf.reshape(test_y, (1, y_dims))\n","        test_x = tf.reshape(test_x, (1, x_dims))\n","\n","        fake_x = tf.cast(fake_x, tf.float64)\n","        fake_y = tf.cast(fake_y, tf.float64)\n","        test_z = tf.cast(test_z, tf.float64)\n","        test_x = tf.cast(test_x, tf.float64)\n","        test_y = tf.cast(test_y, tf.float64)\n","\n","        x_samples.append(fake_x)\n","        y_samples.append(fake_y)\n","        z_input.append(test_z)\n","        x_input.append(test_x)\n","        y_input.append(test_y)\n","\n","    # give the five variables: x_samples, y_samples, z_input, x_input, y_input\n","    # they are lists with length = test_size\n","    # x_samples: [x_sampled1, x_sampled2, ... , x_sampled_test_size] x_sampled1 has shape [M, dx]\n","    # y_samples: [y_sampled1, y_sampled2, ... , y_sampled_test_size] y_sampled1 has shape [M, dy]\n","    # z_input = [z1, z2, ... , z_test_size]\n","    # x_input = [x1, x2, ... , x_test_size]\n","    # y_input = [y1, y2, ... , y_test_size]\n","\n","\n","    standardise = True\n","\n","    if standardise:\n","        x_samples = (x_samples - tf.reduce_mean(x_samples)) / tf.math.reduce_std(x_samples)\n","        y_samples = (y_samples - tf.reduce_mean(y_samples)) / tf.math.reduce_std(y_samples)\n","        x_input = (x_input - tf.reduce_mean(x_input)) / tf.math.reduce_std(x_input)\n","        y_input = (y_input - tf.reduce_mean(y_input)) / tf.math.reduce_std(y_input)\n","        z_input = (z_input - tf.reduce_mean(z_input)) / tf.math.reduce_std(z_input)\n","\n","    f1 = CharacteristicFunction(M, x_dims, z_dim, test_size)\n","    f2 = CharacteristicFunction(M, y_dims, z_dim, test_size)\n","    for i in range(test_samples):\n","        phi_x = tf.reduce_mean(f1.call(x_samples, z_input), axis=1)\n","        phi_y = tf.reduce_mean(f2.call(y_samples, z_input), axis=1)\n","        psy_x = tf.squeeze(f1.call(x_input, z_input))\n","        psy_y = tf.squeeze(f2.call(y_input, z_input))\n","\n","        psy_x_b.append(psy_x)\n","        phi_x_b.append(phi_x)\n","        psy_y_b.append(psy_y)\n","        phi_y_b.append(phi_y)\n","        f1.update()\n","        f2.update()\n","\n","    psy_x_all.append(psy_x_b)\n","    phi_x_all.append(phi_x_b)\n","    psy_y_all.append(psy_y_b)\n","    phi_y_all.append(phi_y_b)\n","\n","    # reshape\n","    psy_x_all = tf.reshape(psy_x_all, [k, test_samples, test_size])\n","    psy_y_all = tf.reshape(psy_y_all, [k, test_samples, test_size])\n","    phi_x_all = tf.reshape(phi_x_all, [k, test_samples, test_size])\n","    phi_y_all = tf.reshape(phi_y_all, [k, test_samples, test_size])\n","\n","    t_b = 0.0\n","    std_b = 0.0\n","    for n in range(k):\n","        t, std = t_and_sigma(psy_x_all[n], psy_y_all[n], phi_x_all[n], phi_y_all[n])\n","        t_b += t\n","        std_b += std\n","    t_b = t_b / tf.cast(k, tf.float64)\n","    std_b = std_b / tf.cast(k, tf.float64)\n","\n","    psy_x_all = tf.transpose(psy_x_all, (1, 0, 2))\n","    psy_y_all = tf.transpose(psy_y_all, (1, 0, 2))\n","    phi_x_all = tf.transpose(phi_x_all, (1, 0, 2))\n","    phi_y_all = tf.transpose(phi_y_all, (1, 0, 2))\n","\n","    psy_x_all = tf.reshape(psy_x_all, [test_samples, test_size*k])\n","    psy_y_all = tf.reshape(psy_y_all, [test_samples, test_size*k])\n","    phi_x_all = tf.reshape(phi_x_all, [test_samples, test_size*k])\n","    phi_y_all = tf.reshape(phi_y_all, [test_samples, test_size*k])\n","\n","    stat, critical_vals = test_statistics(psy_x_all, psy_y_all, phi_x_all, phi_y_all, t_b, std_b, j)\n","    comparison = [c > stat or c == stat for c in critical_vals]\n","    comparison = np.reshape(comparison, (-1,))\n","    p_value = np.sum(comparison.astype(np.float32)) / j\n","\n","    return p_value\n","\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","import torch.nn as nn\n","\n","PCA_model = PCA(n_components=latent_space_dim)\n","class CTDataset(Dataset):\n","    def __init__(self, filepath):\n","        self.x, self.y = torch.load(filepath)\n","        self.x = self.x / 255.\n","        self.x = self.x.reshape(-1, 28*28).cuda().detach().cpu()\n","        self.x = torch.from_numpy(PCA_model.fit_transform(self.x)).to(device).float()\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","\n","class CTDataset_all(Dataset):\n","    def __init__(self, filepath):\n","        self.flatten = nn.Flatten()\n","        self.x, self.y = torch.load(filepath)\n","        self.x = self.x / 255.\n","        self.z = self.flatten(self.x)\n","        self.x = self.x.reshape(-1, 28*28).cuda().detach().cpu()\n","        self.x = torch.from_numpy(PCA_model.transform(self.x)).to(device).float()\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","        # self.y = self.y.to(float)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix], self.z[ix]\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :28, :28]\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","    def __init__(self, X, Y, Z, batch_size):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.batch_size = batch_size\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[(self.batch_size+index) % self.sample_size]\n","\n","class Generator_image(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, noise_dimension):\n","      super(Generator_image, self).__init__()\n","      self.flatten = nn.Flatten()\n","      self.decoder = nn.Sequential(\n","              torch.nn.Linear(input_dimension + noise_dimension, 3136),\n","              Reshape(-1, 64, 7, 7),\n","              nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0),\n","              Trim(),  # 1x29x29 -> 1x28x28\n","              nn.Sigmoid()\n","              )\n","\n","    def forward(self, x):\n","      x = self.decoder(x)\n","      x = self.flatten(x)# 1x28x28 -> 1x784\n","      return x\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                 drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","      self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","        x = self.softmax(x)\n","      else:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","        # x = self.sigmoid(x)\n","        x = self.softmax(x)\n","\n","      return x\n","\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","\n","noise_dimension_image = 50\n","noise_dimension_label = 1\n","input_noise_type = \"normal\"\n","\n","torch.manual_seed(42)\n","train_ds = CTDataset('./training.pt')\n","\n","torch.manual_seed(42)\n","train_AE_set, train_cond_gen_set = torch.utils.data.random_split(train_ds, [30000, 30000])\n","train_ds = train_cond_gen_set\n","DataLoader_train = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, drop_last= False)\n","\n","xs, ys = train_ds[0:10000]\n","\n","torch.manual_seed(42)\n","\n","test_ds = CTDataset_all('./test.pt')\n","\n","DataLoader_test = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=True, drop_last= False, )\n","\n","G_image = Generator_image(latent_space_dim,  noise_dimension_image).to(device)\n","G_image.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_image.pth'))\n","\n","G_label = Generator(input_dimension = latent_space_dim, output_dimension = 10, noise_dimension = noise_dimension_label,\n","           hidden_layer_size = 512, BN_type = True, ReLU_coef = 0.5, drop_out_p= 0.2).to(device)\n","G_label.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_label.pth'))\n","\n","M = 100\n","test_size = 10000\n","Total_num_p_val = 40\n","\n","z_all = torch.zeros(test_size, latent_space_dim)\n","x_all = torch.zeros(test_size, 28*28)\n","y_all = torch.zeros(test_size, 10)\n","\n","\n","for i, (z_test, y_test, x_test) in tqdm(enumerate(DataLoader_test)):\n","    x_all[i,:] = x_test\n","    y_all[i,:] = y_test\n","    z_all[i,:] = z_test"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UKatisIC1Tzv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731957419259,"user_tz":360,"elapsed":167257,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"}},"outputId":"a5c26698-994e-4e17-a817-b97fed59ebf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["the  0  has p value:  0.0\n","the  250  has p value:  0.001\n","the  500  has p value:  0.005\n","the  750  has p value:  0.007\n","the  1000  has p value:  0.0\n","the  1250  has p value:  0.002\n","the  1500  has p value:  0.0\n","the  1750  has p value:  0.001\n","the  2000  has p value:  0.0\n","the  2250  has p value:  0.0\n","the  2500  has p value:  0.005\n","the  2750  has p value:  0.0\n","the  3000  has p value:  0.0\n","the  3250  has p value:  0.0\n","the  3500  has p value:  0.0\n","the  3750  has p value:  0.0\n","the  4000  has p value:  0.0\n","the  4250  has p value:  0.0\n","the  4500  has p value:  0.005\n","the  4750  has p value:  0.0\n","the  5000  has p value:  0.0\n","the  5250  has p value:  0.0\n","the  5500  has p value:  0.0\n","the  5750  has p value:  0.0\n","the  6000  has p value:  0.005\n","the  6250  has p value:  0.0\n","the  6500  has p value:  0.0\n","the  6750  has p value:  0.0\n","the  7000  has p value:  0.001\n","the  7250  has p value:  0.0\n","the  7500  has p value:  0.001\n","the  7750  has p value:  0.0\n","the  8000  has p value:  0.007\n","the  8250  has p value:  0.001\n","the  8500  has p value:  0.001\n","the  8750  has p value:  0.0\n","the  9000  has p value:  0.003\n","the  9250  has p value:  0.0\n","the  9500  has p value:  0.0\n","the  9750  has p value:  0.0\n"]}],"source":["n_length_input = int(test_size/Total_num_p_val)\n","p_val_list = []\n","\n","for i in range(0, Total_num_p_val):\n","    n_length = n_length_input\n","    start_index = n_length_input*(i)\n","    end_index = start_index + n_length\n","\n","    x_all_in = x_all[start_index:end_index,].to(device).detach()\n","    y_all_in = y_all[start_index:end_index,].to(device).detach()\n","    z_all_in = z_all[start_index:end_index,].to(device).detach()\n","\n","    p_val = dgcit(x_all_in, y_all_in, z_all_in, G_image, G_label, test_size=n_length_input, z_dim=latent_space_dim,\n","      x_dims=28*28, y_dims=10, M=100, k=1, b=50, j=1000)\n","\n","    print(\"the \",start_index,\" has p value: \",p_val)\n","    p_val_list.append(p_val)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zNBzifPfFRpw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731957419260,"user_tz":360,"elapsed":6,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"}},"outputId":"4d57194d-c479-4b10-b5e5-d343ad16dcaa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0,\n"," 0.001,\n"," 0.005,\n"," 0.007,\n"," 0.0,\n"," 0.002,\n"," 0.0,\n"," 0.001,\n"," 0.0,\n"," 0.0,\n"," 0.005,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.005,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.005,\n"," 0.0,\n"," 0.0,\n"," 0.0,\n"," 0.001,\n"," 0.0,\n"," 0.001,\n"," 0.0,\n"," 0.007,\n"," 0.001,\n"," 0.001,\n"," 0.0,\n"," 0.003,\n"," 0.0,\n"," 0.0,\n"," 0.0]"]},"metadata":{},"execution_count":4}],"source":["p_val_list"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PLv1oyip7u-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731957419260,"user_tz":360,"elapsed":5,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"}},"outputId":"416b8f03-2bbe-435d-8c1e-58fd6a76c59c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.0, 0.0, 0.001)"]},"metadata":{},"execution_count":5}],"source":["np.quantile(p_val_list, 0.25), np.median(p_val_list), np.quantile(p_val_list, 0.75)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"JQYtNh0fDZlW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731957419260,"user_tz":360,"elapsed":4,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"}},"outputId":"e189f589-b382-4558-81e7-bb187883cef4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":6}],"source":["np.mean([p_val < 0.05 for p_val in p_val_list])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}