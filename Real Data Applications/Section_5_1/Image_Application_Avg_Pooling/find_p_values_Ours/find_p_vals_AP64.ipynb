{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78222,"status":"ok","timestamp":1717329441805,"user":{"displayName":"Linjun","userId":"13995434670626730968"},"user_tz":300},"id":"4aVcP_s6-znC","outputId":"8e9b937c-7fb0-45e8-d954-fdb76114ab16"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [00:34<00:00, 289.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[sigma_w_train 8.039461135864258]\n"]},{"name":"stderr","output_type":"stream","text":["10000it [00:38, 262.91it/s]\n"]}],"source":["latent_space_dim = 64\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import SGD\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.distributions as TD\n","from zmq import device\n","import torch.optim as optim\n","from datetime import datetime\n","import functools\n","from tqdm import tqdm\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","import torch.nn as nn\n","\n","class avg_pooling_cov_net(nn.Module):\n","    def __init__(self, dl = 100):\n","        super(avg_pooling_cov_net, self).__init__()\n","        if dl == 100:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((10, 10))\n","            )\n","        if dl == 81:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((9, 9))\n","            )\n","        if dl == 64:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((8, 8))\n","            )\n","        if dl == 49:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((7, 7)))\n","        if dl == 36:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((6, 6)))\n","        if dl == 25:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((5, 5)))\n","        if dl == 16:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((4, 4)))\n","        if dl == 9:\n","            self.layer1 = nn.Sequential(\n","                nn.AdaptiveAvgPool2d((3, 3)))\n","        self.flatten = nn.Flatten()\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.flatten(out)\n","        return out\n","class CTDataset_all(Dataset):\n","    def __init__(self, filepath, avg_pooling_model):\n","        self.flatten = nn.Flatten()\n","        self.x, self.y = torch.load(filepath)\n","        self.x = self.x / 255.\n","        self.z = self.flatten(self.x)\n","        avg_pooling_model.eval()\n","        with torch.no_grad():\n","            self.x = avg_pooling_model(self.x)\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix], self.z[ix]\n","\n","class CTDataset(Dataset):\n","    def __init__(self, filepath, avg_pooling_model):\n","        self.x, self.y = torch.load(filepath)\n","        self.x = self.x / 255.\n","        avg_pooling_model.eval()\n","        with torch.no_grad():\n","            self.x = avg_pooling_model(self.x)\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","        # self.y = self.y.to(float)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :28, :28]\n","\n","\n","class Generator_image(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, noise_dimension):\n","      super(Generator_image, self).__init__()\n","      self.flatten = nn.Flatten()\n","      self.decoder = nn.Sequential(\n","              torch.nn.Linear(input_dimension + noise_dimension, 3136),\n","              Reshape(-1, 64, 7, 7),\n","              nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0),\n","              Trim(),  # 1x29x29 -> 1x28x28\n","              nn.Sigmoid()\n","              )\n","\n","    def forward(self, x):\n","      x = self.decoder(x)\n","      x = self.flatten(x)# 1x28x28 -> 1x784\n","      return x\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                 drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","      self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","        x = self.softmax(x)\n","      else:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","        # x = self.sigmoid(x)\n","        x = self.softmax(x)\n","\n","      return x\n","\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u=1,\n","            sigma_v=1, boor_rv_type=\"gaussian\", dy_g=10, dx_g = 28*28):\n","\n","    w_mx = torch.zeros(n, n).to(device)\n","\n","    for i in range(n):\n","        w_mx[i,:] = torch.linalg.vector_norm(z_torch[i].reshape(1,-1) - z_torch, ord = 1, dim = 1)\n","\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_temp = torch.zeros(n, n).to(device)\n","\n","    for i in range(n):\n","        u_mx_temp[i,:] = torch.linalg.vector_norm(y_torch[i].reshape(1,-1) - y_torch, ord = 1, dim = 1)\n","\n","    u_mx_1 = torch.exp(-u_mx_temp / sigma_u)\n","\n","    u_mx_temp_2 = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            u_mx_temp_2[i,j,:] = torch.linalg.vector_norm(y_torch[i].reshape(1,-1) - gen_y_all_torch[j,], ord = 1, dim = 1)\n","\n","    u_mx_2 = torch.mean( torch.exp(-u_mx_temp_2 / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    sum_mx_temp = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            sum_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_y_all_torch[j,:,:].reshape(1,M,dy_g) - gen_y_all_torch[i,0,:].reshape(1,1,dy_g), ord = 1, dim = 2)\n","\n","    sum_mx = torch.mean(torch.exp(-sum_mx_temp/ sigma_u), dim=2)\n","\n","    v_mx_temp = torch.zeros(n, n).to(device)\n","\n","    for i in range(n):\n","        v_mx_temp[i,:] = torch.linalg.vector_norm(x_torch[i].reshape(1,-1) - x_torch, ord = 1, dim = 1)\n","\n","    v_mx_1 = torch.exp(-v_mx_temp / sigma_v)\n","\n","    v_mx_temp_2 = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            v_mx_temp_2[i,j,:] = torch.linalg.vector_norm(x_torch[i].reshape(1,-1) - gen_x_all_torch[j,], ord = 1, dim = 1)\n","\n","    v_mx_2 = torch.mean( torch.exp(-v_mx_temp_2 / sigma_v), dim=2)\n","    v_mx_3 = v_mx_2.T\n","\n","    sum2_mx_temp = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            sum2_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_x_all_torch[j,:,:].reshape(1,M,dx_g) - gen_x_all_torch[i,0,:].reshape(1,1,dx_g), ord = 1, dim = 2)\n","\n","    sum2_mx = torch.mean(torch.exp(-sum2_mx_temp/ sigma_v), dim=2)\n","\n","    for k in tqdm(range(1, M)):\n","        sum_mx_temp = torch.zeros(n, n, M).to(device)\n","        sum2_mx_temp = torch.zeros(n, n, M).to(device)\n","        for i in range(n):\n","            for j in range(n):\n","                sum_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_y_all_torch[j,:,:].reshape(1,M,dy_g) - gen_y_all_torch[i,k,:].reshape(1,1,dy_g), ord = 1, dim = 2)\n","                sum2_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_x_all_torch[j,:,:].reshape(1,M,dx_g) - gen_x_all_torch[i,k,:].reshape(1,1,dx_g), ord = 1, dim = 2)\n","\n","        temp_add_mx = torch.mean(torch.exp(-sum_mx_temp/ sigma_u), dim=2)\n","        temp2_add_mx = torch.mean(torch.exp(-sum2_mx_temp/ sigma_v), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","        sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1 / M * sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    stat = 1 / (n - 1) * torch.sum(FF_mx).item()\n","\n","    boottemp = np.array([])\n","    torch.manual_seed(42)\n","    if boor_rv_type == \"rademacher\":\n","        eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","        eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","        random_mx = torch.matmul(eboot[:, bb].reshape(-1, 1), eboot[:, bb].reshape(-1, 1).T)\n","        bootmatrix = FF_mx * random_mx\n","        stat_boot = 1 / (n - 1) * torch.sum(bootmatrix).item()\n","        boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","noise_dimension_image = 1\n","noise_dimension_label = 1\n","input_noise_type = \"normal\"\n","avg_pooling_model = avg_pooling_cov_net(dl = latent_space_dim).to(device)\n","torch.manual_seed(42)\n","train_ds = CTDataset('./training.pt', avg_pooling_model)\n","\n","torch.manual_seed(42)\n","train_AE_set, train_cond_gen_set = torch.utils.data.random_split(train_ds, [30000, 30000])\n","train_ds = train_cond_gen_set\n","DataLoader_train = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, drop_last= False)\n","\n","xs, ys = train_ds[0:10000]\n","pairwise_distance_x = torch.zeros(xs.shape[0], xs.shape[0])\n","\n","for i in tqdm(range(xs.shape[0])):\n","    pairwise_distance_x[i,:] = torch.linalg.vector_norm(xs[i].reshape(1,-1) - xs, ord = 1, dim = 1)\n","\n","sigma_w_train = torch.median(pairwise_distance_x).item()\n","print(f\"[sigma_w_train {sigma_w_train}]\")\n","\n","\n","sigma_w_train = sigma_w_train # for z\n","sigma_v_train = 130.29019165039062 # for x\n","sigma_u_train = 2.0 # for y\n","\n","torch.manual_seed(42)\n","\n","test_ds = CTDataset_all('./test.pt', avg_pooling_model)\n","\n","DataLoader_test = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=True, drop_last= False, )\n","\n","G_image = Generator_image(latent_space_dim,  noise_dimension_image).to(device)\n","G_image.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_image.pth'))\n","\n","G_label = Generator(input_dimension = latent_space_dim, output_dimension = 10, noise_dimension = noise_dimension_label,\n","           hidden_layer_size = 512, BN_type = True, ReLU_coef = 0.5, drop_out_p= 0.2).to(device)\n","G_label.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_label.pth'))\n","\n","M = 100\n","test_size = 10000\n","Total_num_p_val = 40\n","\n","gen_x_all = torch.zeros(test_size, M, 28*28)\n","gen_y_all = torch.zeros(test_size, M, 10)\n","z_all = torch.zeros(test_size, latent_space_dim)\n","x_all = torch.zeros(test_size, 28*28)\n","y_all = torch.zeros(test_size, 10)\n","\n","\n","G_label = G_label.eval()\n","G_image = G_image.eval()\n","for i, (z_test, y_test, x_test) in tqdm(enumerate(DataLoader_test)):\n","    Z_test_repeat = z_test.repeat(M,1).to(device).detach()\n","    Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension_label, input_noise_type, input_var = 1.0/3.0).to(device)\n","    with torch.no_grad():\n","        gen_y = G_label(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device).detach()\n","\n","    Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension_image, input_noise_type, input_var = 1.0/3.0).to(device)\n","    with torch.no_grad():\n","        gen_x = G_image(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device).detach()\n","\n","    gen_x = gen_x.reshape(1, M, 28*28).detach().to(device)\n","    gen_y = gen_y.reshape(1, M, 10).detach().to(device)\n","\n","    x_all[i,:] = x_test\n","    y_all[i,:] = y_test\n","    z_all[i,:] = z_test\n","\n","    gen_x_all[i,:] = gen_x\n","    gen_y_all[i,:] = gen_y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UKatisIC1Tzv","outputId":"e509751f-b26c-44ad-ce42-c7cf7a5b6707"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:57<00:00,  7.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  0  has p value:  0.032\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:59<00:00,  7.88s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  250  has p value:  0.074\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:02<00:00,  7.90s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  500  has p value:  0.597\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:05<00:00,  7.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  750  has p value:  0.367\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:04<00:00,  7.93s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  1000  has p value:  0.018\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:10<00:00,  7.98s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  1250  has p value:  0.027\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:09<00:00,  7.98s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  1500  has p value:  0.225\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:14<00:00,  8.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  1750  has p value:  0.005\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:16<00:00,  8.05s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  2000  has p value:  0.016\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:08<00:00,  7.96s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  2250  has p value:  0.051\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:06<00:00,  7.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  2500  has p value:  0.864\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:01<00:00,  7.89s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  2750  has p value:  0.279\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:58<00:00,  7.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  3000  has p value:  0.057\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:57<00:00,  7.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  3250  has p value:  0.304\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:34<00:00,  8.23s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  3500  has p value:  0.169\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:18<00:00,  8.07s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  3750  has p value:  0.068\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:14<00:00,  8.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  4000  has p value:  0.008\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:15<00:00,  8.04s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  4250  has p value:  0.053\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:13<00:00,  8.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  4500  has p value:  0.015\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:29<00:00,  8.18s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  4750  has p value:  0.334\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:21<00:00,  8.10s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  5000  has p value:  0.193\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:14<00:00,  8.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  5250  has p value:  0.098\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:20<00:00,  8.08s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  5500  has p value:  0.037\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:21<00:00,  8.09s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  5750  has p value:  0.058\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:07<00:00,  7.95s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  6000  has p value:  0.861\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:59<00:00,  7.88s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  6250  has p value:  0.08\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:57<00:00,  7.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  6500  has p value:  0.037\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:57<00:00,  7.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  6750  has p value:  0.078\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:58<00:00,  7.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  7000  has p value:  0.843\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:59<00:00,  7.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  7250  has p value:  0.122\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [12:56<00:00,  7.85s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  7500  has p value:  0.157\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:13<00:00,  8.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  7750  has p value:  0.917\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:12<00:00,  8.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  8000  has p value:  0.097\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:14<00:00,  8.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  8250  has p value:  0.03\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:23<00:00,  8.11s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  8500  has p value:  0.857\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:11<00:00,  8.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  8750  has p value:  0.233\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:19<00:00,  8.07s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  9000  has p value:  0.23\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:11<00:00,  8.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  9250  has p value:  0.156\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:01<00:00,  7.90s/it]\n"]},{"name":"stdout","output_type":"stream","text":["the  9500  has p value:  0.038\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 99/99 [13:08<00:00,  7.96s/it]"]},{"name":"stdout","output_type":"stream","text":["the  9750  has p value:  0.026\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["n_length_input = int(test_size/Total_num_p_val)\n","p_val_list = []\n","\n","for i in range(0, Total_num_p_val):\n","    sigma_w = sigma_w_train\n","    sigma_u = sigma_u_train\n","    sigma_v = sigma_v_train\n","\n","    boot_num = 1000\n","    boor_rv_type = 'gaussian'\n","\n","    n_length = n_length_input\n","    start_index = n_length_input*(i)\n","    end_index = start_index + n_length\n","\n","    gen_x_all_in = gen_x_all[start_index:end_index,].to(device).detach()\n","    gen_y_all_in = gen_y_all[start_index:end_index,].to(device).detach()\n","    x_all_in = x_all[start_index:end_index,].to(device).detach()\n","    y_all_in = y_all[start_index:end_index,].to(device).detach()\n","    z_all_in = z_all[start_index:end_index,].to(device).detach()\n","\n","    cur_stat, cur_boot_temp = get_p_value_stat_1(boot_num, M, n_length, gen_x_all_in, gen_y_all_in,\n","                            x_all_in, y_all_in, z_all_in, sigma_w, sigma_u, sigma_v,\n","                            boor_rv_type)\n","    p_val = np.mean( cur_boot_temp > cur_stat )\n","    print(\"the \",start_index,\" has p value: \",p_val)\n","    p_val_list.append(p_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zNBzifPfFRpw","outputId":"8340dee3-1816-4790-83c0-4bde5debb3a7"},"outputs":[{"data":{"text/plain":["[0.032,\n"," 0.074,\n"," 0.597,\n"," 0.367,\n"," 0.018,\n"," 0.027,\n"," 0.225,\n"," 0.005,\n"," 0.016,\n"," 0.051,\n"," 0.864,\n"," 0.279,\n"," 0.057,\n"," 0.304,\n"," 0.169,\n"," 0.068,\n"," 0.008,\n"," 0.053,\n"," 0.015,\n"," 0.334,\n"," 0.193,\n"," 0.098,\n"," 0.037,\n"," 0.058,\n"," 0.861,\n"," 0.08,\n"," 0.037,\n"," 0.078,\n"," 0.843,\n"," 0.122,\n"," 0.157,\n"," 0.917,\n"," 0.097,\n"," 0.03,\n"," 0.857,\n"," 0.233,\n"," 0.23,\n"," 0.156,\n"," 0.038,\n"," 0.026]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["p_val_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PLv1oyip7u-f","outputId":"c59a587b-620c-49c9-f247-582129a470d8"},"outputs":[{"data":{"text/plain":["(0.037, 0.0885, 0.24450000000000002)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["np.quantile(p_val_list, 0.25), np.median(p_val_list), np.quantile(p_val_list, 0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JQYtNh0fDZlW","outputId":"9349b76c-5ab4-4b79-e785-fffd3c3b3a3d"},"outputs":[{"data":{"text/plain":["0.3"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["np.mean([p_val < 0.05 for p_val in p_val_list])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}