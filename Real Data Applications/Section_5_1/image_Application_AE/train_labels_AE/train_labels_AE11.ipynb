{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"NEewc8I9JZNd","executionInfo":{"status":"ok","timestamp":1716252141897,"user_tz":300,"elapsed":3630,"user":{"displayName":"Linjun","userId":"13995434670626730968"}}},"outputs":[],"source":["latent_space_dim = 11\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import SGD\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.distributions as TD\n","from zmq import device\n","import torch.optim as optim\n","from datetime import datetime\n","import functools\n","from tqdm import tqdm\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","import torch.nn as nn\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :28, :28]\n","\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(self, d_l):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential( #784\n","                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","                nn.Flatten(),\n","                nn.Linear(3136, d_l)\n","        )\n","        self.decoder = nn.Sequential(\n","                torch.nn.Linear(d_l, 3136),\n","                Reshape(-1, 64, 7, 7),\n","                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1), # 64x7x7 -> 64x7x7\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1), # 64x7x7 -> 64x13x13\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0), # 64x13x13 -> 32x27x27\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), # 32x27x27 -> 1x29x29\n","                Trim(),  # 1x29x29 -> 1x28x28\n","                nn.Sigmoid()\n","                )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","    def get_latent_space(self, x):\n","        return self.encoder(x)\n","\n","    def get_decoded_images(self, x):\n","        return self.decoder(x)\n","AE_model = AutoEncoder(d_l = latent_space_dim)\n","AE_model.load_state_dict(torch.load('./AE_'+ str(latent_space_dim) +'.pth'))\n","AE_model.to(device)\n","\n","AE_model.eval()\n","class CTDataset(Dataset):\n","    def __init__(self, filepath):\n","        self.x, self.y = torch.load(filepath)\n","        self.x = self.x / 255.\n","        self.x = self.x.reshape(-1, 1, 28, 28).cuda().detach()\n","        with torch.no_grad():\n","            self.x = AE_model.get_latent_space(self.x)\n","        self.x = self.x.detach()\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","        # self.x_max, _ = torch.max(self.x, dim=0, keepdim=True)\n","        # self.x_min, _ = torch.min(self.x, dim=0, keepdim=True)\n","        # self.x = (self.x - self.x_min) / (self.x_max - self.x_min)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","\n","def find_loss(y_torch, gen_y_all_torch, z_torch, sigma_w, sigma_u, M, dim_y):\n","    n = z_torch.shape[0]\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.linalg.vector_norm(y_torch.repeat(n, 1, 1) - torch.swapaxes(y_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, dim_y) - y_torch.repeat(1, n).reshape(n, n, 1, dim_y), ord=1, dim=3) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, dim_y)\n","    temp_mx = torch.swapaxes(gen_y_all_torch_rep[:, :, 0, :], 0, 1)\n","    sum_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1, dim_y), ord=1, dim=3) / sigma_u), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = torch.swapaxes(gen_y_all_torch_rep[:, :, i, :], 0, 1)\n","        temp_add_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1, dim_y), ord=1, dim=3) / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","\n","    FF_mx = u_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    loss = 1 / (n) * torch.sum(FF_mx)\n","    return loss\n","\n","##### GAN architecture #####\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                 drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","      self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","        x = self.softmax(x)\n","      else:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","        # x = self.sigmoid(x)\n","        x = self.softmax(x)\n","\n","      return x\n","\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","def train_label(DataLoader_test, train_ds, test_ds,\n","      noise_dimension, noise_type, G_lr, hidden_layer_size,\n","      DataLoader, BN_type, ReLU_coef,\n","      epochs_num=10,  sigma_z = 1, sigma_y = 1,\n","      lambda_1 = 1, wgt_decay = 0,\n","      lambda_3 = 0, drop_out_p = 0.2, M_train = 3, input_noise_var = 1./3.):\n","    \"\"\"\n","    Train loop for GAN.\n","\n","    Inputs:\n","    - X: PyTorch Tensor (sample_size, input_dimension) of training input.\n","    - Y: PyTorch Tensor (sample_size, output_dimension) of training output.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","    - D_lr, G_lr: Float giving the learning rate of the discriminator and\n","      the generator.\n","    - discriminator_type, generator_type: (\"KL\", \"JS\", \"WS\", \"LS\"), giving the loss criterion\n","      of the discriminator and generator, respectively.\n","    - discriminator_loss, generator_loss: Functions to use for computing the\n","      generator and discriminator loss, respectively.\n","    - DataLoader: DataLoader object used to generate training batches.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","    - batch_size: Integer giving the size of batches for each epoch.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - lambda_gp: Float giving the coefficient of gradient penalty for WS.\n","\n","    Outputs:\n","    - D: PyTorch Net giving the trained discriminator.\n","    - G: PyTorch Net giving the trained generator.\n","    - Output the trained D and G at 250, 500, 750, 1000 epochs.\n","    \"\"\"\n","\n","    input_dimension = latent_space_dim\n","    output_dimension_y = 10\n","\n","    criterion = nn.CrossEntropyLoss() # for testing\n","\n","    G_zy = Generator(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p).to(device)\n","    G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","    # G_zy_solver = optim.SGD(G_zy.parameters(), lr=G_lr, weight_decay=wgt_decay)\n","\n","    scheduler = torch.optim.lr_scheduler.StepLR(G_zy_solver, step_size=20, gamma=0.1)\n","\n","    batch_size_eval = 128\n","    DataLoader_eval = torch.utils.data.DataLoader(test_ds, batch_size=batch_size_eval, shuffle=False, drop_last= False)\n","\n","    train_acc = 0.0\n","\n","    G_zy = G_zy.eval()\n","    with torch.no_grad():\n","        mmd_loss = []\n","        acc_list = []\n","\n","        running_loss = []\n","        for i, data in enumerate(DataLoader_eval, 0):\n","            Z_test, Y_test = data\n","            Z_test = Z_test.to(device)\n","            Y_test = Y_test.to(device)\n","\n","            batch_size = Z_test.shape[0]\n","            Z_real_repeat = Z_test.repeat(M_train,1)\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","            with torch.no_grad():\n","                Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","            Y_fake = Y_fake.reshape(batch_size, M_train, output_dimension_y)\n","            loss = find_loss(Y_test, Y_fake, Z_test, sigma_z, sigma_y, M_train, output_dimension_y)\n","            running_loss.append(loss.item())\n","\n","        running_loss_train = []\n","        for i, data in enumerate(DataLoader, 0):\n","            Z_test, Y_test = data\n","            Z_test = Z_test.to(device)\n","            Y_test = Y_test.to(device)\n","\n","            batch_size = Z_test.shape[0]\n","            Z_real_repeat = Z_test.repeat(M_train,1)\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","            with torch.no_grad():\n","                Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","            Y_fake = Y_fake.reshape(batch_size, M_train, output_dimension_y)\n","            loss = find_loss(Y_test, Y_fake, Z_test, sigma_z, sigma_y, M_train, output_dimension_y)\n","            running_loss_train.append(loss.item())\n","\n","        train_acc_list = []\n","        for i, data in enumerate(DataLoader, 0):\n","            Z_test, Y_test = data\n","            Z_test = Z_test.to(device)\n","            Y_test = Y_test.to(device)\n","\n","            batch_size = Z_test.shape[0]\n","            Z_real_repeat = Z_test.repeat(100,1)\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","            with torch.no_grad():\n","                Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device).argmax(axis = 1)\n","\n","            Y_fake, _ = torch.mode(Y_fake.reshape(100, batch_size).T, dim = 1)\n","            train_acc = torch.mean((Y_fake == Y_test.argmax(axis = 1)).float()).item()\n","            train_acc_list.append(train_acc)\n","\n","        train_acc = np.mean(train_acc_list)\n","\n","        test_acc_list = []\n","        for i, data in enumerate(DataLoader_eval, 0):\n","            Z_test, Y_test = data\n","            Z_test = Z_test.to(device)\n","            Y_test = Y_test.to(device)\n","\n","            batch_size = Z_test.shape[0]\n","            Z_real_repeat = Z_test.repeat(100,1)\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","            with torch.no_grad():\n","                Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device).argmax(axis = 1)\n","\n","            Y_fake, _ = torch.mode(Y_fake.reshape(100, batch_size).T, dim = 1)\n","            test_acc = torch.mean((Y_fake == Y_test.argmax(axis = 1)).float()).item()\n","            test_acc_list.append(test_acc)\n","\n","        test_acc = np.mean(test_acc_list)\n","\n","        mmd_loss_temp = sum(running_loss)/(len(running_loss)*batch_size_eval)\n","        mmd_loss_temp_train = sum(running_loss_train)/(len(running_loss_train)*batch_size_eval)\n","        print(f\"[Epoch 0, train Loss: { mmd_loss_temp_train }], test Loss: { mmd_loss_temp }], [train acc: { train_acc }], [test acc: { test_acc }]\")\n","        acc_list.append(train_acc)\n","        mmd_loss.append(mmd_loss_temp)\n","        torch.cuda.empty_cache()\n","\n","\n","    iter_count = 0\n","    G_zy = G_zy.train()\n","\n","    for epoch in tqdm(range(epochs_num)):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zy = G_zy.train()\n","        for Z_real, Y_real in DataLoader:\n","            Y_real = Y_real.to(device)\n","            Z_real = Z_real.to(device)\n","\n","            batch_size = Z_real.shape[0]\n","            Z_real_repeat = Z_real.repeat(M_train,1)\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","            Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","\n","            Y_fake = Y_fake.reshape(batch_size, M_train, output_dimension_y)\n","\n","            # Generator step\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            l1_regularization = 0\n","\n","            for param in G_zy.parameters():\n","                l1_regularization += torch.linalg.vector_norm(param, ord = 1)\n","\n","            g_zy_error = lambda_1 * find_loss(Y_real, Y_fake, Z_real, sigma_z, sigma_y, M_train, output_dimension_y) + lambda_3 * l1_regularization\n","\n","            g_zy_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zy.parameters(), max_norm=0.5)\n","            G_zy_solver.step()\n","\n","            iter_count += 1\n","            batch_count += 1\n","        scheduler.step()\n","\n","        if epoch % 20 == 19:\n","            G_zy = G_zy.eval()\n","            with torch.no_grad():\n","                running_loss = []\n","                for i, data in enumerate(DataLoader_eval, 0):\n","                    Z_test, Y_test = data\n","                    Z_test = Z_test.to(device)\n","                    Y_test = Y_test.to(device)\n","\n","                    batch_size = Z_test.shape[0]\n","                    Z_real_repeat = Z_test.repeat(M_train,1)\n","                    # Generate fake data\n","                    Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","                    Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","                    Y_fake = Y_fake.reshape(batch_size, M_train, output_dimension_y)\n","                    loss = find_loss(Y_test, Y_fake, Z_test, sigma_z, sigma_y, M_train, output_dimension_y)\n","                    running_loss.append(loss.item())\n","\n","                running_loss_train = []\n","                for i, data in enumerate(DataLoader, 0):\n","                    Z_test, Y_test = data\n","                    Z_test = Z_test.to(device)\n","                    Y_test = Y_test.to(device)\n","\n","                    batch_size = Z_test.shape[0]\n","                    Z_real_repeat = Z_test.repeat(M_train,1)\n","                    # Generate fake data\n","                    Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","                    Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","                    Y_fake = Y_fake.reshape(batch_size, M_train, output_dimension_y)\n","                    loss = find_loss(Y_test, Y_fake, Z_test, sigma_z, sigma_y, M_train, output_dimension_y)\n","                    running_loss_train.append(loss.item())\n","\n","                test_acc_list = []\n","                for i, data in enumerate(DataLoader_eval, 0):\n","                    Z_test, Y_test = data\n","                    Z_test = Z_test.to(device)\n","                    Y_test = Y_test.to(device)\n","\n","                    batch_size = Z_test.shape[0]\n","                    Z_real_repeat = Z_test.repeat(100,1)\n","                    # Generate fake data\n","                    Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","                    with torch.no_grad():\n","                        Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device).argmax(axis = 1)\n","\n","                    Y_fake, _ = torch.mode(Y_fake.reshape(100, batch_size).T, dim = 1)\n","                    test_acc = torch.mean((Y_fake == Y_test.argmax(axis = 1)).float()).item()\n","                    test_acc_list.append(test_acc)\n","\n","                test_acc = np.mean(test_acc_list)\n","\n","                train_acc_list = []\n","                for i, data in enumerate(DataLoader, 0):\n","                    Z_test, Y_test = data\n","                    Z_test = Z_test.to(device)\n","                    Y_test = Y_test.to(device)\n","\n","                    batch_size = Z_test.shape[0]\n","                    Z_real_repeat = Z_test.repeat(100,1)\n","                    # Generate fake data\n","                    Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_noise_var).to(device)\n","                    with torch.no_grad():\n","                        Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device).argmax(axis = 1)\n","\n","                    Y_fake, _ = torch.mode(Y_fake.reshape(100, batch_size).T, dim = 1)\n","                    train_acc = torch.mean((Y_fake == Y_test.argmax(axis = 1)).float()).item()\n","                    train_acc_list.append(train_acc)\n","\n","                train_acc = np.mean(train_acc_list)\n","\n","                mmd_loss_temp = sum(running_loss)/(len(running_loss)*batch_size_eval)\n","                mmd_loss_temp_train = sum(running_loss_train)/(len(running_loss_train)*batch_size_eval)\n","                print(f\"[Epoch {epoch + 1}, train Loss: { mmd_loss_temp_train }], test Loss: { mmd_loss_temp }], [train acc: { train_acc }], [test acc: { test_acc }]\")\n","                acc_list.append(train_acc)\n","                mmd_loss.append(mmd_loss_temp)\n","\n","            torch.cuda.empty_cache()\n","\n","    return G_zy, acc_list, mmd_loss, train_acc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bp6XzWtgPEBB","outputId":"18821a67-c758-4671-a428-4dd16cd42be9"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:01<00:00, 9480.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[sigma_w_train 22.991113662719727]\n","[seed: 0]\n","[Epoch 0, train Loss: 0.21798990233242485], test Loss: 0.22811116347423244], [train acc: 0.13650265957446808], [test acc: 0.14458069620253164]\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 20/60 [01:01<02:28,  3.72s/it]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 20, train Loss: 0.0070898152664829375], test Loss: 0.009163318647128636], [train acc: 0.7140735816448293], [test acc: 0.7177610759493671]\n"]},{"output_type":"stream","name":"stderr","text":[" 67%|██████▋   | 40/60 [02:01<01:14,  3.71s/it]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 40, train Loss: 0.0067192624662066125], test Loss: 0.008426472245144758], [train acc: 0.7511746454746165], [test acc: 0.7532634493670886]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [03:02<00:00,  3.05s/it]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 60, train Loss: 0.006631252887649724], test Loss: 0.008401441085122096], [train acc: 0.7570811170212766], [test acc: 0.7592958860759493]\n","[seed: 1]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 0, train Loss: 0.2197782801848758], test Loss: 0.22955264768240563], [train acc: 0.07142065604950519], [test acc: 0.06675237341772151]\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 7/60 [00:20<02:33,  2.89s/it]"]}],"source":["torch.cuda.empty_cache()\n","\n","batch_size = 128\n","noise_dimension = 1\n","G_lr = 1e-2\n","input_noise_type = \"normal\"\n","input_noise_var = 1.0/3.0\n","\n","torch.manual_seed(42)\n","train_ds = CTDataset('./training.pt')\n","\n","torch.manual_seed(42)\n","train_AE_set, train_cond_gen_set = torch.utils.data.random_split(train_ds, [30000, 30000])\n","train_ds = train_cond_gen_set\n","DataLoader_train = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last= False)\n","\n","test_ds = CTDataset('./test.pt')\n","DataLoader_test = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, drop_last= False)\n","\n","xs, ys = train_ds[0:10000]\n","pairwise_distance_x = torch.zeros(xs.shape[0], xs.shape[0])\n","\n","for i in tqdm(range(xs.shape[0])):\n","    pairwise_distance_x[i,:] = torch.linalg.vector_norm(xs[i].reshape(1,-1) - xs, ord = 1, dim = 1)\n","\n","sigma_w_train = torch.median(pairwise_distance_x).item()\n","print(f\"[sigma_w_train {sigma_w_train}]\")\n","\n","sigma_w_train = sigma_w_train\n","sigma_u_train = 2.0\n","\n","# seed_list = [0, 42, 114514, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","seed_list = range(40)\n","train_acc_list = []\n","\n","for seed in seed_list:\n","    torch.manual_seed(seed)\n","    print(f\"[seed: {seed}]\")\n","    G_zy, acc_list, mmd_loss, train_acc = train_label(DataLoader_test = DataLoader_test, train_ds = train_ds, test_ds = test_ds,\n","                                noise_dimension = noise_dimension, noise_type = input_noise_type, G_lr = G_lr, hidden_layer_size = 512,\n","                                DataLoader = DataLoader_train, BN_type = True, ReLU_coef = 0.5,\n","                                epochs_num=60,  sigma_z = sigma_w_train, sigma_y = sigma_u_train,\n","                                lambda_1 = 1, wgt_decay = 1e-5,\n","                                lambda_3 = 1e-5, drop_out_p = 0.2, M_train = 10, input_noise_var = input_noise_var)\n","    train_acc_list.append(train_acc)\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_QmGJ2ZL_mm"},"outputs":[],"source":["print(*train_acc_list, sep = \", \")\n","print(f\"[argmax train_acc: {np.argmax(train_acc_list)}]\")\n","print(f\"[best seed: {seed_list[np.argmax(train_acc_list)]}]\")\n","\n","\n","torch.cuda.empty_cache()\n","\n","batch_size = 128\n","noise_dimension = 1\n","G_lr = 1e-2\n","input_noise_type = \"normal\"\n","\n","torch.manual_seed(seed_list[np.argmax(train_acc_list)])\n","G_zy, acc_list, mmd_loss, train_acc = train_label(DataLoader_test = DataLoader_test, train_ds = train_ds, test_ds = test_ds,\n","                            noise_dimension = noise_dimension, noise_type = input_noise_type, G_lr = G_lr, hidden_layer_size = 512,\n","                            DataLoader = DataLoader_train, BN_type = True, ReLU_coef = 0.5,\n","                            epochs_num=60,  sigma_z = sigma_w_train, sigma_y = sigma_u_train,\n","                            lambda_1 = 1, wgt_decay = 1e-5,\n","                            lambda_3 = 1e-5, drop_out_p = 0.2, M_train = 10, input_noise_var = input_noise_var)\n","\n","torch.cuda.empty_cache()\n","\n","torch.save(G_zy.state_dict(), \"./AE\"+ str(latent_space_dim) +\"_label.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ao5EhiCzPOI"},"outputs":[],"source":["fig, ax = plt.subplots(2,1,figsize=(10,15))\n","\n","plt.subplot(2,1,1)\n","plt.plot(np.log10(mmd_loss))\n","plt.xlabel('Epoch Number')\n","plt.ylabel('log 10 MMD2 loss')\n","plt.title('log 10 MMD2 loss')\n","\n","plt.subplot(2,1,2)\n","plt.plot(acc_list)\n","plt.xlabel('Epoch Number')\n","plt.ylabel('classification accuracy')\n","plt.title('classification accuracy')\n","\n","fig.tight_layout()\n","plt.savefig('training_loss.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0FlbR_1sw_2"},"outputs":[],"source":["G_zy.eval()\n","fig, ax = plt.subplots(20,5,figsize=(10,25))\n","\n","for i in range(100):\n","    xs, ys = train_ds[i]\n","    xs, ys = xs.to(device), ys.to(device)\n","\n","    xs_rep = xs.repeat(1000,1).detach()\n","\n","    Noise_fake = sample_noise(xs_rep.shape[0], noise_dimension, input_noise_type, input_var = input_noise_var).to(device).detach()\n","    with torch.no_grad():\n","        yhats = G_zy(torch.cat((xs_rep,Noise_fake),dim=1)).to(device).argmax(axis = 1)\n","    yhats_mode, _ = torch.mode(yhats.reshape(1000, 1).T, dim = 1)\n","    ys_true = ys.argmax(axis = 0).cpu()\n","    # Plot histogram\n","    plt.subplot(20,5,i+1)\n","    plt.hist(yhats.cpu().numpy(), bins=30, alpha=0.5, color='b')\n","    plt.title(f'Predicted: {yhats_mode.item()}, True: {ys_true}')\n","fig.tight_layout()\n","plt.savefig('training_results_02.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDA9mEw2qOcs"},"outputs":[],"source":["G_zy.eval()\n","fig, ax = plt.subplots(20,5,figsize=(10,25))\n","\n","for i in range(100):\n","    xs, ys = test_ds[i]\n","    xs, ys = xs.to(device), ys.to(device)\n","\n","    xs_rep = xs.repeat(1000,1).detach()\n","\n","    Noise_fake = sample_noise(xs_rep.shape[0], noise_dimension, input_noise_type, input_var = input_noise_var).to(device).detach()\n","    with torch.no_grad():\n","        yhats = G_zy(torch.cat((xs_rep,Noise_fake),dim=1)).to(device).argmax(axis = 1)\n","    yhats_mode, _ = torch.mode(yhats.reshape(1000, 1).T, dim = 1)\n","    ys_true = ys.argmax(axis = 0).cpu()\n","    # Plot histogram\n","    plt.subplot(20,5,i+1)\n","    plt.hist(yhats.cpu().numpy(), bins=30, alpha=0.5, color='b')\n","    plt.title(f'Predicted: {yhats_mode.item()}, True: {ys_true}')\n","fig.tight_layout()\n","plt.savefig('training_results_02.png')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}