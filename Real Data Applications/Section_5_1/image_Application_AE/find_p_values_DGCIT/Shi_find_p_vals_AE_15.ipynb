{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118446,"status":"ok","timestamp":1731969644529,"user":{"displayName":"Linjun Huang","userId":"03749337197712996902"},"user_tz":360},"id":"4aVcP_s6-znC","outputId":"443f95b3-3627-48b5-89f0-e85a8976c819"},"outputs":[{"output_type":"stream","name":"stderr","text":["10000it [00:01, 5044.97it/s]\n","100%|██████████| 40/40 [01:44<00:00,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["the latent_space is:  7 \n","\n","the lower bound is:  0.28099999999999997 the median is:  0.3845 the higher bound is:  0.48724999999999996 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["latent_space_dim = 15 # [3, 4, 5, 6, 7, 8, 10, 12, 15, 16, 17, 20, 30, 40, 100]\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import SGD\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.distributions as TD\n","from zmq import device\n","import torch.optim as optim\n","from datetime import datetime\n","import functools\n","from tqdm import tqdm\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","import torch.nn as nn\n","\n","set_seed = 42\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# Utilites related to Sinkhorn computations and training for TensorFlow 2.0\n","import tensorflow as tf\n","import logging\n","import tensorflow_probability as tfp\n","from sklearn.metrics.pairwise import rbf_kernel\n","from scipy.stats import rankdata, ks_2samp, wilcoxon\n","from sklearn.model_selection import KFold\n","from datetime import datetime\n","import decimal\n","import torch\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","import gc # Garbage Collector\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","logging.getLogger('tensorflow').disabled = True\n","tf.keras.backend.set_floatx('float32')\n","\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","    def __init__(self, X, Y, Z, batch_size):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.batch_size = batch_size\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[(self.batch_size+index) % self.sample_size]\n","\n","class CharacteristicFunction:\n","    '''\n","    class to construct a function that represents the characteristic function\n","    '''\n","\n","    def __init__(self, size, x_dims, z_dims, test_size):\n","        self.n_samples = size\n","        self.hidden_dims = 20 # default: 20\n","        self.test_size = test_size\n","\n","        self.input_dim = z_dims + x_dims\n","        self.z_dims = z_dims\n","        self.x_dims = x_dims\n","        self.input_shape1x = [self.x_dims, self.hidden_dims]\n","        self.input_shape1z = [self.z_dims, self.hidden_dims]\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, 1]\n","\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = tf.sqrt(2.0 / (input_shape[0]))\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def update(self):\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","\n","    def call(self, x, z):\n","        # inputs are concatenations of z and v\n","        x = tf.reshape(tensor=x, shape=[self.test_size, -1, self.x_dims])\n","        z = tf.reshape(tensor=z, shape=[self.test_size, -1, self.z_dims])\n","        # we asssume parameter b for z to be 0\n","        h1 = tf.nn.sigmoid(tf.matmul(x, self.w1x) + self.b1)\n","        out = tf.nn.sigmoid(tf.matmul(h1, self.w2))\n","        return out\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","#\n","# test statistics for DGCIT\n","#\n","\n","\n","def t_and_sigma(psy_x_i, psy_y_i, phi_x_i, phi_y_i):\n","    b, n = psy_x_i.shape\n","    x_mtx = phi_x_i - psy_x_i\n","    y_mtx = phi_y_i - psy_y_i\n","    matrix = tf.reshape(x_mtx[None, :, :] * y_mtx[:, None, :], [-1, n])\n","    t_b = tf.reduce_sum(matrix, axis=1) / tf.cast(n, tf.float64)\n","    t_b = tf.expand_dims(t_b, axis=1)\n","\n","    crit_matrix = matrix - t_b\n","    std_b = tf.sqrt(tf.reduce_sum(crit_matrix**2, axis=1) / tf.cast(n-1, tf.float64))\n","    return t_b, std_b\n","\n","\n","def test_statistics(psy_x_i, psy_y_i, phi_x_i, phi_y_i, t_b, std_b, j):\n","    b, n = psy_x_i.shape\n","    x_mtx = phi_x_i - psy_x_i\n","    y_mtx = phi_y_i - psy_y_i\n","    matrix = tf.reshape(x_mtx[None, :, :] * y_mtx[:, None, :], [-1, n])\n","    crit_matrix = matrix - t_b\n","    test_stat = tf.reduce_max(tf.abs(tf.sqrt(tf.cast(n, tf.float64)) * tf.squeeze(t_b) / std_b))\n","\n","    sig = tf.reduce_sum(crit_matrix[None, :, :] * crit_matrix[:, None, :], axis=2)\n","    coef = std_b[None, :] * std_b[:, None] * tf.cast(n-1, tf.float64)\n","    sig_xy = sig / coef\n","\n","    eigenvalues, eigenvectors = tf.linalg.eigh(sig_xy)\n","    base = tf.zeros_like(eigenvectors)\n","    eig_vals = tf.sqrt(eigenvalues + 1e-12)\n","    lamda = tf.linalg.set_diag(base, eig_vals)\n","    sig_sqrt = tf.matmul(tf.matmul(eigenvectors, lamda), tf.linalg.inv(eigenvectors))\n","\n","    z_dist = tfp.distributions.Normal(0.0, scale=1.0)\n","    z_samples = z_dist.sample([b*b, j])\n","    z_samples = tf.cast(z_samples, tf.float64)\n","    vals = tf.matmul(sig_sqrt, z_samples)\n","    t_j = tf.reduce_max(vals, axis=0)\n","    return test_stat, t_j\n","\n","#\n","# Training algorithm for DGCIT\n","#\n","\n","def dgcit(x, y, z, generator_x, generator_y, test_size=500, z_dim=100,\n","      x_dims=1, y_dims=1, M=100, k=1,\n","      b=30, j=1000):\n","\n","    noise_dimension_image = 50\n","    noise_dimension_label = 1\n","    input_noise_type = \"normal\"\n","\n","    psy_x_all = []\n","    phi_x_all = []\n","    psy_y_all = []\n","    phi_y_all = []\n","    test_samples = b\n","\n","    psy_x_b = []\n","    phi_x_b = []\n","    psy_y_b = []\n","    phi_y_b = []\n","\n","    x_samples = []\n","    y_samples = []\n","    z_input = []\n","    x_input = []\n","    y_input = []\n","\n","    test_xyz = DatasetSelect_GAN(x, y, z, 1)\n","    testing_dataset = torch.utils.data.DataLoader(test_xyz, batch_size=1, shuffle=False)\n","\n","    G_image = generator_x.eval()\n","    G_label = generator_y.eval()\n","\n","    for test_x, test_y, test_z, Z_fake in testing_dataset:\n","\n","        Z_test_repeat = test_z.repeat(M,1).to(device).detach()\n","\n","        # Generate fake data\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension_label, input_noise_type, input_var = 1.0/3.0).to(device)\n","        with torch.no_grad():\n","            gen_y = G_label(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device).detach()\n","\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension_image, input_noise_type, input_var = 1.0/3.0).to(device)\n","        with torch.no_grad():\n","            gen_x = G_image(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device).detach()\n","\n","        gen_x_all = gen_x.reshape(M, x_dims).cpu().detach().numpy()\n","        gen_y_all = gen_y.reshape(M, y_dims).cpu().detach().numpy()\n","\n","        fake_x = tf.convert_to_tensor(gen_x_all)\n","        fake_y = tf.convert_to_tensor(gen_y_all)\n","\n","        test_z = tf.convert_to_tensor(test_z.cpu().detach().numpy())\n","        test_x = tf.convert_to_tensor(test_x.cpu().detach().numpy())\n","        test_y = tf.convert_to_tensor(test_y.cpu().detach().numpy())\n","\n","        test_z = tf.reshape(test_z, (1, z_dim))\n","        test_y = tf.reshape(test_y, (1, y_dims))\n","        test_x = tf.reshape(test_x, (1, x_dims))\n","\n","        fake_x = tf.cast(fake_x, tf.float64)\n","        fake_y = tf.cast(fake_y, tf.float64)\n","        test_z = tf.cast(test_z, tf.float64)\n","        test_x = tf.cast(test_x, tf.float64)\n","        test_y = tf.cast(test_y, tf.float64)\n","\n","        x_samples.append(fake_x)\n","        y_samples.append(fake_y)\n","        z_input.append(test_z)\n","        x_input.append(test_x)\n","        y_input.append(test_y)\n","\n","    # give the five variables: x_samples, y_samples, z_input, x_input, y_input\n","    # they are lists with length = test_size\n","    # x_samples: [x_sampled1, x_sampled2, ... , x_sampled_test_size] x_sampled1 has shape [M, dx]\n","    # y_samples: [y_sampled1, y_sampled2, ... , y_sampled_test_size] y_sampled1 has shape [M, dy]\n","    # z_input = [z1, z2, ... , z_test_size]\n","    # x_input = [x1, x2, ... , x_test_size]\n","    # y_input = [y1, y2, ... , y_test_size]\n","\n","\n","    standardise = True\n","\n","    if standardise:\n","        x_samples = (x_samples - tf.reduce_mean(x_samples)) / tf.math.reduce_std(x_samples)\n","        y_samples = (y_samples - tf.reduce_mean(y_samples)) / tf.math.reduce_std(y_samples)\n","        x_input = (x_input - tf.reduce_mean(x_input)) / tf.math.reduce_std(x_input)\n","        y_input = (y_input - tf.reduce_mean(y_input)) / tf.math.reduce_std(y_input)\n","        z_input = (z_input - tf.reduce_mean(z_input)) / tf.math.reduce_std(z_input)\n","\n","    f1 = CharacteristicFunction(M, x_dims, z_dim, test_size)\n","    f2 = CharacteristicFunction(M, y_dims, z_dim, test_size)\n","    for i in range(test_samples):\n","        phi_x = tf.reduce_mean(f1.call(x_samples, z_input), axis=1)\n","        phi_y = tf.reduce_mean(f2.call(y_samples, z_input), axis=1)\n","        psy_x = tf.squeeze(f1.call(x_input, z_input))\n","        psy_y = tf.squeeze(f2.call(y_input, z_input))\n","\n","        psy_x_b.append(psy_x)\n","        phi_x_b.append(phi_x)\n","        psy_y_b.append(psy_y)\n","        phi_y_b.append(phi_y)\n","        f1.update()\n","        f2.update()\n","\n","    psy_x_all.append(psy_x_b)\n","    phi_x_all.append(phi_x_b)\n","    psy_y_all.append(psy_y_b)\n","    phi_y_all.append(phi_y_b)\n","\n","    # reshape\n","    psy_x_all = tf.reshape(psy_x_all, [k, test_samples, test_size])\n","    psy_y_all = tf.reshape(psy_y_all, [k, test_samples, test_size])\n","    phi_x_all = tf.reshape(phi_x_all, [k, test_samples, test_size])\n","    phi_y_all = tf.reshape(phi_y_all, [k, test_samples, test_size])\n","\n","    t_b = 0.0\n","    std_b = 0.0\n","    for n in range(k):\n","        t, std = t_and_sigma(psy_x_all[n], psy_y_all[n], phi_x_all[n], phi_y_all[n])\n","        t_b += t\n","        std_b += std\n","    t_b = t_b / tf.cast(k, tf.float64)\n","    std_b = std_b / tf.cast(k, tf.float64)\n","\n","    psy_x_all = tf.transpose(psy_x_all, (1, 0, 2))\n","    psy_y_all = tf.transpose(psy_y_all, (1, 0, 2))\n","    phi_x_all = tf.transpose(phi_x_all, (1, 0, 2))\n","    phi_y_all = tf.transpose(phi_y_all, (1, 0, 2))\n","\n","    psy_x_all = tf.reshape(psy_x_all, [test_samples, test_size*k])\n","    psy_y_all = tf.reshape(psy_y_all, [test_samples, test_size*k])\n","    phi_x_all = tf.reshape(phi_x_all, [test_samples, test_size*k])\n","    phi_y_all = tf.reshape(phi_y_all, [test_samples, test_size*k])\n","\n","    stat, critical_vals = test_statistics(psy_x_all, psy_y_all, phi_x_all, phi_y_all, t_b, std_b, j)\n","    comparison = [c > stat or c == stat for c in critical_vals]\n","    comparison = np.reshape(comparison, (-1,))\n","    p_value = np.sum(comparison.astype(np.float32)) / j\n","\n","    return p_value\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :28, :28]\n","\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(self, d_l):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential( #784\n","                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","                nn.Flatten(),\n","                nn.Linear(3136, d_l)\n","        )\n","        self.decoder = nn.Sequential(\n","                torch.nn.Linear(d_l, 3136),\n","                Reshape(-1, 64, 7, 7),\n","                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1), # 64x7x7 -> 64x7x7\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1), # 64x7x7 -> 64x13x13\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0), # 64x13x13 -> 32x27x27\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), # 32x27x27 -> 1x29x29\n","                Trim(),  # 1x29x29 -> 1x28x28\n","                nn.Sigmoid()\n","                )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","    def get_latent_space(self, x):\n","        return self.encoder(x)\n","\n","    def get_decoded_images(self, x):\n","        return self.decoder(x)\n","\n","class Generator_image(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, noise_dimension):\n","      super(Generator_image, self).__init__()\n","      self.flatten = nn.Flatten()\n","      self.decoder = nn.Sequential(\n","              torch.nn.Linear(input_dimension + noise_dimension, 3136),\n","              Reshape(-1, 64, 7, 7),\n","              nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),\n","              nn.LeakyReLU(0.01),\n","              nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0),\n","              Trim(),  # 1x29x29 -> 1x28x28\n","              nn.Sigmoid()\n","              )\n","\n","    def forward(self, x):\n","      x = self.decoder(x)\n","      x = self.flatten(x)# 1x28x28 -> 1x784\n","      return x\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","      self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","        x = self.softmax(x)\n","      else:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","        # x = self.sigmoid(x)\n","        x = self.softmax(x)\n","\n","      return x\n","\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u=1,\n","            sigma_v=1, boor_rv_type=\"gaussian\", dy_g=10, dx_g = 28*28):\n","\n","    w_mx = torch.zeros(n, n).to(device)\n","\n","    for i in range(n):\n","        w_mx[i,:] = torch.linalg.vector_norm(z_torch[i].reshape(1,-1) - z_torch, ord = 1, dim = 1)\n","\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_temp = torch.zeros(n, n).to(device)\n","\n","    for i in range(n):\n","        u_mx_temp[i,:] = torch.linalg.vector_norm(y_torch[i].reshape(1,-1) - y_torch, ord = 1, dim = 1)\n","\n","    u_mx_1 = torch.exp(-u_mx_temp / sigma_u)\n","\n","    u_mx_temp_2 = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            u_mx_temp_2[i,j,:] = torch.linalg.vector_norm(y_torch[i].reshape(1,-1) - gen_y_all_torch[j,], ord = 1, dim = 1)\n","\n","    u_mx_2 = torch.mean( torch.exp(-u_mx_temp_2 / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    sum_mx_temp = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            sum_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_y_all_torch[j,:,:].reshape(1,M,dy_g) - gen_y_all_torch[i,0,:].reshape(1,1,dy_g), ord = 1, dim = 2)\n","\n","    sum_mx = torch.mean(torch.exp(-sum_mx_temp/ sigma_u), dim=2)\n","\n","    v_mx_temp = torch.zeros(n, n).to(device)\n","\n","    for i in range(n):\n","        v_mx_temp[i,:] = torch.linalg.vector_norm(x_torch[i].reshape(1,-1) - x_torch, ord = 1, dim = 1)\n","\n","    v_mx_1 = torch.exp(-v_mx_temp / sigma_v)\n","\n","    v_mx_temp_2 = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            v_mx_temp_2[i,j,:] = torch.linalg.vector_norm(x_torch[i].reshape(1,-1) - gen_x_all_torch[j,], ord = 1, dim = 1)\n","\n","    v_mx_2 = torch.mean( torch.exp(-v_mx_temp_2 / sigma_v), dim=2)\n","    v_mx_3 = v_mx_2.T\n","\n","    sum2_mx_temp = torch.zeros(n, n, M).to(device)\n","    for i in range(n):\n","        for j in range(n):\n","            sum2_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_x_all_torch[j,:,:].reshape(1,M,dx_g) - gen_x_all_torch[i,0,:].reshape(1,1,dx_g), ord = 1, dim = 2)\n","\n","    sum2_mx = torch.mean(torch.exp(-sum2_mx_temp/ sigma_v), dim=2)\n","\n","    for k in tqdm(range(1, M)):\n","        sum_mx_temp = torch.zeros(n, n, M).to(device)\n","        sum2_mx_temp = torch.zeros(n, n, M).to(device)\n","        for i in range(n):\n","            for j in range(n):\n","                sum_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_y_all_torch[j,:,:].reshape(1,M,dy_g) - gen_y_all_torch[i,k,:].reshape(1,1,dy_g), ord = 1, dim = 2)\n","                sum2_mx_temp[i,j,:] = torch.linalg.vector_norm(gen_x_all_torch[j,:,:].reshape(1,M,dx_g) - gen_x_all_torch[i,k,:].reshape(1,1,dx_g), ord = 1, dim = 2)\n","\n","        temp_add_mx = torch.mean(torch.exp(-sum_mx_temp/ sigma_u), dim=2)\n","        temp2_add_mx = torch.mean(torch.exp(-sum2_mx_temp/ sigma_v), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","        sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1 / M * sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    stat = 1 / (n - 1) * torch.sum(FF_mx).item()\n","\n","    boottemp = np.array([])\n","    torch.manual_seed(42)\n","    if boor_rv_type == \"rademacher\":\n","        eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","        eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","        random_mx = torch.matmul(eboot[:, bb].reshape(-1, 1), eboot[:, bb].reshape(-1, 1).T)\n","        bootmatrix = FF_mx * random_mx\n","        stat_boot = 1 / (n - 1) * torch.sum(bootmatrix).item()\n","        boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","class CTDataset_all(Dataset):\n","    def __init__(self, filepath, AE_model):\n","        self.flatten = nn.Flatten()\n","        self.x, self.y = torch.load(filepath, weights_only=False)\n","        self.x = self.x / 255.\n","        self.z = self.flatten(self.x)\n","        self.x = self.x.reshape(-1, 1, 28, 28).cuda().detach()\n","        AE_model.eval()\n","        with torch.no_grad():\n","            self.x = AE_model.get_latent_space(self.x)\n","        self.x = self.x.detach()\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","        # self.y = self.y.to(float)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix], self.z[ix]\n","\n","class CTDataset(Dataset):\n","    def __init__(self, filepath, AE_model):\n","        self.x, self.y = torch.load(filepath, weights_only=False)\n","        self.x = self.x / 255.\n","        self.x = self.x.reshape(-1, 1, 28, 28).cuda().detach()\n","        AE_model.eval()\n","        with torch.no_grad():\n","            self.x = AE_model.get_latent_space(self.x)\n","        self.x = self.x.detach()\n","        self.y = F.one_hot(self.y, num_classes=10).to(float)\n","        # self.x_max, _ = torch.max(self.x, dim=0, keepdim=True)\n","        # self.x_min, _ = torch.min(self.x, dim=0, keepdim=True)\n","        # self.x = (self.x - self.x_min) / (self.x_max - self.x_min)\n","    def __len__(self):\n","        return self.x.shape[0]\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","\n","AE_model = AutoEncoder(d_l = latent_space_dim)\n","AE_model.load_state_dict(torch.load('./AE_'+ str(latent_space_dim) +'.pth', weights_only=True))\n","AE_model.to(device)\n","\n","noise_dimension_image = 50\n","noise_dimension_label = 1\n","input_noise_type = \"normal\"\n","\n","torch.manual_seed(42)\n","train_ds = CTDataset('./training.pt', AE_model)\n","\n","torch.manual_seed(42)\n","train_AE_set, train_cond_gen_set = torch.utils.data.random_split(train_ds, [30000, 30000])\n","train_ds = train_cond_gen_set\n","DataLoader_train = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, drop_last= False)\n","\n","xs, ys = train_ds[0:10000]\n","\n","torch.manual_seed(42)\n","\n","test_ds = CTDataset_all('./test.pt', AE_model)\n","\n","DataLoader_test = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=True, drop_last= False, )\n","\n","G_image = Generator_image(latent_space_dim,  noise_dimension_image).to(device)\n","G_image.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_image.pth', weights_only=True))\n","\n","G_label = Generator(input_dimension = latent_space_dim, output_dimension = 10, noise_dimension = noise_dimension_label,\n","          hidden_layer_size = 512, BN_type = True, ReLU_coef = 0.5, drop_out_p= 0.2).to(device)\n","G_label.load_state_dict(torch.load('./AE'+str(latent_space_dim)+'_label.pth', weights_only=True))\n","\n","M = 100\n","test_size = 10000\n","Total_num_p_val = 40\n","\n","z_all = torch.zeros(test_size, latent_space_dim)\n","x_all = torch.zeros(test_size, 28*28)\n","y_all = torch.zeros(test_size, 10)\n","\n","\n","for i, (z_test, y_test, x_test) in tqdm(enumerate(DataLoader_test)):\n","    x_all[i,:] = x_test\n","    y_all[i,:] = y_test\n","    z_all[i,:] = z_test\n","\n","n_length_input = int(test_size/Total_num_p_val)\n","p_val_list = []\n","\n","for i in tqdm(range(0, Total_num_p_val)):\n","    n_length = n_length_input\n","    start_index = n_length_input*(i)\n","    end_index = start_index + n_length\n","\n","    x_all_in = x_all[start_index:end_index,].to(device).detach()\n","    y_all_in = y_all[start_index:end_index,].to(device).detach()\n","    z_all_in = z_all[start_index:end_index,].to(device).detach()\n","\n","    p_val = dgcit(x_all_in, y_all_in, z_all_in, G_image, G_label, test_size=n_length_input, z_dim=latent_space_dim,\n","      x_dims=28*28, y_dims=10, M=100, k=1, b=30, j=1000)\n","\n","    # print(\"the \",start_index,\" has p value: \",p_val)\n","    p_val_list.append(p_val)\n","p_lower, p_med, p_higher = np.quantile(p_val_list, 0.25), np.median(p_val_list), np.quantile(p_val_list, 0.75)\n","print(\"the latent_space is: \", latent_space_dim ,\"\\n\")\n","print(\"the lower bound is: \", p_lower, \"the median is: \", p_med, \"the higher bound is: \", p_higher,\"\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}