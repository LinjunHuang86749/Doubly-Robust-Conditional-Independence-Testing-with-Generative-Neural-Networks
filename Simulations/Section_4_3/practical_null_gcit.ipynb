{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6fkoQPNyLf0","outputId":"08c9aa77-7203-42ae-9cfb-41f2dc527efc","executionInfo":{"status":"ok","timestamp":1709688837884,"user_tz":360,"elapsed":577694,"user":{"displayName":"Linjun Huang","userId":"04332996936315086074"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--- The 1'th iteration take 0:00:14.521534 seconds ---\n","The stat is 0.2809999883174896\n","Type 1 error: 0.0 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.0 for z dimension 1 with significance level 0.05\n","--- The 2'th iteration take 0:00:11.209522 seconds ---\n","The stat is 0.22300000488758087\n","Type 1 error: 0.0 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.0 for z dimension 1 with significance level 0.05\n","--- The 3'th iteration take 0:00:11.259131 seconds ---\n","The stat is 0.49000000953674316\n","Type 1 error: 0.0 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.0 for z dimension 1 with significance level 0.05\n","--- The 4'th iteration take 0:00:11.272632 seconds ---\n","The stat is 0.019999999552965164\n","Type 1 error: 0.25 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.25 for z dimension 1 with significance level 0.05\n","--- The 5'th iteration take 0:00:11.493549 seconds ---\n","The stat is 0.26100000739097595\n","Type 1 error: 0.2 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.2 for z dimension 1 with significance level 0.05\n","--- The 6'th iteration take 0:00:11.104738 seconds ---\n","The stat is 0.3400000035762787\n","Type 1 error: 0.16666666666666666 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.16666666666666666 for z dimension 1 with significance level 0.05\n","--- The 7'th iteration take 0:00:11.081037 seconds ---\n","The stat is 0.48500001430511475\n","Type 1 error: 0.14285714285714285 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.14285714285714285 for z dimension 1 with significance level 0.05\n","--- The 8'th iteration take 0:00:11.204125 seconds ---\n","The stat is 0.07900000363588333\n","Type 1 error: 0.125 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.125 for z dimension 1 with significance level 0.05\n","--- The 9'th iteration take 0:00:11.137136 seconds ---\n","The stat is 0.28299999237060547\n","Type 1 error: 0.1111111111111111 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.1111111111111111 for z dimension 1 with significance level 0.05\n","--- The 10'th iteration take 0:00:11.240337 seconds ---\n","The stat is 0.15600000321865082\n","Type 1 error: 0.1 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.1 for z dimension 1 with significance level 0.05\n","--- The 11'th iteration take 0:00:11.277101 seconds ---\n","The stat is 0.12800000607967377\n","Type 1 error: 0.09090909090909091 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.09090909090909091 for z dimension 1 with significance level 0.05\n","--- The 12'th iteration take 0:00:11.168005 seconds ---\n","The stat is 0.4129999876022339\n","Type 1 error: 0.08333333333333333 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.08333333333333333 for z dimension 1 with significance level 0.05\n","--- The 13'th iteration take 0:00:11.215135 seconds ---\n","The stat is 0.2329999953508377\n","Type 1 error: 0.07692307692307693 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.07692307692307693 for z dimension 1 with significance level 0.05\n","--- The 14'th iteration take 0:00:11.237219 seconds ---\n","The stat is 0.4350000023841858\n","Type 1 error: 0.07142857142857142 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.07142857142857142 for z dimension 1 with significance level 0.05\n","--- The 15'th iteration take 0:00:11.181679 seconds ---\n","The stat is 0.31299999356269836\n","Type 1 error: 0.06666666666666667 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.06666666666666667 for z dimension 1 with significance level 0.05\n","--- The 16'th iteration take 0:00:11.161160 seconds ---\n","The stat is 0.050999999046325684\n","Type 1 error: 0.0625 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.0625 for z dimension 1 with significance level 0.05\n","--- The 17'th iteration take 0:00:11.088515 seconds ---\n","The stat is 0.05900000035762787\n","Type 1 error: 0.058823529411764705 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.058823529411764705 for z dimension 1 with significance level 0.05\n","--- The 18'th iteration take 0:00:11.297774 seconds ---\n","The stat is 0.03200000151991844\n","Type 1 error: 0.1111111111111111 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05555555555555555 for z dimension 1 with significance level 0.05\n","--- The 19'th iteration take 0:00:11.130590 seconds ---\n","The stat is 0.2199999988079071\n","Type 1 error: 0.10526315789473684 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05263157894736842 for z dimension 1 with significance level 0.05\n","--- The 20'th iteration take 0:00:11.129853 seconds ---\n","The stat is 0.4300000071525574\n","Type 1 error: 0.1 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05 for z dimension 1 with significance level 0.05\n","--- The 21'th iteration take 0:00:11.161674 seconds ---\n","The stat is 0.48100000619888306\n","Type 1 error: 0.09523809523809523 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.047619047619047616 for z dimension 1 with significance level 0.05\n","--- The 22'th iteration take 0:00:11.054124 seconds ---\n","The stat is 0.3050000071525574\n","Type 1 error: 0.09090909090909091 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.045454545454545456 for z dimension 1 with significance level 0.05\n","--- The 23'th iteration take 0:00:11.126445 seconds ---\n","The stat is 0.06199999898672104\n","Type 1 error: 0.08695652173913043 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.043478260869565216 for z dimension 1 with significance level 0.05\n","--- The 24'th iteration take 0:00:11.217959 seconds ---\n","The stat is 0.0689999982714653\n","Type 1 error: 0.08333333333333333 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.041666666666666664 for z dimension 1 with significance level 0.05\n","--- The 25'th iteration take 0:00:11.181559 seconds ---\n","The stat is 0.22599999606609344\n","Type 1 error: 0.08 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.04 for z dimension 1 with significance level 0.05\n","--- The 26'th iteration take 0:00:11.127146 seconds ---\n","The stat is 0.36000001430511475\n","Type 1 error: 0.07692307692307693 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.038461538461538464 for z dimension 1 with significance level 0.05\n","--- The 27'th iteration take 0:00:11.147023 seconds ---\n","The stat is 0.36000001430511475\n","Type 1 error: 0.07407407407407407 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.037037037037037035 for z dimension 1 with significance level 0.05\n","--- The 28'th iteration take 0:00:11.153192 seconds ---\n","The stat is 0.4390000104904175\n","Type 1 error: 0.07142857142857142 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.03571428571428571 for z dimension 1 with significance level 0.05\n","--- The 29'th iteration take 0:00:11.227104 seconds ---\n","The stat is 0.13199999928474426\n","Type 1 error: 0.06896551724137931 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.034482758620689655 for z dimension 1 with significance level 0.05\n","--- The 30'th iteration take 0:00:11.182762 seconds ---\n","The stat is 0.29600000381469727\n","Type 1 error: 0.06666666666666667 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.03333333333333333 for z dimension 1 with significance level 0.05\n","--- The 31'th iteration take 0:00:11.181783 seconds ---\n","The stat is 0.3160000145435333\n","Type 1 error: 0.06451612903225806 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.03225806451612903 for z dimension 1 with significance level 0.05\n","--- The 32'th iteration take 0:00:11.308001 seconds ---\n","The stat is 0.1469999998807907\n","Type 1 error: 0.0625 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.03125 for z dimension 1 with significance level 0.05\n","--- The 33'th iteration take 0:00:11.361898 seconds ---\n","The stat is 0.11699999868869781\n","Type 1 error: 0.06060606060606061 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.030303030303030304 for z dimension 1 with significance level 0.05\n","--- The 34'th iteration take 0:00:11.257165 seconds ---\n","The stat is 0.012000000104308128\n","Type 1 error: 0.08823529411764706 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.058823529411764705 for z dimension 1 with significance level 0.05\n","--- The 35'th iteration take 0:00:11.418003 seconds ---\n","The stat is 0.12999999523162842\n","Type 1 error: 0.08571428571428572 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05714285714285714 for z dimension 1 with significance level 0.05\n","--- The 36'th iteration take 0:00:11.606999 seconds ---\n","The stat is 0.45399999618530273\n","Type 1 error: 0.08333333333333333 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05555555555555555 for z dimension 1 with significance level 0.05\n","--- The 37'th iteration take 0:00:11.698646 seconds ---\n","The stat is 0.2930000126361847\n","Type 1 error: 0.08108108108108109 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05405405405405406 for z dimension 1 with significance level 0.05\n","--- The 38'th iteration take 0:00:11.638000 seconds ---\n","The stat is 0.39899998903274536\n","Type 1 error: 0.07894736842105263 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05263157894736842 for z dimension 1 with significance level 0.05\n","--- The 39'th iteration take 0:00:11.673403 seconds ---\n","The stat is 0.07400000095367432\n","Type 1 error: 0.07692307692307693 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05128205128205128 for z dimension 1 with significance level 0.05\n","--- The 40'th iteration take 0:00:11.788655 seconds ---\n","The stat is 0.328000009059906\n","Type 1 error: 0.075 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.05 for z dimension 1 with significance level 0.05\n","--- The 41'th iteration take 0:00:11.486688 seconds ---\n","The stat is 0.2199999988079071\n","Type 1 error: 0.07317073170731707 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.04878048780487805 for z dimension 1 with significance level 0.05\n","--- The 42'th iteration take 0:00:11.515370 seconds ---\n","The stat is 0.460999995470047\n","Type 1 error: 0.07142857142857142 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.047619047619047616 for z dimension 1 with significance level 0.05\n","--- The 43'th iteration take 0:00:11.513138 seconds ---\n","The stat is 0.49000000953674316\n","Type 1 error: 0.06976744186046512 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.046511627906976744 for z dimension 1 with significance level 0.05\n","--- The 44'th iteration take 0:00:11.485156 seconds ---\n","The stat is 0.05999999865889549\n","Type 1 error: 0.06818181818181818 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.045454545454545456 for z dimension 1 with significance level 0.05\n","--- The 45'th iteration take 0:00:11.669085 seconds ---\n","The stat is 0.49799999594688416\n","Type 1 error: 0.06666666666666667 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.044444444444444446 for z dimension 1 with significance level 0.05\n","--- The 46'th iteration take 0:00:11.747872 seconds ---\n","The stat is 0.20100000500679016\n","Type 1 error: 0.06521739130434782 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.043478260869565216 for z dimension 1 with significance level 0.05\n","--- The 47'th iteration take 0:00:11.808003 seconds ---\n","The stat is 0.37299999594688416\n","Type 1 error: 0.06382978723404255 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.0425531914893617 for z dimension 1 with significance level 0.05\n","--- The 48'th iteration take 0:00:11.462382 seconds ---\n","The stat is 0.10999999940395355\n","Type 1 error: 0.0625 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.041666666666666664 for z dimension 1 with significance level 0.05\n","--- The 49'th iteration take 0:00:11.588210 seconds ---\n","The stat is 0.25699999928474426\n","Type 1 error: 0.061224489795918366 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.04081632653061224 for z dimension 1 with significance level 0.05\n","--- The 50'th iteration take 0:00:11.553264 seconds ---\n","The stat is 0.10100000351667404\n","Type 1 error: 0.06 for z dimension 1 with significance level 0.1\n","Type 1 error: 0.04 for z dimension 1 with significance level 0.05\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0.28099999, 0.223     , 0.49000001, 0.02      , 0.26100001,\n","       0.34      , 0.48500001, 0.079     , 0.28299999, 0.156     ,\n","       0.12800001, 0.41299999, 0.233     , 0.435     , 0.31299999,\n","       0.051     , 0.059     , 0.032     , 0.22      , 0.43000001,\n","       0.48100001, 0.30500001, 0.062     , 0.069     , 0.226     ,\n","       0.36000001, 0.36000001, 0.43900001, 0.132     , 0.296     ,\n","       0.31600001, 0.147     , 0.117     , 0.012     , 0.13      ,\n","       0.454     , 0.29300001, 0.39899999, 0.074     , 0.32800001,\n","       0.22      , 0.461     , 0.49000001, 0.06      , 0.498     ,\n","       0.20100001, 0.373     , 0.11      , 0.257     , 0.101     ])"]},"metadata":{},"execution_count":1}],"source":["param = {\n","    \"test\": \"type1error\",  # ['type1error', 'power']\n","    \"sample_size\": 200,  # [200, 400, 600, 800, 1000]\n","    \"batch_size\": 256,  # [32, 64, 128, 256]\n","    \"z_dim\": 1,  # [5, 50, 250]\n","    \"dx\": 1,\n","    \"dy\": 1,\n","    \"n_test\": 500,  # [200, 2000]\n","    \"epochs_num\": 500,  # [1000, 1500]\n","    \"eps_std\": 0.5,\n","    \"dist_z\": 'gaussian',  # ['laplace', 'gaussian']\n","    \"alpha_x\": 0.75,  # only used under alternative\n","    \"m_value\": 100,  # [100, 200]\n","    \"k_value\": 2,  # [1, 2, 4]\n","    \"j_value\": 1000,  # [1000, 2000]\n","    \"noise_dimension\": 3,  # [5, 10, 20]\n","    \"hidden_layer_size\": 50,  # [64, 128, 256, 512, 1024]\n","    \"normal_ini\": False,  # [True, False]\n","    \"preprocess\": 'normalize',  # ['normalize',  'scale_Z', 'None' ]\n","    \"G_lr\": 5e-2,  # [5e-6, 1e-5, 2e-5， 5e-5]\n","    \"alpha\": 0.1,\n","    \"alpha1\": 0.05,\n","    \"set_seeds\": 42,\n","    \"using_orcale\": False,\n","    \"lambda_1\": 1,  # loss with Laplace kernel\n","    \"lambda_2\": 0,  # loss with Gaussian kernel\n","    \"using_Gen\": '1',  # ['1', '2'], types of generator \"1\" is fully connect, \"2\" is non fully\n","    \"boor_rv_type\": 'gaussian',  # ['rademacher', 'gaussian']\n","    \"wgt_decay\": 1e-5,  # weight decay for adam optimizer L2 regularization parameter\n","    \"lambda_3\": 1e-5,  # L1 regularization parameter\n","    \"drop_out_p\": 0.2,  # probability of an element to be zeroed. Default: 0.5, best 0.2\n","    \"M_train\": 10\n","}\n","\n","import torch\n","import torch.distributions as TD\n","from torch.utils.data import Dataset, DataLoader\n","from zmq import device\n","import torch.optim as optim\n","import numpy as np\n","from datetime import datetime\n","import functools\n","import tensorflow as tf\n","from scipy.stats import rankdata, ks_2samp, wilcoxon\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","\n","def generate_samples_random(Ax, Ay, size=1000, sType='CI', dx=1, dy=1, dz=20, nstd=0.05, alpha_x=0.05,\n","                            preprocess=\"None\", dist_z='gaussian'):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    numbers_z = np.random.multinomial(num, [1 / 2.] * 2, size=1)\n","    number_z_zeros = numbers_z[0][0]\n","    number_z_ones = numbers_z[0][1]\n","\n","    xy_arr = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    xy_z_zero_index = np.random.choice(a = len(xy_arr), size=number_z_zeros, p=[1/4., 1/4., 1/4., 1/4.])\n","    xy_z_one_index = np.random.choice(a = len(xy_arr), size=number_z_ones, p=[1/4., 1/4., 1/4., 1/4.])\n","\n","    xy_z_zero = xy_arr[xy_z_zero_index]\n","    xy_z_one = xy_arr[xy_z_one_index]\n","\n","    xy = np.concatenate((xy_z_zero, xy_z_one), axis=0)\n","\n","    x = xy[:, 0]\n","    y = xy[:, 1]\n","    z = np.concatenate((np.zeros(number_z_zeros), np.ones(number_z_ones)), axis=0)\n","\n","    indices = np.random.permutation(num)\n","\n","    x, y, z = x[indices], y[indices], z[indices]\n","    X, Y, Z = x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)\n","\n","    if preprocess == \"normalize\":\n","        Z = (Z - Z.min()) / (Z.max() - Z.min())\n","        X = (X - X.min()) / (X.max() - X.min())\n","        Y = (Y - Y.min()) / (Y.max() - Y.min())\n","\n","    elif preprocess == \"scale_Z\":\n","        Z = Z / Z.max()\n","\n","    elif preprocess == \"None\":\n","        X, Y, Z = X, Y, Z\n","\n","    X, Y, Z = torch.from_numpy(np.array(X)).float(), torch.from_numpy(np.array(Y)).float(), torch.from_numpy(\n","        np.array(Z)).float()\n","    return X, Y, Z\n","\n","\n","def generate_samples_from_fixed_Z_random(Ax, Ay, Z, size=1000, sType='CI', dx=1, dy=1, dz=20, nstd=0.05, alpha_x=0.05,\n","                                         normalize=True, seed=None, dist_z='gaussian'):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    error_generator_x = TD.MultivariateNormal(\n","        torch.zeros(dx), 1 * torch.eye(dx))\n","\n","    error_generator_y = TD.MultivariateNormal(\n","        torch.zeros(dy), 1 * torch.eye(dy))\n","\n","    Axy = torch.ones((dx, dy)) * alpha_x\n","\n","    if sType == 'CI':\n","        X = torch.sin(torch.matmul(Z, Ax) + nstd * error_generator_x.sample(\n","            (num,)))  ##variance is 1, not 0.25, as mentioned in the paper\n","        Y = torch.cos(torch.matmul(Z, Ay) + nstd * error_generator_y.sample((num,)))\n","    elif sType == 'I':\n","        X = torch.sin(nstd * error_generator_x.sample((num,)))\n","        Y = torch.cos(nstd * error_generator_y.sample((num,)))\n","    else:\n","        X = torch.sin(torch.matmul(Z, Ax) + nstd * error_generator_x.sample((num,)))\n","        Y = torch.cos(\n","            torch.matmul(torch.sin(torch.matmul(Z, Ax) + nstd * error_generator_x.sample((num,))), Axy) + torch.matmul(\n","                Z, Ay) + nstd * error_generator_y.sample((num,)))\n","\n","    return X, Y\n","\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u=1,\n","                       sigma_v=1,\n","                       boor_rv_type=\"gaussian\"):\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1,\n","                                    dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.abs(y_torch.repeat(1, n) - y_torch.repeat(1, n).T) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.abs(gen_y_all_torch.repeat(n, 1, 1) - y_torch.repeat(1, n).reshape(n, n, 1)) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1)\n","\n","    temp_mx = gen_y_all_torch_rep[:, :, 0].T\n","    sum_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","\n","    v_mx_1 = torch.exp(-torch.abs(x_torch.repeat(1, n) - x_torch.repeat(1, n).T) / sigma_v)\n","    v_mx_2 = torch.mean(\n","        torch.exp(-torch.abs(gen_x_all_torch.repeat(n, 1, 1) - x_torch.repeat(1, n).reshape(n, n, 1)) / sigma_v), dim=2)\n","    v_mx_3 = v_mx_2.T\n","\n","    gen_x_all_torch_rep = gen_x_all_torch.repeat(n, 1, 1)\n","\n","    temp2_mx = gen_x_all_torch_rep[:, :, 0].T\n","    sum2_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n, n, 1)) / sigma_v), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = gen_y_all_torch_rep[:, :, i].T\n","        temp_add_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","        temp2_mx = gen_x_all_torch_rep[:, :, i].T\n","        temp2_add_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n, n, 1)) / sigma_v),\n","                                  dim=2)\n","        sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1 / M * sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    stat = 1 / (n - 1) * torch.sum(FF_mx).item()\n","\n","    # print(\"U_mx:\", u_mx)\n","    # print(\"V_mx:\", v_mx)\n","    # print(\"W_mx:\", w_mx)\n","    boottemp = np.array([])\n","    if boor_rv_type == \"rademacher\":\n","        eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","        eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","        random_mx = torch.matmul(eboot[:, bb].reshape(-1, 1), eboot[:, bb].reshape(-1, 1).T)\n","        bootmatrix = FF_mx * random_mx\n","        stat_boot = 1 / (n - 1) * torch.sum(bootmatrix).item()\n","        boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","\n","class DatasetSelect(Dataset):\n","    def __init__(self, X, Y, Z):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index]\n","\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","    \"\"\"\n","      Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","      Input:\n","      - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","      - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","      - batch_size: Integer giving the batch size.\n","    \"\"\"\n","\n","    def __init__(self, X, Y, Z, batch_size):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.batch_size = batch_size\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[\n","            (self.batch_size + index) % self.sample_size]\n","\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN_ver2(torch.utils.data.Dataset):\n","    \"\"\"\n","      Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","      Input:\n","      - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","      - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","      - batch_size: Integer giving the batch size.\n","    \"\"\"\n","\n","    def __init__(self, Y, Z, batch_size):\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.batch_size = batch_size\n","        self.sample_size = Z.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.Y_real[index], self.Z_real[index]\n","\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","        noise_generator = TD.MultivariateNormal(\n","            torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","        Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","        Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","        Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","\n","##### GAN architecture #####\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input X.\n","    - output_dimension: Integer giving the dimension of output Y.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the discriminator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef,\n","                 drop_out_p,\n","                 drop_input=False):\n","        super(Generator, self).__init__()\n","        self.BN_type = BN_type\n","        self.ReLU_coef = ReLU_coef\n","        self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","        if BN_type:\n","            self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","            self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","            self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","        self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","        self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","        self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","        self.sigmoid = torch.nn.Sigmoid()\n","        self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_input = drop_input\n","\n","    def forward(self, x):\n","        if self.BN_type:\n","            if self.drop_input:\n","                x = self.drop_out0(x)\n","            x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","            x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","            # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","            x = self.fc_last(x)\n","        else:\n","            if self.drop_input:\n","                x = self.drop_out0(x)\n","            x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","            x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","            # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","            x = self.fc_last(x)\n","            x = self.sigmoid(x)\n","            # x = (x > 0.5).float()\n","        return x\n","\n","\n","class NonFullyConnected_1(torch.nn.Module):\n","\n","    def __init__(self, size_in, size_out, m, bias=True):\n","        super(NonFullyConnected_1, self).__init__()\n","        self.linear = torch.nn.Linear(m * size_in, m * size_out, bias=bias).to(device)\n","        self.mask = functools.reduce(torch.block_diag, [torch.ones(size_out, size_in) for i in range(m)]).to(device)\n","\n","    def forward(self, x):\n","        self.linear.weight.data *= self.mask\n","        return self.linear(x)\n","\n","\n","class Generator_2(torch.nn.Module):\n","    def __init__(\n","            self,\n","            input_dimension,\n","            output_dimension,\n","            noise_dimension,\n","            hidden_layer_size,\n","            BN_type,\n","            ReLU_coef,\n","            hidden_layer_depth=1,\n","            ntargets_k=5):\n","        super(Generator_2, self).__init__()\n","        self.input_dimension = input_dimension + noise_dimension\n","        self.output_dimension = output_dimension\n","        self.ntargets_k = ntargets_k\n","        self.hidden_layer_sizes = [hidden_layer_size] * hidden_layer_depth\n","        self.BN_type = BN_type\n","        self.leakyrelu = torch.nn.LeakyReLU(ReLU_coef)\n","        self.linear_layers_from_input = torch.nn.Linear(self.input_dimension, ntargets_k * self.hidden_layer_sizes[0])\n","\n","        self.linear_layers_between = torch.nn.ModuleList([\n","            NonFullyConnected_1(self.hidden_layer_sizes[0], self.hidden_layer_sizes[0], ntargets_k)\n","            for i in range(len(self.hidden_layer_sizes))\n","        ])\n","        # self.linear8 = torch.nn.Linear(self.hidden_layer_sizes[0]*ntargets_k, self.hidden_layer_sizes[0]*ntargets_k)\n","        # self.linear8.weight = torch.nn.Parameter(torch.eye(self.hidden_layer_sizes[0]*ntargets_k), requires_grad=False)\n","        self.linear8 = torch.nn.Linear(self.hidden_layer_sizes[0] * ntargets_k, self.output_dimension)\n","        if BN_type:\n","            self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","\n","    def forward(self, input):\n","        if self.BN_type:\n","            output = self.linear_layers_from_input(input)\n","            output = self.leakyrelu(self.BN1(output))\n","            for linear_layers_between in self.linear_layers_between:\n","                output = linear_layers_between(output)\n","                output = self.leakyrelu(self.BN1(output))\n","        else:\n","            output = self.linear_layers_from_input(input)\n","            output = self.leakyrelu(output)\n","            for linear_layers_between in self.linear_layers_between:\n","                output = linear_layers_between(output)\n","                output = self.leakyrelu(output)\n","\n","        return self.linear8(output)  # torch.mean(self.linear8(output)).reshape(1)\n","\n","\n","##### Training procedures #####\n","\n","\n","def find_loss(y_torch, gen_y_all_torch, z_torch, sigma_w, sigma_u, M):\n","    n = z_torch.shape[0]\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1,\n","                                    dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.abs(y_torch.repeat(1, n) - y_torch.repeat(1, n).T) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.abs(gen_y_all_torch.repeat(n, 1, 1) - y_torch.repeat(1, n).reshape(n, n, 1)) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1)\n","\n","    temp_mx = gen_y_all_torch_rep[:, :, 0].T\n","    sum_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = gen_y_all_torch_rep[:, :, i].T\n","        temp_add_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","\n","    FF_mx = u_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    loss = 1 / (n) * torch.sum(FF_mx)\n","    return loss\n","\n","\n","def train_ver3(X, Y, Z, X_test, Y_test, Z_test, M,\n","               noise_dimension, noise_type, G_lr, hidden_layer_size,\n","               DataLoader, BN_type, ReLU_coef,\n","               epochs_num=10, sigma_z=1, sigma_x=1, sigma_y=1,\n","               normal_ini=False,\n","               lambda_1=1, lambda_2=1, using_Gen='1', wgt_decay=0,\n","               lambda_3=0, drop_out_p=0.2, M_train=3):\n","    \"\"\"\n","    Train loop for GAN.\n","\n","    Inputs:\n","    - X: PyTorch Tensor (sample_size, input_dimension) of training input.\n","    - Y: PyTorch Tensor (sample_size, output_dimension) of training output.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","    - D_lr, G_lr: Float giving the learning rate of the discriminator and\n","      the generator.\n","    - discriminator_type, generator_type: (\"KL\", \"JS\", \"WS\", \"LS\"), giving the loss criterion\n","      of the discriminator and generator, respectively.\n","    - discriminator_loss, generator_loss: Functions to use for computing the\n","      generator and discriminator loss, respectively.\n","    - DataLoader: DataLoader object used to generate training batches.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","    - batch_size: Integer giving the size of batches for each epoch.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - lambda_gp: Float giving the coefficient of gradient penalty for WS.\n","\n","    Outputs:\n","    - D: PyTorch Net giving the trained discriminator.\n","    - G: PyTorch Net giving the trained generator.\n","    - Output the trained D and G at 250, 500, 750, 1000 epochs.\n","    \"\"\"\n","\n","    input_dimension = Z.shape[1]\n","    output_dimension_y = Y.shape[1]\n","    output_dimension_x = X.shape[1]\n","\n","    if using_Gen == '1':\n","\n","        G_zy = Generator(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type, ReLU_coef,\n","                         drop_out_p).to(device)\n","        G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","        G_zx = Generator(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type, ReLU_coef,\n","                         drop_out_p).to(device)\n","        G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    elif using_Gen == '2':\n","        G_zy = Generator_2(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type,\n","                           ReLU_coef).to(device)\n","        G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","        G_zx = Generator_2(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type,\n","                           ReLU_coef).to(device)\n","        G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    iter_count = 0\n","    G_zy = G_zy.train()\n","    G_zx = G_zx.train()\n","\n","    if normal_ini:\n","        for p in G_zy.parameters():\n","            p.data = torch.randn(\n","                p.shape, device=device,\n","                dtype=torch.float32) / np.sqrt(float(hidden_layer_size * 2))\n","\n","        for p in G_zx.parameters():\n","            p.data = torch.randn(\n","                p.shape, device=device,\n","                dtype=torch.float32) / np.sqrt(float(hidden_layer_size * 2))\n","\n","    for epoch in range(epochs_num):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zy = G_zy.train()\n","        G_zx = G_zx.train()\n","        for X_real, Y_real, Z_real, Z_fake in DataLoader:\n","            X_real = X_real.to(device)\n","            Y_real = Y_real.to(device)\n","            Z_real = Z_real.to(device)\n","            Z_fake = Z_fake.to(device)\n","\n","            batch_size = Z_real.shape[0]\n","            Z_real_repeat = Z_real.repeat(M_train, 1)\n","\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","                device)\n","            X_fake = G_zx(torch.cat((Z_real_repeat, Noise_fake), dim=1)).to(device)\n","\n","            X_fake = X_fake.reshape(batch_size, M_train)\n","\n","            standardise = False\n","\n","            if standardise:\n","                Y_fake = (Y_fake - torch.mean(Y_fake, dim=0, keepdim=True)) / torch.std(Y_fake, dim=0, keepdim=True)\n","                X_fake = (X_fake - torch.mean(X_fake, dim=0, keepdim=True)) / torch.std(X_fake, dim=0, keepdim=True)\n","                X_real = (X_real - torch.mean(X_real, dim=0, keepdim=True)) / torch.std(X_real, dim=0, keepdim=True)\n","                Y_real = (Y_real - torch.mean(Y_real, dim=0, keepdim=True)) / torch.std(Y_real, dim=0, keepdim=True)\n","                Z_real = (Z_real - torch.mean(Z_real, dim=0, keepdim=True)) / torch.std(Z_real, dim=0, keepdim=True)\n","\n","            # Generator step\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            l1_regularization = 0\n","\n","            for param in G_zx.parameters():\n","                l1_regularization += torch.linalg.vector_norm(param, ord=1)\n","\n","            g_zx_error = lambda_1 * find_loss(X_real, X_fake, Z_real, sigma_z, sigma_x,\n","                                              M_train) + lambda_3 * l1_regularization\n","\n","            g_zx_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zx.parameters(), max_norm=0.5)\n","            G_zx_solver.step()\n","\n","            iter_count += 1\n","            batch_count += 1\n","\n","        # G_zx = G_zx.eval()\n","        # G_zy = G_zy.eval()\n","        # if ((epoch + 1) % 100 == 0):\n","        #     dataset_test = DatasetSelect(X_test, Y_test, Z_test)\n","        #     dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=True)\n","\n","        #     test_size = Z_test.shape[0]\n","        #     gen_x_all = torch.zeros(test_size, M)\n","        #     gen_y_all = torch.zeros(test_size, M)\n","        #     z_all = torch.zeros(test_size, input_dimension)\n","        #     x_all = torch.zeros(test_size, output_dimension_y)\n","        #     y_all = torch.zeros(test_size, output_dimension_x)\n","\n","        #     cur_itr = 0\n","\n","        #     for i, (x_test, y_test, z_test) in enumerate(dataloader_test):\n","        #         z_test_temp = z_test.repeat(M, 1).to(device)\n","        #         Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","        #             device)\n","        #         fake_x = G_zx(torch.cat((z_test_temp, Noise_fake), dim=1)).reshape(1, -1)\n","\n","        #         Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","        #             device)\n","        #         fake_y = G_zy(torch.cat((z_test_temp, Noise_fake), dim=1)).reshape(1, -1)\n","\n","        #         gen_x_all[cur_itr, :] = fake_x.detach().reshape(-1)\n","        #         gen_y_all[cur_itr, :] = fake_y.detach().reshape(-1)\n","        #         x_all[cur_itr, :] = x_test\n","        #         y_all[cur_itr, :] = y_test\n","        #         z_all[cur_itr, :] = z_test\n","        #         cur_itr = cur_itr + 1\n","        #     gen_x_mean = torch.mean(gen_x_all, dim=1).reshape(-1, 1)\n","        #     gen_y_mean = torch.mean(gen_y_all, dim=1).reshape(-1, 1)\n","\n","        #     mse_x = torch.mean((gen_x_mean - x_all) ** 2).item()\n","        #     mse_y = torch.mean((gen_y_mean - y_all) ** 2).item()\n","\n","        #     print(f'Epoch [{epoch + 1}/{epochs_num}], test MSE x [{mse_x}], MSE y [{mse_y}]')\n","\n","        #     dataset_test = DatasetSelect(X, Y, Z)\n","        #     dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=True)\n","\n","        #     test_size = Z_test.shape[0]\n","        #     gen_x_all = torch.zeros(test_size, M)\n","        #     gen_y_all = torch.zeros(test_size, M)\n","        #     z_all = torch.zeros(test_size, input_dimension)\n","        #     x_all = torch.zeros(test_size, output_dimension_y)\n","        #     y_all = torch.zeros(test_size, output_dimension_x)\n","\n","        #     cur_itr = 0\n","\n","        #     for i, (x_test, y_test, z_test) in enumerate(dataloader_test):\n","        #         z_test_temp = z_test.repeat(M, 1).to(device)\n","        #         Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","        #             device)\n","        #         fake_x = G_zx(torch.cat((z_test_temp, Noise_fake), dim=1)).reshape(1, -1)\n","\n","        #         Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","        #             device)\n","        #         fake_y = G_zy(torch.cat((z_test_temp, Noise_fake), dim=1)).reshape(1, -1)\n","\n","        #         gen_x_all[cur_itr, :] = fake_x.detach().reshape(-1)\n","        #         gen_y_all[cur_itr, :] = fake_y.detach().reshape(-1)\n","        #         x_all[cur_itr, :] = x_test\n","        #         y_all[cur_itr, :] = y_test\n","        #         z_all[cur_itr, :] = z_test\n","        #         cur_itr = cur_itr + 1\n","        #     gen_x_mean = torch.mean(gen_x_all, dim=1).reshape(-1, 1)\n","        #     gen_y_mean = torch.mean(gen_y_all, dim=1).reshape(-1, 1)\n","\n","        #     mse_x = torch.mean((gen_x_mean - x_all) ** 2).item()\n","        #     mse_y = torch.mean((gen_y_mean - y_all) ** 2).item()\n","\n","        #     print(f'Epoch [{epoch + 1}/{epochs_num}], train MSE x [{mse_x}], MSE y [{mse_y}]')\n","    return G_zy, G_zx\n","\n","def rdc(x, y, f=np.sin, k=20, s=1 / 6., n=1):\n","    \"\"\"\n","    Computes the Randomized Dependence Coefficient\n","    x,y: numpy arrays 1-D or 2-D\n","         If 1-D, size (samples,)\n","         If 2-D, size (samples, variables)\n","    f:   function to use for random projection\n","    k:   number of random projections to use\n","    s:   scale parameter\n","    n:   number of times to compute the RDC and\n","         return the median (for stability)\n","    According to the paper, the coefficient should be relatively insensitive to\n","    the settings of the f, k, and s parameters.\n","\n","    Source: https://github.com/garydoranjr/rdc\n","    \"\"\"\n","    x = tf.reshape(x, shape=(x.shape[0], ))\n","    y = tf.reshape(y, shape=(y.shape[0], ))\n","\n","    if n > 1:\n","        values = []\n","        for i in range(n):\n","            try:\n","                values.append(rdc(x, y, f, k, s, 1))\n","            except np.linalg.linalg.LinAlgError:\n","                pass\n","        return np.median(values)\n","\n","    if len(x.shape) == 1: x = tf.reshape(x, shape=(-1, 1))\n","    if len(y.shape) == 1: y = tf.reshape(y, shape=(-1, 1))\n","\n","    # Copula Transformation\n","    cx = np.column_stack([rankdata(xc, method='ordinal') for xc in np.transpose(x)]) / float(x.shape[0])\n","    cy = np.column_stack([rankdata(yc, method='ordinal') for yc in np.transpose(y)]) / float(y.shape[0])\n","\n","    # Add a vector of ones so that w.x + b is just a dot product\n","    O = np.ones(cx.shape[0])\n","    X = np.column_stack([cx, O])\n","    Y = np.column_stack([cy, O])\n","\n","    # Random linear projections\n","    Rx = (s / X.shape[1]) * np.random.randn(X.shape[1], k)\n","    Ry = (s / Y.shape[1]) * np.random.randn(Y.shape[1], k)\n","    X = np.dot(X, Rx)\n","    Y = np.dot(Y, Ry)\n","\n","    # Apply non-linear function to random projections\n","    fX = f(X)\n","    fY = f(Y)\n","\n","    # Compute full covariance matrix\n","    C = np.cov(np.hstack([fX, fY]).T)\n","\n","    # Due to numerical issues, if k is too large,\n","    # then rank(fX) < k or rank(fY) < k, so we need\n","    # to find the largest k such that the eigenvalues\n","    # (canonical correlations) are real-valued\n","    k0 = k\n","    lb = 1\n","    ub = k\n","    while True:\n","        # Compute canonical correlations\n","        Cxx = C[:k, :k]\n","        Cyy = C[k0:k0 + k, k0:k0 + k]\n","        Cxy = C[:k, k0:k0 + k]\n","        Cyx = C[k0:k0 + k, :k]\n","\n","        eigs = np.linalg.eigvals(np.dot(np.dot(np.linalg.pinv(Cxx), Cxy),\n","                                        np.dot(np.linalg.pinv(Cyy), Cyx)))\n","\n","        # Binary search if k is too large\n","        if not (np.all(np.isreal(eigs)) and\n","                0 <= np.min(eigs) and\n","                np.max(eigs) <= 1):\n","            ub -= 1\n","            k = (ub + lb) // 2\n","            continue\n","        if lb == ub: break\n","        lb = k\n","        if ub == lb + 1:\n","            k = ub\n","        else:\n","            k = (ub + lb) // 2\n","\n","    return np.sqrt(np.max(eigs))\n","\n","\n","\n","def GCIT(Ax, Ay, n=500, z_dim=100, simulation='type1error', batch_size=64, epochs_num=1000,\n","         nstd=1.0, z_dist='gaussian', x_dims=1, y_dims=1, a_x=0.05, M=500, k=2, boot_num=1000,\n","         noise_dimension=10, hidden_layer_size=512, normal_ini=False, preprocess='normalize',\n","         G_lr=1e-5, using_orcale=False, lambda_1=1, lambda_2=1, using_Gen='1',\n","         boor_rv_type=\"gaussian\", wgt_decay=0, lambda_3=1, drop_out_p=0.2, exp_num=0, M_train=3):\n","    if simulation == 'type1error':\n","        # generate samples x, y, z under null hypothesis - x and y are conditional independent\n","        sim_x, sim_y, sim_z = generate_samples_random(Ax, Ay, size=n, sType='CI', dx=x_dims, dy=y_dims, dz=z_dim,\n","                                                      nstd=nstd, alpha_x=a_x,\n","                                                      dist_z=z_dist, preprocess=preprocess)\n","\n","    elif simulation == 'power':\n","        # generate samples x, y, z under alternative hypothesis - x and y are dependent\n","        sim_x, sim_y, sim_z = generate_samples_random(Ax, Ay, size=n, sType='dependent', dx=x_dims, dy=y_dims, dz=z_dim,\n","                                                      nstd=nstd,\n","                                                      alpha_x=a_x, dist_z=z_dist, preprocess=preprocess)\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    x, y, z = sim_x, sim_y, sim_z\n","\n","    # w_mx = torch.linalg.vector_norm(z.repeat(n,1,1) - torch.swapaxes(z.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_w_train = 1.0  # torch.median(w_mx).item()\n","\n","    # u_mx = torch.abs(y.repeat(1, n) - y.repeat(1, n).T)\n","    sigma_u_train = 1.0  # torch.median(u_mx).item()\n","\n","    # v_mx = torch.abs(x.repeat(1, n) - x.repeat(1, n).T)\n","    sigma_v_train = 1.0  # torch.median(v_mx).item()\n","\n","    split_factor = 2/3.\n","\n","    X_test, Y_test, Z_test = x[int( n * split_factor ):int(n)], y[int(n * split_factor ):int(n)], z[int(n * split_factor ):int(n)]\n","    X_train, Y_train, Z_train = x[0:int( n * split_factor )], y[0:int(n * split_factor )], z[0:int(n * split_factor )]\n","\n","    if (k == 1):\n","        X_train, Y_train, Z_train = X_test, Y_test, Z_test\n","\n","    train_xyz = DatasetSelect_GAN(X_train, Y_train, Z_train, batch_size)\n","    DataLoader_xyz = torch.utils.data.DataLoader(train_xyz, batch_size=batch_size, shuffle=True)\n","    G_zy, G_zx = train_ver3(X=X_train, Y=Y_train, Z=Z_train, M=M,\n","                            X_test=X_test, Y_test=Y_test, Z_test=Z_test,\n","                            noise_dimension=noise_dimension, noise_type=\"normal\",\n","                            G_lr=G_lr, hidden_layer_size=hidden_layer_size,\n","                            DataLoader=DataLoader_xyz, BN_type=False, ReLU_coef=0.1,\n","                            epochs_num=epochs_num, sigma_z=sigma_w_train, sigma_x=sigma_v_train,\n","                            sigma_y=sigma_u_train,\n","                            normal_ini=normal_ini, lambda_1=lambda_1, lambda_2=lambda_2,\n","                            using_Gen=using_Gen, wgt_decay=wgt_decay, lambda_3=lambda_3,\n","                            drop_out_p=drop_out_p, M_train=M_train)\n","\n","    G_zx = G_zx.eval()\n","    G_zy = G_zy.eval()\n","\n","    test_samples = 1000\n","    rho = []\n","    test_size = Z_test.shape[0]\n","    y_test = Y_test.numpy()\n","    x_test = X_test.numpy()\n","\n","    for i in range(test_samples):\n","        Z_test = Z_test.to(device)\n","        Noise_fake = sample_noise(Z_test.shape[0], noise_dimension, \"normal\", input_var=1.0 / 3.0).to(device)\n","        fake_data = G_zx(torch.cat((Z_test, Noise_fake), dim=1)).to(device)\n","        fake_data = fake_data.reshape(test_size, 1).cpu().detach().numpy()\n","        rho.append(rdc(fake_data, y_test))\n","\n","    rho = tf.stack(rho)\n","    stat_real = rdc(x_test, y_test)\n","    # p-value computation as a two-sided test\n","    p_value = min(tf.reduce_sum(tf.cast(rho < stat_real, tf.float32)) / test_samples,\n","                  tf.reduce_sum(tf.cast(rho > stat_real, tf.float32)) / test_samples)\n","\n","    return p_value\n","\n","\n","def run_experiment(params):\n","    test = params[\"test\"]\n","    sample_size = params[\"sample_size\"]\n","    batch_size = params[\"batch_size\"]\n","    z_dim = params[\"z_dim\"]\n","    dx = params[\"dx\"]\n","    dy = params[\"dy\"]\n","    n_test = params[\"n_test\"]\n","    epochs_num = params[\"epochs_num\"]\n","    eps_std = params[\"eps_std\"]\n","    dist_z = params[\"dist_z\"]\n","    alpha_x = params[\"alpha_x\"]\n","    m_value = params[\"m_value\"]\n","    k_value = params[\"k_value\"]\n","    j_value = params[\"j_value\"]\n","    noise_dimension = params[\"noise_dimension\"]\n","    hidden_layer_size = params[\"hidden_layer_size\"]\n","    normal_ini = params[\"normal_ini\"]\n","    preprocess = params[\"preprocess\"]\n","    G_lr = params[\"G_lr\"]\n","    alpha = params[\"alpha\"]\n","    alpha1 = params[\"alpha1\"]\n","    set_seeds = params[\"set_seeds\"]\n","    using_orcale = params[\"using_orcale\"]\n","    lambda_1 = params[\"lambda_1\"]\n","    lambda_2 = params[\"lambda_2\"]\n","    using_Gen = params[\"using_Gen\"]\n","    boor_rv_type = params[\"boor_rv_type\"]\n","    wgt_decay = params[\"wgt_decay\"]\n","    lambda_3 = params[\"lambda_3\"]\n","    drop_out_p = params[\"drop_out_p\"]\n","    M_train = params[\"M_train\"]\n","\n","    np.random.seed(set_seeds)\n","    tf.random.set_seed(set_seeds)\n","    torch.manual_seed(set_seeds)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(set_seeds)\n","\n","    a_f = torch.rand((z_dim, dx))\n","    l1_norm_a_f = torch.linalg.vector_norm(a_f, ord=1)\n","    Ax = a_f / l1_norm_a_f\n","    a_g = torch.rand((z_dim, dy))\n","    l1_norm_a_g = torch.linalg.vector_norm(a_g, ord=1)\n","    Ay = a_g / l1_norm_a_g\n","\n","    p_values = np.array([])\n","    test_count = 0\n","    if test == 'type1error':\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","\n","            p_value = GCIT(Ax=Ax, Ay=Ay, n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                           epochs_num=epochs_num,\n","                           nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                           k=k_value, boot_num=j_value,\n","                           noise_dimension=noise_dimension, hidden_layer_size=hidden_layer_size, normal_ini=normal_ini,\n","                           preprocess=preprocess, G_lr=G_lr, using_orcale=using_orcale,\n","                           lambda_1=lambda_1, lambda_2=lambda_2, using_Gen=using_Gen, boor_rv_type=boor_rv_type,\n","                           wgt_decay=wgt_decay, lambda_3=lambda_3, drop_out_p=drop_out_p, exp_num=n + 1,\n","                           M_train=M_train)\n","            test_count += 1\n","            print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha / 2.0 for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 / 2.0 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            print('The stat is {}'.format(p_value))\n","            print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","    if test == 'power':\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","\n","            p_value = GCIT(Ax=Ax, Ay=Ay, n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                           epochs_num=epochs_num,\n","                           nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                           k=k_value, boot_num=j_value,\n","                           noise_dimension=noise_dimension, hidden_layer_size=hidden_layer_size, normal_ini=normal_ini,\n","                           preprocess=preprocess, G_lr=G_lr, using_orcale=using_orcale,\n","                           lambda_1=lambda_1, lambda_2=lambda_2, using_Gen=using_Gen, boor_rv_type=boor_rv_type,\n","                           wgt_decay=wgt_decay, lambda_3=lambda_3, drop_out_p=drop_out_p, exp_num=n + 1,\n","                           M_train=M_train)\n","\n","            test_count += 1\n","            print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha / 2.0 for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 / 2.0 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            print('The stat is {}'.format(p_value))\n","            print('Power: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            print('Power: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","    return p_values\n","\n","\n","run_experiment(param)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}