{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7_tv5Mqx5ggc"},"outputs":[],"source":["param = {\n","    \"test\": \"type1error\",  # ['type1error', 'power']\n","    \"sample_size\": 1000,  # [200, 400, 600, 800, 1000]\n","    \"batch_size\": 256,  # [32, 64, 128, 256]\n","    \"z_dim\": 1,  # [5, 50, 250]\n","    \"dx\": 1,\n","    \"dy\": 1,\n","    \"n_test\": 500,  # [200, 2000]\n","    \"epochs_num\": 200,  # [1000, 1500]\n","    \"eps_std\": 0.5,\n","    \"dist_z\": 'gaussian',  # ['laplace', 'gaussian']\n","    \"alpha_x\": 0.75,  # only used under alternative\n","    \"m_value\": 100,  # [100, 200]\n","    \"k_value\": 2,  # [1, 2, 4]\n","    \"j_value\": 1000,  # [1000, 2000]\n","    \"noise_dimension\": 3,  # [5, 10, 20]\n","    \"hidden_layer_size\": 20,  # [64, 128, 256, 512, 1024]\n","    \"normal_ini\": False,  # [True, False]\n","    \"preprocess\": 'None',  # ['normalize',  'scale_Z', 'None' ]\n","    \"G_lr\": 1e-3,  # [5e-6, 1e-5, 2e-5ï¼Œ 5e-5]\n","    \"alpha\": 0.1,\n","    \"alpha1\": 0.05,\n","    \"set_seeds\": 42,\n","    \"using_orcale\": False,\n","    \"lambda_1\": 1,  # loss with Laplace kernel\n","    \"lambda_2\": 0,  # loss with Gaussian kernel\n","    \"using_Gen\": '1',  # ['1', '2'], types of generator \"1\" is fully connect, \"2\" is non fully\n","    \"boor_rv_type\": 'gaussian',  # ['rademacher', 'gaussian']\n","    \"wgt_decay\": 1e-5,  # weight decay for adam optimizer L2 regularization parameter\n","    \"lambda_3\": 1e-5,  # L1 regularization parameter\n","    \"drop_out_p\": 0.2,  # probability of an element to be zeroed. Default: 0.5, best 0.2\n","    \"M_train\": 10\n","}\n","\n","import torch\n","import torch.distributions as TD\n","from torch.utils.data import Dataset, DataLoader\n","from zmq import device\n","import torch.optim as optim\n","import numpy as np\n","from datetime import datetime\n","import functools\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","\n","def generate_samples_random(Ax, Ay, size=1000, sType='CI', dx=1, dy=1, dz=20, nstd=0.05, alpha_x=0.05,\n","                            preprocess=\"None\", dist_z='gaussian'):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    numbers_z = np.random.multinomial(num, [1 / 2.] * 2, size=1)\n","    number_z_zeros = numbers_z[0][0]\n","    number_z_ones = numbers_z[0][1]\n","\n","    xy_arr = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    xy_z_zero_index = np.random.choice(a=len(xy_arr), size=number_z_zeros, p=[1 / 4., 1 / 4., 1 / 4., 1 / 4.])\n","    xy_z_one_index = np.random.choice(a=len(xy_arr), size=number_z_ones, p=[1 / 4., 1 / 4., 1 / 4., 1 / 4.])\n","\n","    xy_z_zero = xy_arr[xy_z_zero_index]\n","    xy_z_one = xy_arr[xy_z_one_index]\n","\n","    xy = np.concatenate((xy_z_zero, xy_z_one), axis=0)\n","\n","    x = xy[:, 0]\n","    y = xy[:, 1]\n","    z = np.concatenate((np.zeros(number_z_zeros), np.ones(number_z_ones)), axis=0)\n","\n","    indices = np.random.permutation(num)\n","\n","    x, y, z = x[indices], y[indices], z[indices]\n","    X, Y, Z = x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)\n","\n","    if preprocess == \"normalize\":\n","        Z = (Z - Z.min()) / (Z.max() - Z.min())\n","        X = (X - X.min()) / (X.max() - X.min())\n","        Y = (Y - Y.min()) / (Y.max() - Y.min())\n","\n","    elif preprocess == \"scale_Z\":\n","        Z = Z / Z.max()\n","\n","    elif preprocess == \"None\":\n","        X, Y, Z = X, Y, Z\n","\n","    X, Y, Z = torch.from_numpy(np.array(X)).float(), torch.from_numpy(np.array(Y)).float(), torch.from_numpy(\n","        np.array(Z)).float()\n","    return X, Y, Z\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u=1, sigma_v=1, boor_rv_type=\"gaussian\"):\n","    '''\n","    Compute the p-value\n","\n","    Input:\n","    - boot_num: Integer giving the number of bootstrap samples.\n","    - M: Integer giving the number of training samples per batch.\n","    - n: Integer giving the number of training samples.\n","    - gen_x_all_torch: PyTorch Tensor (batch_size, M) of generated data of X.\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data of Y.\n","    - x_torch: PyTorch Tensor (batch_size) of training input X.\n","    - y_torch: PyTorch Tensor (batch_size) of training input Y.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z\n","    - sigma_w: Float of the bandwidth of the Laplace kernel.\n","    - sigma_u: Float of the bandwidth of the Laplace kernel.\n","    - sigma_v: Float of the bandwidth of the Laplace kernel.\n","    - boor_rv_type: \"rademacher\" or \"gaussian\", specifying the reference distribution.\n","\n","    Output:\n","    - p_value: Float giving the p-value.\n","    '''\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.abs(y_torch.repeat(1, n) - y_torch.repeat(1, n).T) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.abs(gen_y_all_torch.repeat(n, 1, 1) - y_torch.repeat(1, n).reshape(n, n, 1)) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1)\n","\n","    temp_mx = gen_y_all_torch_rep[:, :, 0].T\n","    sum_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","\n","    v_mx_1 = torch.exp(-torch.abs(x_torch.repeat(1, n) - x_torch.repeat(1, n).T) / sigma_v)\n","    v_mx_2 = torch.mean(\n","        torch.exp(-torch.abs(gen_x_all_torch.repeat(n, 1, 1) - x_torch.repeat(1, n).reshape(n, n, 1)) / sigma_v), dim=2)\n","    v_mx_3 = v_mx_2.T\n","\n","    gen_x_all_torch_rep = gen_x_all_torch.repeat(n, 1, 1)\n","\n","    temp2_mx = gen_x_all_torch_rep[:, :, 0].T\n","    sum2_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n, n, 1)) / sigma_v), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = gen_y_all_torch_rep[:, :, i].T\n","        temp_add_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","        temp2_mx = gen_x_all_torch_rep[:, :, i].T\n","        temp2_add_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n, n, 1)) / sigma_v), dim=2)\n","        sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1 / M * sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    stat = 1 / (n - 1) * torch.sum(FF_mx).item()\n","\n","    # print(\"U_mx:\", u_mx)\n","    # print(\"V_mx:\", v_mx)\n","    # print(\"W_mx:\", w_mx)\n","    boottemp = np.array([])\n","    if boor_rv_type == \"rademacher\":\n","        eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","        eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","        random_mx = torch.matmul(eboot[:, bb].reshape(-1, 1), eboot[:, bb].reshape(-1, 1).T)\n","        bootmatrix = FF_mx * random_mx\n","        stat_boot = 1 / (n - 1) * torch.sum(bootmatrix).item()\n","        boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","\n","class DatasetSelect(Dataset):\n","    \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - Z: PyTorch Tensor of shape (N, output_dimension) giving the training data of Z.\n","    \"\"\"\n","\n","    def __init__(self, X, Y, Z):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index]\n","\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","    \"\"\"\n","      Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","      Input:\n","      - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","      - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","      - batch_size: Integer giving the batch size.\n","    \"\"\"\n","\n","    def __init__(self, X, Y, Z, batch_size):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.batch_size = batch_size\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[\n","            (self.batch_size + index) % self.sample_size]\n","\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN_ver2(torch.utils.data.Dataset):\n","    \"\"\"\n","      Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","      Input:\n","      - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","      - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","      - batch_size: Integer giving the batch size.\n","    \"\"\"\n","\n","    def __init__(self, Y, Z, batch_size):\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.batch_size = batch_size\n","        self.sample_size = Z.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.Y_real[index], self.Z_real[index]\n","\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","        noise_generator = TD.MultivariateNormal(\n","            torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","        Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","        Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","        Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","\n","##### GAN architecture #####\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input Z.\n","    - output_dimension: Integer giving the dimension of output X or Y.\n","    - noise_dimension: Integer giving the dimension of random noise.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","    - drop_out_p: Float giving the dropout probability.\n","    - drop_input: Boolean specifying whether to add dropout to the input layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the generator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef,\n","                 drop_out_p,\n","                 drop_input=False):\n","        super(Generator, self).__init__()\n","        self.BN_type = BN_type\n","        self.ReLU_coef = ReLU_coef\n","        self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","        if BN_type:\n","            self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","            self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","            self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","        self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","        self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","        self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","        self.sigmoid = torch.nn.Sigmoid()\n","        self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","        self.drop_input = drop_input\n","\n","    def forward(self, x):\n","        if self.BN_type:\n","            if self.drop_input:\n","                x = self.drop_out0(x)\n","            x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","            x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","            # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","            x = self.fc_last(x)\n","        else:\n","            if self.drop_input:\n","                x = self.drop_out0(x)\n","            x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","            x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","            # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","            x = self.fc_last(x)\n","            x = self.sigmoid(x)\n","            # x = (x > 0.5).float()\n","        return x\n","\n","\n","class NonFullyConnected_1(torch.nn.Module):\n","\n","    def __init__(self, size_in, size_out, m, bias=True):\n","        super(NonFullyConnected_1, self).__init__()\n","        self.linear = torch.nn.Linear(m * size_in, m * size_out, bias=bias).to(device)\n","        self.mask = functools.reduce(torch.block_diag, [torch.ones(size_out, size_in) for i in range(m)]).to(device)\n","\n","    def forward(self, x):\n","        self.linear.weight.data *= self.mask\n","        return self.linear(x)\n","\n","\n","class Generator_2(torch.nn.Module):\n","    def __init__(\n","            self,\n","            input_dimension,\n","            output_dimension,\n","            noise_dimension,\n","            hidden_layer_size,\n","            BN_type,\n","            ReLU_coef,\n","            hidden_layer_depth=1,\n","            ntargets_k=5):\n","        super(Generator_2, self).__init__()\n","        self.input_dimension = input_dimension + noise_dimension\n","        self.output_dimension = output_dimension\n","        self.ntargets_k = ntargets_k\n","        self.hidden_layer_sizes = [hidden_layer_size] * hidden_layer_depth\n","        self.BN_type = BN_type\n","        self.leakyrelu = torch.nn.LeakyReLU(ReLU_coef)\n","        self.linear_layers_from_input = torch.nn.Linear(self.input_dimension, ntargets_k * self.hidden_layer_sizes[0])\n","\n","        self.linear_layers_between = torch.nn.ModuleList([\n","            NonFullyConnected_1(self.hidden_layer_sizes[0], self.hidden_layer_sizes[0], ntargets_k)\n","            for i in range(len(self.hidden_layer_sizes))\n","        ])\n","        # self.linear8 = torch.nn.Linear(self.hidden_layer_sizes[0]*ntargets_k, self.hidden_layer_sizes[0]*ntargets_k)\n","        # self.linear8.weight = torch.nn.Parameter(torch.eye(self.hidden_layer_sizes[0]*ntargets_k), requires_grad=False)\n","        self.linear8 = torch.nn.Linear(self.hidden_layer_sizes[0] * ntargets_k, self.output_dimension)\n","        if BN_type:\n","            self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","\n","    def forward(self, input):\n","        if self.BN_type:\n","            output = self.linear_layers_from_input(input)\n","            output = self.leakyrelu(self.BN1(output))\n","            for linear_layers_between in self.linear_layers_between:\n","                output = linear_layers_between(output)\n","                output = self.leakyrelu(self.BN1(output))\n","        else:\n","            output = self.linear_layers_from_input(input)\n","            output = self.leakyrelu(output)\n","            for linear_layers_between in self.linear_layers_between:\n","                output = linear_layers_between(output)\n","                output = self.leakyrelu(output)\n","\n","        return self.linear8(output)  # torch.mean(self.linear8(output)).reshape(1)\n","\n","\n","##### Training procedures #####\n","\n","\n","def find_loss(y_torch, gen_y_all_torch, z_torch, sigma_w, sigma_u, M):\n","    \"\"\"\n","    Compute the MMD loss via Laplace kernel.\n","\n","    Inputs:\n","    - y_torch: PyTorch Tensor (batch_size) of training input. (X or Y)\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z.\n","    - sigma_w: Float of the bandwith of the kernel.\n","    - sigma_u: Float of the bandwith of the kernel.\n","    - M: Number of training samples per batch.\n","\n","    Outputs:\n","    - loss: PyTorch Tensor containing the MMD loss.\n","    \"\"\"\n","    n = z_torch.shape[0]\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1,\n","                                    dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.abs(y_torch.repeat(1, n) - y_torch.repeat(1, n).T) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.abs(gen_y_all_torch.repeat(n, 1, 1) - y_torch.repeat(1, n).reshape(n, n, 1)) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1)\n","\n","    temp_mx = gen_y_all_torch_rep[:, :, 0].T\n","    sum_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = gen_y_all_torch_rep[:, :, i].T\n","        temp_add_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1)) / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","\n","    FF_mx = u_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    loss = 1 / (n) * torch.sum(FF_mx)\n","    return loss\n","\n","\n","def train_ver3(X, Y, Z, X_test, Y_test, Z_test, M,\n","               noise_dimension, noise_type, G_lr, hidden_layer_size,\n","               DataLoader, BN_type, ReLU_coef,\n","               epochs_num=10, sigma_z=1, sigma_x=1, sigma_y=1,\n","               normal_ini=False,\n","               lambda_1=1, lambda_2=1, using_Gen='1', wgt_decay=0,\n","               lambda_3=0, drop_out_p=0.2, M_train=3):\n","    \"\"\"\n","    Train loop for GAN.\n","\n","    Inputs:\n","    - X: PyTorch Tensor (sample_size, dimension_X) of training input.\n","    - Y: PyTorch Tensor (sample_size, dimension_Y) of training input.\n","    - Z: PyTorch Tensor (sample_size, dimension_Z) of training input.\n","    - X_test: PyTorch Tensor (sample_size, dimension_X) of test input.\n","    - Y_test: PyTorch Tensor (sample_size, dimension_Y) of test input.\n","    - Z_test: PyTorch Tensor (sample_size, dimension_Z) of test input.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","    - G_lr: Float giving the learning rate of the generator.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - DataLoader: DataLoader object used to generate training batches.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Float giving the coefficient of the Leaky ReLU layer.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - sigma_z: Float of the bandwith of the kernel.\n","    - sigma_x: Float of the bandwith of the kernel.\n","    - sigma_y: Float of the bandwith of the kernel.\n","    - normal_ini: Boolean specifying whether to initialize the generator with normal initialization.\n","    - lambda_1: Float giving the coefficient of the MMD loss using Laplace kernel.\n","    - lambda_2: Float giving the coefficient of the MMD loss using Gaussian kernel. (not using)\n","    - using_Gen: '1' or '2' specifying whether to use the first or second generator.(not using)\n","    - wgt_decay: Float giving the weight decay. (L2 regularization)\n","    - lambda_3: Scalar giving the coefficient of the L1 regularization.\n","    - drop_out_p: Float giving the dropout probability.\n","    - M_train: Number of training samples per batch used in the Laplace or Gaussian kernel.\n","\n","    Outputs:\n","    - G_zy: PyTorch Net giving the trained generator.\n","    - G_zx: PyTorch Net giving the trained generator.\n","    \"\"\"\n","\n","    input_dimension = Z.shape[1]\n","    output_dimension_y = Y.shape[1]\n","    output_dimension_x = X.shape[1]\n","\n","    if using_Gen == '1':\n","\n","        G_zy = Generator(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type, ReLU_coef,\n","                         drop_out_p).to(device)\n","        G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","        G_zx = Generator(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type, ReLU_coef,\n","                         drop_out_p).to(device)\n","        G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    elif using_Gen == '2':\n","        G_zy = Generator_2(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type,\n","                           ReLU_coef).to(device)\n","        G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","        G_zx = Generator_2(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type,\n","                           ReLU_coef).to(device)\n","        G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    iter_count = 0\n","    G_zy = G_zy.train()\n","    G_zx = G_zx.train()\n","\n","    if normal_ini:\n","        for p in G_zy.parameters():\n","            p.data = torch.randn(\n","                p.shape, device=device,\n","                dtype=torch.float32) / np.sqrt(float(hidden_layer_size * 2))\n","\n","        for p in G_zx.parameters():\n","            p.data = torch.randn(\n","                p.shape, device=device,\n","                dtype=torch.float32) / np.sqrt(float(hidden_layer_size * 2))\n","\n","    for epoch in range(epochs_num):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zy = G_zy.train()\n","        G_zx = G_zx.train()\n","        for X_real, Y_real, Z_real, Z_fake in DataLoader:\n","            X_real = X_real.to(device)\n","            Y_real = Y_real.to(device)\n","            Z_real = Z_real.to(device)\n","            Z_fake = Z_fake.to(device)\n","\n","            batch_size = Z_real.shape[0]\n","            Z_real_repeat = Z_real.repeat(M_train, 1)\n","\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","                device)\n","            Y_fake = G_zy(torch.cat((Z_real_repeat, Noise_fake), dim=1)).to(device)\n","\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var=1.0 / 3.0).to(\n","                device)\n","            X_fake = G_zx(torch.cat((Z_real_repeat, Noise_fake), dim=1)).to(device)\n","\n","            Y_fake = Y_fake.reshape(batch_size, M_train)\n","            X_fake = X_fake.reshape(batch_size, M_train)\n","\n","            standardise = False\n","\n","            if standardise:\n","                Y_fake = (Y_fake - torch.mean(Y_fake, dim=0, keepdim=True)) / torch.std(Y_fake, dim=0, keepdim=True)\n","                X_fake = (X_fake - torch.mean(X_fake, dim=0, keepdim=True)) / torch.std(X_fake, dim=0, keepdim=True)\n","                X_real = (X_real - torch.mean(X_real, dim=0, keepdim=True)) / torch.std(X_real, dim=0, keepdim=True)\n","                Y_real = (Y_real - torch.mean(Y_real, dim=0, keepdim=True)) / torch.std(Y_real, dim=0, keepdim=True)\n","                Z_real = (Z_real - torch.mean(Z_real, dim=0, keepdim=True)) / torch.std(Z_real, dim=0, keepdim=True)\n","\n","            # Generator step\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            l1_regularization = 0\n","\n","            for param in G_zy.parameters():\n","                l1_regularization += torch.linalg.vector_norm(param, ord=1)\n","\n","            g_zy_error = lambda_1 * find_loss(Y_real, Y_fake, Z_real, sigma_z, sigma_y,\n","                                              M_train) + lambda_3 * l1_regularization\n","\n","            g_zy_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zy.parameters(), max_norm=0.5)\n","            G_zy_solver.step()\n","\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            l1_regularization = 0\n","\n","            for param in G_zx.parameters():\n","                l1_regularization += torch.linalg.vector_norm(param, ord=1)\n","\n","            g_zx_error = lambda_1 * find_loss(X_real, X_fake, Z_real, sigma_z, sigma_x,\n","                                              M_train) + lambda_3 * l1_regularization\n","\n","            g_zx_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zx.parameters(), max_norm=0.5)\n","            G_zx_solver.step()\n","\n","            iter_count += 1\n","            batch_count += 1\n","\n","    return G_zy, G_zx\n","\n","\n","def mGAN(Ax, Ay, n=500, z_dim=100, simulation='type1error', batch_size=64, epochs_num=1000,\n","         nstd=1.0, z_dist='gaussian', x_dims=1, y_dims=1, a_x=0.05, M=500, k=2, boot_num=1000,\n","         noise_dimension=10, hidden_layer_size=512, normal_ini=False, preprocess='normalize',\n","         G_lr=1e-5, using_orcale=False, lambda_1=1, lambda_2=1, using_Gen='1',\n","         boor_rv_type=\"gaussian\", wgt_decay=0, lambda_3=1, drop_out_p=0.2, exp_num=0, M_train=3):\n","\n","    \"\"\"\n","    Compute the test statistics\n","\n","    Inputs:\n","    - Ax: Torch Tensor of shape (sample_size, dimension_X) giving the matrix to generate training data of X.\n","    - Ay: Torch Tensor of shape (sample_size, dimension_Y) giving the matrix to generate training data of Y.\n","    - n: Integer giving the number of samples to generate.\n","    - z_dim: Integer giving the dimension of Z.\n","    - simulation: 'type1error' or 'power'.\n","    - batch_size: Integer giving the batch size.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - nstd: Float. standard deviation of the noise in the simulated data.\n","    - z_dist: 'gaussian' or 'uniform'.\n","    - x_dims: Integer giving the dimension of X.\n","    - y_dims: Integer giving the dimension of Y.\n","    - a_x: Float using in the alternative case. alpha_x.\n","    - M: Integer giving the number of training samples per batch used in the Laplace or Gaussian kernel.\n","    - k: Integer giving the number of cross-validation folds.\n","    - boot_num: Integer of the number of wild bootstrap when computing p-value.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - normal_ini: Boolean specifying whether to initialize the generator with normal initialization.\n","    - G_lr: Float giving the learning rate of the generator.\n","    - using_orcale: 'True' or 'False' specifying whether to use the orcale method.\n","    - lambda_1: Float giving the coefficient of the MMD loss using Laplace kernel.\n","    - lambda_2: Float giving the coefficient of the MMD loss using Gaussian kernel.\n","    - using_Gen: '1' or '2' specifying whether to use the first or second generator.\n","    - boor_rv_type: 'rademacher', 'gaussian'. type of the bootstrap random variable.\n","    - wgt_decay: Float giving the weight decay. (L2 regularization)\n","    - lambda_3: Scalar giving the coefficient of the L1 regularization.\n","    - drop_out_p: Float giving the dropout probability\n","    - exp_num: not using\n","    - M_train: Number of training samples per batch used in the Laplace or Gaussian kernel.\n","\n","    Outputs:\n","    - p-value: Float of computed p-value.\n","\n","    \"\"\"\n","    if simulation == 'type1error':\n","        # generate samples x, y, z under null hypothesis - x and y are conditional independent\n","        sim_x, sim_y, sim_z = generate_samples_random(Ax, Ay, size=n, sType='CI', dx=x_dims, dy=y_dims, dz=z_dim,\n","                                                      nstd=nstd, alpha_x=a_x,\n","                                                      dist_z=z_dist, preprocess=preprocess)\n","\n","    elif simulation == 'power':\n","        # generate samples x, y, z under alternative hypothesis - x and y are dependent\n","        sim_x, sim_y, sim_z = generate_samples_random(Ax, Ay, size=n, sType='dependent', dx=x_dims, dy=y_dims, dz=z_dim,\n","                                                      nstd=nstd,\n","                                                      alpha_x=a_x, dist_z=z_dist, preprocess=preprocess)\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    x, y, z = sim_x, sim_y, sim_z\n","\n","    # w_mx = torch.linalg.vector_norm(z.repeat(n,1,1) - torch.swapaxes(z.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_w_train = 1.0  # torch.median(w_mx).item()\n","\n","    # u_mx = torch.abs(y.repeat(1, n) - y.repeat(1, n).T)\n","    sigma_u_train = 1.0  # torch.median(u_mx).item()\n","\n","    # v_mx = torch.abs(x.repeat(1, n) - x.repeat(1, n).T)\n","    sigma_v_train = 1.0  # torch.median(v_mx).item()\n","\n","    test_size = int(n / k)\n","    stat_all = torch.zeros(k, 1)\n","    boot_temp_all = torch.zeros(k, boot_num)\n","    cur_k = 0\n","\n","    mse_x_list = np.array([])\n","    mse_y_list = np.array([])\n","\n","    for k_fold in range(k):\n","        k_fold_start = int(n / k * k_fold)\n","        k_fold_end = int(n / k * (k_fold + 1))\n","        X_test, Y_test, Z_test = x[k_fold_start:k_fold_end], y[k_fold_start:k_fold_end], z[k_fold_start:k_fold_end]\n","        X_train, Y_train, Z_train = torch.cat((x[0:k_fold_start], x[k_fold_end:])), torch.cat(\n","            (y[0:k_fold_start], y[k_fold_end:])), torch.cat((z[0:k_fold_start], z[k_fold_end:]))\n","\n","        if (k == 1):\n","            X_train, Y_train, Z_train = X_test, Y_test, Z_test\n","\n","        train_xyz = DatasetSelect_GAN(X_train, Y_train, Z_train, batch_size)\n","        DataLoader_xyz = torch.utils.data.DataLoader(train_xyz, batch_size=batch_size, shuffle=True)\n","        if not using_orcale:\n","            G_zy, G_zx = train_ver3(X=X_train, Y=Y_train, Z=Z_train, M=M,\n","                                    X_test=X_test, Y_test=Y_test, Z_test=Z_test,\n","                                    noise_dimension=noise_dimension, noise_type=\"normal\",\n","                                    G_lr=G_lr, hidden_layer_size=hidden_layer_size,\n","                                    DataLoader=DataLoader_xyz, BN_type=False, ReLU_coef=0.1,\n","                                    epochs_num=epochs_num, sigma_z=sigma_w_train, sigma_x=sigma_v_train,\n","                                    sigma_y=sigma_u_train,\n","                                    normal_ini=normal_ini, lambda_1=lambda_1, lambda_2=lambda_2,\n","                                    using_Gen=using_Gen, wgt_decay=wgt_decay, lambda_3=lambda_3,\n","                                    drop_out_p=drop_out_p, M_train=M_train)\n","\n","        dataset_test = DatasetSelect(X_test, Y_test, Z_test)\n","        dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True)\n","\n","        gen_x_all = torch.zeros(test_size, M)\n","        gen_y_all = torch.zeros(test_size, M)\n","        z_all = torch.zeros(test_size, z_dim)\n","        x_all = torch.zeros(test_size, x_dims)\n","        y_all = torch.zeros(test_size, y_dims)\n","\n","        cur_itr = 0\n","        G_zx = G_zx.eval()\n","        G_zy = G_zy.eval()\n","        for i, (x_test, y_test, z_test) in enumerate(dataloader_test):\n","            z_test_temp = z_test.repeat(M,1)\n","\n","            z_test_temp = z_test_temp.to(device)\n","            Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, \"normal\", input_var = 1.0/3.0).to(device)\n","            fake_x = G_zx(torch.cat((z_test_temp, Noise_fake),dim=1)).reshape(1, -1)\n","\n","            Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, \"normal\", input_var = 1.0/3.0).to(device)\n","            fake_y = G_zy(torch.cat((z_test_temp, Noise_fake),dim=1)).reshape(1, -1)\n","\n","            gen_x_all[cur_itr,:] = fake_x.detach().reshape(-1)\n","            gen_y_all[cur_itr,:] = fake_y.detach().reshape(-1)\n","            x_all[cur_itr,:] = x_test\n","            y_all[cur_itr,:] = y_test\n","            z_all[cur_itr,:] = z_test\n","            cur_itr = cur_itr + 1\n","\n","        gen_x_all = gen_x_all.to(device)\n","        gen_y_all = gen_y_all.to(device)\n","\n","        z_all = Z_test.to(device)\n","        x_all = X_test.to(device)\n","        y_all = Y_test.to(device)\n","\n","        # standardise = True\n","\n","        # if standardise:\n","        #     gen_x_all = (gen_x_all - torch.mean(gen_x_all, dim=0, keepdim=True)) / torch.std(gen_x_all, dim=0, keepdim=True)\n","        #     gen_y_all = (gen_y_all - torch.mean(gen_y_all, dim=0, keepdim=True)) / torch.std(gen_y_all, dim=0, keepdim=True)\n","        #     x_all = (x_all - torch.mean(x_all, dim=0, keepdim=True)) / torch.std(x_all, dim=0, keepdim=True)\n","        #     y_all = (y_all - torch.mean(y_all, dim=0, keepdim=True)) / torch.std(y_all, dim=0, keepdim=True)\n","        #     z_all = (z_all - torch.mean(z_all, dim=0, keepdim=True)) / torch.std(z_all, dim=0, keepdim=True)\n","\n","        # w_mx = torch.linalg.vector_norm(z_all.repeat(test_size,1,1) - torch.swapaxes(z_all.repeat(test_size,1,1), 0, 1), ord = 1, dim = 2)\n","        sigma_w = 1.0  # torch.median(w_mx).item()\n","\n","        # u_mx = torch.abs(y_all.repeat(1, test_size) - y_all.repeat(1, test_size).T)\n","        sigma_u = 1.0  # torch.median(u_mx).item()\n","\n","        # v_mx = torch.abs(x_all.repeat(1, test_size) - x_all.repeat(1, test_size).T)\n","        sigma_v = 1.0  # torch.median(v_mx).item()\n","\n","        cur_stat, cur_boot_temp = get_p_value_stat_1(boot_num, M, test_size, gen_x_all.to(device), gen_y_all.to(device),\n","                                                     x_all.to(device), y_all.to(device), z_all.to(device), sigma_w,\n","                                                     sigma_u, sigma_v,\n","                                                     boor_rv_type)\n","        stat_all[cur_k, :] = cur_stat\n","        boot_temp_all[cur_k, :] = torch.from_numpy(cur_boot_temp)\n","        cur_k = cur_k + 1\n","\n","        if using_orcale:\n","            gen_x_mean = torch.mean(gen_x_all, dim=1).reshape(-1, 1)\n","            gen_y_mean = torch.mean(gen_y_all, dim=1).reshape(-1, 1)\n","\n","            mse_x = torch.mean((gen_x_mean - x_all) ** 2).item()\n","            mse_y = torch.mean((gen_y_mean - y_all) ** 2).item()\n","\n","            print(f'Test MSE x [{mse_x}], MSE y [{mse_y}]')\n","    # print(\"boot_temp_all:\", torch.mean(boot_temp_all, dim = 0).numpy())\n","    # print(\"stat_all:\", torch.mean(stat_all).item())\n","    return np.mean(torch.mean(boot_temp_all, dim=0).numpy() > torch.mean(stat_all).item())\n","\n","\n","def run_experiment(params):\n","    test = params[\"test\"]\n","    sample_size = params[\"sample_size\"]\n","    batch_size = params[\"batch_size\"]\n","    z_dim = params[\"z_dim\"]\n","    dx = params[\"dx\"]\n","    dy = params[\"dy\"]\n","    n_test = params[\"n_test\"]\n","    epochs_num = params[\"epochs_num\"]\n","    eps_std = params[\"eps_std\"]\n","    dist_z = params[\"dist_z\"]\n","    alpha_x = params[\"alpha_x\"]\n","    m_value = params[\"m_value\"]\n","    k_value = params[\"k_value\"]\n","    j_value = params[\"j_value\"]\n","    noise_dimension = params[\"noise_dimension\"]\n","    hidden_layer_size = params[\"hidden_layer_size\"]\n","    normal_ini = params[\"normal_ini\"]\n","    preprocess = params[\"preprocess\"]\n","    G_lr = params[\"G_lr\"]\n","    alpha = params[\"alpha\"]\n","    alpha1 = params[\"alpha1\"]\n","    set_seeds = params[\"set_seeds\"]\n","    using_orcale = params[\"using_orcale\"]\n","    lambda_1 = params[\"lambda_1\"]\n","    lambda_2 = params[\"lambda_2\"]\n","    using_Gen = params[\"using_Gen\"]\n","    boor_rv_type = params[\"boor_rv_type\"]\n","    wgt_decay = params[\"wgt_decay\"]\n","    lambda_3 = params[\"lambda_3\"]\n","    drop_out_p = params[\"drop_out_p\"]\n","    M_train = params[\"M_train\"]\n","\n","    np.random.seed(set_seeds)\n","    torch.manual_seed(set_seeds)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(set_seeds)\n","\n","    a_f = torch.rand((z_dim, dx))\n","    l1_norm_a_f = torch.linalg.vector_norm(a_f, ord=1)\n","    Ax = a_f / l1_norm_a_f\n","    a_g = torch.rand((z_dim, dy))\n","    l1_norm_a_g = torch.linalg.vector_norm(a_g, ord=1)\n","    Ay = a_g / l1_norm_a_g\n","\n","    p_values = np.array([])\n","    test_count = 0\n","    if test == 'type1error':\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","\n","            p_value = mGAN(Ax=Ax, Ay=Ay, n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                           epochs_num=epochs_num,\n","                           nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                           k=k_value, boot_num=j_value,\n","                           noise_dimension=noise_dimension, hidden_layer_size=hidden_layer_size, normal_ini=normal_ini,\n","                           preprocess=preprocess, G_lr=G_lr, using_orcale=using_orcale,\n","                           lambda_1=lambda_1, lambda_2=lambda_2, using_Gen=using_Gen, boor_rv_type=boor_rv_type,\n","                           wgt_decay=wgt_decay, lambda_3=lambda_3, drop_out_p=drop_out_p, exp_num=n + 1,\n","                           M_train=M_train)\n","            test_count += 1\n","            print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            print('The stat is {}'.format(p_value))\n","            print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","    if test == 'power':\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","\n","            p_value = mGAN(Ax=Ax, Ay=Ay, n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                           epochs_num=epochs_num,\n","                           nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                           k=k_value, boot_num=j_value,\n","                           noise_dimension=noise_dimension, hidden_layer_size=hidden_layer_size, normal_ini=normal_ini,\n","                           preprocess=preprocess, G_lr=G_lr, using_orcale=using_orcale,\n","                           lambda_1=lambda_1, lambda_2=lambda_2, using_Gen=using_Gen, boor_rv_type=boor_rv_type,\n","                           wgt_decay=wgt_decay, lambda_3=lambda_3, drop_out_p=drop_out_p, exp_num=n + 1,\n","                           M_train=M_train)\n","\n","            test_count += 1\n","            print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            print('The stat is {}'.format(p_value))\n","            print('Power: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            print('Power: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","    return p_values\n","\n","\n","run_experiment(param)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}