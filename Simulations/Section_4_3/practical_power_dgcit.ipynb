{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p6fkoQPNyLf0"},"outputs":[],"source":["model = 'dgcit'\n","# number of samples\n","sample_size = 200 # [200, 400, 600, 800, 1000]\n","batch_size = 64\n","z_dims_scheme = [1]\n","dx = 1\n","dy = 1\n","test = 'power'\n","n_test = 500\n","n_iters = 3000 #original 1000\n","eps_std = 0.5\n","dist_z = 'gaussian'#### choices=['gaussian', 'laplace']\n","alpha_x = 0.75 ##only used under alternative\n","m_value = 100\n","k_value = 2\n","b_value = 30 #original 30\n","j_value = 1000 #original 1000##bootstrap number\n","\n","set_seed = 42\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# Utilites related to Sinkhorn computations and training for TensorFlow 2.0\n","import tensorflow as tf\n","import logging\n","import tensorflow_probability as tfp\n","from sklearn.metrics.pairwise import rbf_kernel\n","from scipy.stats import rankdata, ks_2samp, wilcoxon\n","from sklearn.model_selection import KFold\n","from datetime import datetime\n","import decimal\n","import torch\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","import gc # Garbage Collector\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","logging.getLogger('tensorflow').disabled = True\n","tf.keras.backend.set_floatx('float32')\n","\n","\n","'''\n","This code reproduces the real data experiments using the CCLE data.\n","Preprocessing steps follow the code of W. Tansey at https://github.com/tansey/hrt.\n","And the following code is obtained from https://github.com/alexisbellot/GCIT.\n","'''\n","\n","\n","def load_ccle(drug_target='PLX4720', feature_type='both', normalize=False):\n","    '''\n","    :param drug target: specific drug we w ant to analyse\n","    :param normalize: normalize data\n","    :return: genetic features (mutations) as a 2d array for each cancer cell and corresponding drug response measured with Amax\n","    '''\n","    if feature_type in ['expression', 'both']:\n","        # Load gene expression\n","        expression = pd.read_csv('./ccle_data/expression.txt', delimiter='\\t', header=2, index_col=1).iloc[:, 1:]\n","        expression.columns = [c.split(' (ACH')[0] for c in expression.columns]\n","        features = expression\n","    if feature_type in ['mutation', 'both']:\n","        # Load gene mutation\n","        mutations = pd.read_csv('./ccle_data/mutation.txt', delimiter='\\t', header=2, index_col=1).iloc[:,1:]\n","        mutations = mutations.iloc[[c.endswith('_MUT') for c in mutations.index]]\n","        features = mutations\n","    if feature_type == 'both':\n","        # get cells having both expression and mutation data\n","        both_cells = set(expression.columns) & set(mutations.columns)\n","        z = {}\n","        for c in both_cells:\n","            exp = expression[c].values\n","            if len(exp.shape) > 1:\n","                exp = exp[:, 0]\n","            z[c] = np.concatenate([exp, mutations[c].values])\n","        both_df = pd.DataFrame(z, index=[c for c in expression.index] + [c for c in mutations.index])\n","        features = both_df\n","\n","    print('Genetic features dimension = {} on {} cancer cells'.format(features.shape[0], features.shape[1]))\n","\n","    # Get per-drug X and y regression targets\n","    response = pd.read_csv('./ccle_data/response.csv', header=0, index_col=[0, 2])\n","\n","    # names of cell lines, there are 504\n","    cells = response.index.levels[0]\n","    # names of drugs, there are 24\n","    drugs = response.index.levels[1]\n","\n","    X_drugs = [[] for _ in drugs]\n","    y_drugs = [[] for _ in drugs]\n","\n","    # subset data to include only cells, mutations and response associated with chosen drug\n","    for j, drug in enumerate(drugs):\n","            if drug_target is not None and drug != drug_target:\n","                continue # return to beginning of the loop\n","            for i, cell in enumerate(cells):\n","                if cell not in features.columns or (cell, drug) not in response.index:\n","                    continue\n","                # all j empty except index that corresponds to target drug\n","                # for this j we iteratively append all the mutations on every cell\n","                X_drugs[j].append(features[cell].values) # store genetic features (mutations and expression) that appear in cells\n","                y_drugs[j].append(response.loc[(cell, drug), 'Amax']) # store response of the drug\n","            print('{}: Cell number = {}'.format(drug, len(y_drugs[j])))\n","\n","    # convert to np array\n","    X_drugs = [np.array(x_i) for x_i in X_drugs]\n","    y_drugs = [np.array(y_i) for y_i in y_drugs]\n","\n","    if normalize:\n","        X_drugs = [(x_i if (len(x_i) == 0) else (x_i - x_i.min(axis=0, keepdims=True)) /\n","                                                (x_i.max(axis=0, keepdims=True) - x_i.min(axis=0, keepdims=True)))\n","                   for x_i in X_drugs]\n","        y_drugs = [(y_i if (len(y_i) == 0 or y_i.std() == 0) else (y_i - y_i.min(axis=0, keepdims=True)) /\n","                                                (y_i.max(axis=0, keepdims=True) - y_i.min(axis=0, keepdims=True)))\n","                   for y_i in y_drugs]\n","\n","    '''\n","    if normalize:\n","        X_drugs = [(x_i if (len(x_i) == 0) else (x_i - x_i.mean(axis=0, keepdims=True)) /\n","        x_i.std(axis=0).clip(1e-6)) for x_i in X_drugs]\n","        y_drugs = [(y_i if (len(y_i) == 0 or y_i.std() == 0) else (y_i - y_i.mean()) / y_i.std()) for y_i in y_drugs]\n","    '''\n","    drug_idx = drugs.get_loc(drug_target)\n","    # 2d array for features and 1d array for response\n","    X_drug, y_drug = X_drugs[drug_idx], y_drugs[drug_idx]\n","\n","    return X_drug, y_drug, features\n","\n","\n","# X_drug, y_drug, features = load_ccle(feature_type='mutation')\n","\n","\n","def ccle_feature_filter(X, y, threshold=0.1):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param threshold: correlation threshold\n","    :return: logical array with False for all features that do not have at least pearson correlation at threshold with y\n","    and correlations for all variables\n","    '''\n","    corrs = np.array([np.abs(np.corrcoef(x, y)[0, 1]) if x.std() > 0 else 0 for x in X.T])\n","    selected = corrs >= threshold # True/False\n","    print(selected.sum(), selected.shape, corrs)\n","    return selected, corrs\n","\n","# ccle_selected, corrs = ccle_feature_filter(X_drug, y_drug, threshold=0.1)\n","\n","# features.index[ccle_selected]\n","# stats.describe(corrs[ccle_selected])\n","\n","\n","def fit_elastic_net_ccle(X, y, nfolds=3):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param nfolds: number of folds for hyperparameter tuning\n","    :return: fitted elastic net model\n","    '''\n","    from sklearn.linear_model import ElasticNetCV\n","    # The parameter l1_ratio corresponds to alpha in the glmnet R package\n","    # while alpha corresponds to the lambda parameter in glmnet\n","    # enet = ElasticNetCV(l1_ratio=np.linspace(0.2, 1.0, 10),\n","    #                     alphas=np.exp(np.linspace(-6, 5, 250)),\n","    #                     cv=nfolds)\n","    enet = ElasticNetCV(l1_ratio=0.2, # It always chooses l1_ratio=0.2\n","                        alphas=np.exp(np.linspace(-6, 5, 250)),\n","                        cv=nfolds)\n","    print('Fitting via CV')\n","    enet.fit(X, y)\n","    alpha, l1_ratio = enet.alpha_, enet.l1_ratio_\n","    print('Chose values: alpha={}, l1_ratio={}'.format(alpha, l1_ratio))\n","    return enet\n","\n","# elastic_model = fit_elastic_net_ccle(X_drug[:,ccle_selected], y_drug)\n","\n","\n","def fit_random_forest_ccle(X, y):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param nfolds: number of folds for hyperparameter tuning\n","    :return: fitted elastic net model\n","    '''\n","    from sklearn.ensemble import RandomForestRegressor\n","\n","    rf = RandomForestRegressor()\n","\n","    rf.fit(X,y)\n","\n","    return rf\n","\n","# rf_model = fit_random_forest_ccle(X_drug[:,ccle_selected], y_drug)\n","\n","\n","def plot_ccle_predictions(model, X, y):\n","    from sklearn.metrics import r2_score\n","    plt.close()\n","    y_hat = model.predict(X)\n","    plt.scatter(y_hat, y, color='blue')\n","    plt.plot([min(y.min(), y_hat.min()), max(y.max(), y_hat.max())],\n","             [min(y.min(), y_hat.min()),max(y.max(), y_hat.max())], color='red', lw=3)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Truth')\n","    plt.title(' ($r^2$={:.4f})'.format( r2_score(y, y_hat)))\n","    plt.tight_layout()\n","\n","# plot_ccle_predictions(elastic_model,X_drug[:,ccle_selected],y_drug)\n","\n","\n","def print_top_features(model):\n","    # model_weights = np.mean([m.coef_ for m in model.models], axis=0)\n","    if model == rf_model:\n","        model_weights = model.feature_importances_\n","    else:\n","        model_weights = model.coef_\n","\n","    ccle_features = features[ccle_selected]\n","\n","    print('Top by fit:')\n","    for idx, top in enumerate(np.argsort(np.abs(model_weights))[::-1]):\n","        print('{}. {}: {:.4f}'.format(idx+1, ccle_features.index[top], model_weights[top]))\n","\n","# print_top_features(rf_model)\n","# print_top_features(elastic_model)\n","\n","\n","def run_test_ccle(X, Y):\n","    pval = []\n","    for x_index in range(X.shape[1]):\n","        z = np.delete(X, x_index, axis=1)\n","        x = X[:, x_index]\n","        x = x.reshape((len(x), 1))\n","        Y = Y.reshape((len(Y), 1))\n","        # now run test\n","        pval.append(GCIT(x, Y, z))\n","\n","    ccle_features = features[ccle_selected]\n","\n","    print('Top by fit:')\n","    for idx, top in enumerate(np.argsort(np.abs(pval))):\n","        print('{}. {}: {:.4f}'.format(idx+1, ccle_features.index[top], pval[top]))\n","\n","# run_test_ccle(X_drug[:,ccle_selected],y_drug)\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","def cost_xy(x, y, scaling_coef):\n","    '''\n","    L2 distance between vectors, using expanding and hence is more memory intensive\n","    :param x: x is tensor of shape [batch_size, x dims]\n","    :param y: y is tensor of shape [batch_size, y dims]\n","    :param scaling_coef: a scaling coefficient for distance between x and y\n","    :return: cost matrix: a matrix of size [batch_size, batch_size] where\n","    '''\n","    x = tf.expand_dims(x, 1)\n","    y = tf.expand_dims(y, 0)\n","    return tf.reduce_sum((x - y)**2, -1) * scaling_coef\n","\n","\n","def benchmark_sinkhorn(x, y, scaling_coef, epsilon=1.0, L=10):\n","    '''\n","    :param x: a tensor of shape [batch_size, sequence length]\n","    :param y: a tensor of shape [batch_size, sequence length]\n","    :param scaling_coef: a scaling coefficient for squared distance between x and y\n","    :param epsilon: (float) entropic regularity constant\n","    :param L: (int) number of iterations\n","    :return: V: (float) value of regularized optimal transport\n","    '''\n","    n_data = x.shape[0]\n","    # Note that batch size of x can be different from batch size of y\n","    m = 1.0 / tf.cast(n_data, tf.float64) * tf.ones(n_data, dtype=tf.float64)\n","    n = 1.0 / tf.cast(n_data, tf.float64) * tf.ones(n_data, dtype=tf.float64)\n","    m = tf.expand_dims(m, axis=1)\n","    n = tf.expand_dims(n, axis=1)\n","\n","    c_xy = cost_xy(x, y, scaling_coef)  # shape: [batch_size, batch_size]\n","\n","    k = tf.exp(-c_xy / epsilon) + 1e-09  # add 1e-09 to prevent numerical issues\n","    k_t = tf.transpose(k)\n","\n","    a = tf.expand_dims(tf.ones(n_data, dtype=tf.float64), axis=1)\n","    b = tf.expand_dims(tf.ones(n_data, dtype=tf.float64), axis=1)\n","\n","    for i in range(L):\n","        b = m / tf.matmul(k_t, a)  # shape: [m,]\n","        a = n / tf.matmul(k, b)  # shape: [m,]\n","\n","    return tf.reduce_sum(a * k * tf.reshape(b, (1, -1)) * c_xy)\n","\n","\n","def benchmark_loss(x, y, scaling_coef, sinkhorn_eps, sinkhorn_l, xp=None, yp=None):\n","    '''\n","    :param x: real data of shape [batch size, sequence length]\n","    :param y: fake data of shape [batch size, sequence length]\n","    :param scaling_coef: a scaling coefficient\n","    :param sinkhorn_eps: Sinkhorn parameter - epsilon\n","    :param sinkhorn_l: Sinkhorn parameter - the number of iterations\n","    :return: final Sinkhorn loss(and several values for monitoring the training process)\n","    '''\n","    if yp is None:\n","        yp = y\n","    if xp is None:\n","        xp = x\n","    x = tf.reshape(x, [x.shape[0], -1])\n","    y = tf.reshape(y, [y.shape[0], -1])\n","    xp = tf.reshape(xp, [xp.shape[0], -1])\n","    yp = tf.reshape(yp, [yp.shape[0], -1])\n","    loss_xy = benchmark_sinkhorn(x, y, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","    loss_xx = benchmark_sinkhorn(x, xp, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","    loss_yy = benchmark_sinkhorn(y, yp, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","\n","    loss = loss_xy - 0.5 * loss_xx - 0.5 * loss_yy\n","\n","    return loss\n","\n","class WGanGenerator(tf.keras.Model):\n","    '''\n","    class for WGAN generator\n","    Args:\n","        inputs, noise and confounding factor [v, z], of shape [batch size, z_dims + v_dims]\n","    return:\n","       fake samples of shape [batch size, x_dims]\n","    '''\n","    def __init__(self, n_samples, z_dims, h_dims, v_dims, x_dims, batch_size):\n","        super(WGanGenerator, self).__init__()\n","        self.n_samples = n_samples\n","        self.hidden_dims = h_dims\n","        self.batch_size = batch_size\n","        self.dz = z_dims\n","        self.dx = x_dims\n","        self.dv = v_dims\n","\n","        self.input_dim = self.dz + self.dv\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, self.hidden_dims]\n","        self.input_shape3 = [self.hidden_dims, self.dx]\n","\n","        self.w1 = self.xavier_var_creator(self.input_shape1)\n","        self.b1 = tf.Variable(tf.zeros(self.input_shape1[1], tf.float64))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator(self.input_shape3)\n","        self.b3 = tf.Variable(tf.zeros(self.input_shape3[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = 1.0 / tf.sqrt(input_shape[0] / 2.0)\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def call(self, inputs, training=None, mask=None):\n","        # inputs are concatenations of z and v\n","        z = tf.reshape(tensor=inputs, shape=[-1, self.input_dim])\n","        h1 = tf.nn.relu(tf.matmul(z, self.w1) + self.b1)\n","        # h2 = tf.nn.relu(tf.matmul(h1, self.w2) + self.b2)\n","        out = tf.math.sigmoid(tf.matmul(h1, self.w3) + self.b3)\n","        return out\n","\n","\n","class WGanDiscriminator(tf.keras.Model):\n","    '''\n","    class for WGAN discriminator\n","    Args:\n","        inputss: real and fake samples of shape [batch size, x_dims]\n","    return:\n","       features f_x of shape [batch size, features]\n","    '''\n","    def __init__(self, n_samples, z_dims, h_dims, v_dims, batch_size):\n","        super(WGanDiscriminator, self).__init__()\n","        self.n_samples = n_samples\n","        self.hidden_dims = h_dims\n","        self.batch_size = batch_size\n","\n","        self.input_dim = z_dims + v_dims\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, self.hidden_dims]\n","        self.input_shape3 = [self.hidden_dims, 1]\n","\n","        self.w1 = self.xavier_var_creator(self.input_shape1)\n","        self.b1 = tf.Variable(tf.zeros(self.input_shape1[1], tf.float64))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator(self.input_shape3)\n","        self.b3 = tf.Variable(tf.zeros(self.input_shape3[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = 1.0 / tf.sqrt(input_shape[0] / 2.0)\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def call(self, inputs, training=None, mask=None):\n","        # inputs are concatenations of z and v\n","        z = tf.reshape(tensor=inputs, shape=[self.batch_size, -1])\n","        z = tf.cast(z, tf.float64)\n","        h1 = tf.nn.relu(tf.matmul(z, self.w1) + self.b1)\n","        # h2 = tf.nn.sigmoid(tf.matmul(h1, self.w2) + self.b2)\n","        # out = tf.nn.sigmoid(tf.matmul(h1, self.w3) + self.b3)\n","        out = tf.matmul(h1, self.w3) + self.b3\n","        return out\n","\n","\n","class MINEDiscriminator(tf.keras.layers.Layer):\n","    '''\n","    class for MINE discriminator for benchmark GCIT\n","    '''\n","\n","    def __init__(self, in_dims, output_activation='linear'):\n","        super(MINEDiscriminator, self).__init__()\n","        self.output_activation = output_activation\n","        self.input_dim = in_dims\n","\n","        self.w1a = self.xavier_var_creator()\n","        self.w1b = self.xavier_var_creator()\n","        self.b1 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","        self.w2a = self.xavier_var_creator()\n","        self.w2b = self.xavier_var_creator()\n","        self.b2 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator()\n","        self.b3 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","    def xavier_var_creator(self):\n","        xavier_stddev = 1.0 / tf.sqrt(self.input_dim / 2.0)\n","        init = tf.random.normal(shape=[self.input_dim, ], mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(self.input_dim, ), trainable=True)\n","        return var\n","\n","    def mine_layer(self, x, x_hat, wa, wb, b):\n","        return tf.math.tanh(wa * x + wb * x_hat + b)\n","\n","    def call(self, x, x_hat):\n","        h1 = self.mine_layer(x, x_hat, self.w1a, self.w1b, self.b1)\n","        h2 = self.mine_layer(x, x_hat, self.w2a, self.w2b, self.b2)\n","        out = self.w3 * (h1 + h2) + self.b3\n","        return out, tf.exp(out)\n","\n","\n","class CharacteristicFunction:\n","    '''\n","    class to construct a function that represents the characteristic function\n","    '''\n","\n","    def __init__(self, size, x_dims, z_dims, test_size):\n","        self.n_samples = size\n","        self.hidden_dims = 20\n","        self.test_size = test_size\n","\n","        self.input_dim = z_dims + x_dims\n","        self.z_dims = z_dims\n","        self.x_dims = x_dims\n","        self.input_shape1x = [self.x_dims, self.hidden_dims]\n","        self.input_shape1z = [self.z_dims, self.hidden_dims]\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, 1]\n","\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = tf.sqrt(2.0 / (input_shape[0]))\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def update(self):\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","\n","    def call(self, x, z):\n","        # inputs are concatenations of z and v\n","        x = tf.reshape(tensor=x, shape=[self.test_size, -1, self.x_dims])\n","        z = tf.reshape(tensor=z, shape=[self.test_size, -1, self.z_dims])\n","        # we asssume parameter b for z to be 0\n","        h1 = tf.nn.sigmoid(tf.matmul(x, self.w1x) + self.b1)\n","        out = tf.nn.sigmoid(tf.matmul(h1, self.w2))\n","        return out\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","def generate_samples_random(size=1000, sType='CI', dx=1, dy=1, dz=20, nstd=0.05, alpha_x=0.05,\n","               normalize=True, seed=None, dist_z='gaussian', scaling_z = False):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    numbers_z = np.random.multinomial(num, [1/2.]*2, size=1)\n","    number_z_zeros = numbers_z[0][0]\n","    number_z_ones = numbers_z[0][1]\n","\n","    xy_arr = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    xy_z_zero_index = np.random.choice(a = len(xy_arr), size=number_z_zeros, p=[1/6., 1/3., 1/3., 1/6.])\n","    xy_z_one_index = np.random.choice(a = len(xy_arr), size=number_z_ones, p=[1/3., 1/6., 1/6., 1/3.])\n","\n","    xy_z_zero = xy_arr[xy_z_zero_index]\n","    xy_z_one = xy_arr[xy_z_one_index]\n","\n","    xy = np.concatenate((xy_z_zero, xy_z_one), axis=0)\n","\n","    x = xy[:, 0]\n","    y = xy[:, 1]\n","    z = np.concatenate((np.zeros(number_z_zeros), np.ones(number_z_ones)), axis=0)\n","\n","    indices = np.random.permutation(num)\n","\n","    x, y, z = x[indices], y[indices], z[indices]\n","    X, Y, Z = x.reshape(-1,1), y.reshape(-1,1), z.reshape(-1,1)\n","\n","    if normalize:\n","        Z = (Z - Z.min()) / (Z.max() - Z.min())\n","        X = (X - X.min()) / (X.max() - X.min())\n","        Y = (Y - Y.min()) / (Y.max() - Y.min())\n","    if scaling_z:\n","        Z = Z / Z.max()\n","    return np.array(X), np.array(Y), np.array(Z)\n","\n","\n","#\n","# test statistics for DGCIT\n","#\n","\n","\n","def t_and_sigma(psy_x_i, psy_y_i, phi_x_i, phi_y_i):\n","    b, n = psy_x_i.shape\n","    x_mtx = phi_x_i - psy_x_i\n","    y_mtx = phi_y_i - psy_y_i\n","    matrix = tf.reshape(x_mtx[None, :, :] * y_mtx[:, None, :], [-1, n])\n","    t_b = tf.reduce_sum(matrix, axis=1) / tf.cast(n, tf.float64)\n","    t_b = tf.expand_dims(t_b, axis=1)\n","\n","    crit_matrix = matrix - t_b\n","    std_b = tf.sqrt(tf.reduce_sum(crit_matrix**2, axis=1) / tf.cast(n-1, tf.float64))\n","    return t_b, std_b\n","\n","\n","def test_statistics(psy_x_i, psy_y_i, phi_x_i, phi_y_i, t_b, std_b, j):\n","    b, n = psy_x_i.shape\n","    x_mtx = phi_x_i - psy_x_i\n","    y_mtx = phi_y_i - psy_y_i\n","    matrix = tf.reshape(x_mtx[None, :, :] * y_mtx[:, None, :], [-1, n])\n","    crit_matrix = matrix - t_b\n","    test_stat = tf.reduce_max(tf.abs(tf.sqrt(tf.cast(n, tf.float64)) * tf.squeeze(t_b) / std_b))\n","\n","    sig = tf.reduce_sum(crit_matrix[None, :, :] * crit_matrix[:, None, :], axis=2)\n","    coef = std_b[None, :] * std_b[:, None] * tf.cast(n-1, tf.float64)\n","    sig_xy = sig / coef\n","\n","    eigenvalues, eigenvectors = tf.linalg.eigh(sig_xy)\n","    base = tf.zeros_like(eigenvectors)\n","    eig_vals = tf.sqrt(eigenvalues + 1e-12)\n","    lamda = tf.linalg.set_diag(base, eig_vals)\n","    sig_sqrt = tf.matmul(tf.matmul(eigenvectors, lamda), tf.linalg.inv(eigenvectors))\n","\n","    z_dist = tfp.distributions.Normal(0.0, scale=1.0)\n","    z_samples = z_dist.sample([b*b, j])\n","    z_samples = tf.cast(z_samples, tf.float64)\n","    vals = tf.matmul(sig_sqrt, z_samples)\n","    t_j = tf.reduce_max(vals, axis=0)\n","    return test_stat, t_j\n","\n","#\n","# Training algorithm for DGCIT\n","#\n","\n","\n","def dgcit(n=500, z_dim=100, simulation='type1error', batch_size=64, n_iter=1000, train_writer=None,\n","          current_iters=0, nstd=1.0, z_dist='gaussian', x_dims=1, y_dims=1, a_x=0.05, M=500, k=2,\n","          var_idx=1, b=30, j=1000):\n","    # generate samples x, y, z\n","    # arguments: size, sType='CI', dx=1, dy=1, dz=20, nstd=1, fixed_function='linear',\n","    # debug=False, normalize=True, seed=None, dist_z='gaussian'\n","    if simulation == 'type1error':\n","        # generate samples x, y, z under null hypothesis - x and y are conditional independent\n","        x, y, z = generate_samples_random(size=n, sType='CI', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd, alpha_x=a_x,\n","                                          dist_z=z_dist)\n","\n","    elif simulation == 'power':\n","        # generate samples x, y, z under alternative hypothesis - x and y are dependent\n","        x, y, z = generate_samples_random(size=n, sType='dependent', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd,\n","                                          alpha_x=a_x, dist_z=z_dist)\n","\n","    elif simulation == 'ccle':\n","        x_drug, y, features = load_ccle(feature_type='mutation', normalize=False)\n","\n","        ccle_selected, corrs = ccle_feature_filter(x_drug, y, threshold=0.05)\n","\n","        features.index[ccle_selected]\n","\n","        var_names = ['BRAF.MC_MUT', 'BRAF.V600E_MUT', 'HIP1_MUT', 'CDC42BPA_MUT', 'THBS3_MUT', 'DNMT1_MUT', 'PRKD1_MUT',\n","                     'FLT3_MUT', 'PIP5K1A_MUT', 'MAP3K5_MUT']\n","        idx = []\n","\n","        for var in var_names:\n","            idx.append(features.T.columns.get_loc(var))\n","\n","        x = x_drug[:, idx[5]]\n","        z = np.delete(x_drug, (idx[5]), axis=1).astype(np.float64)\n","        z_dim = z.shape[1]\n","\n","        x = np.expand_dims(x, axis=1).astype(np.float64)\n","        y = np.expand_dims(y, axis=1)\n","        n = y.shape[0]\n","    elif simulation == 'brain':\n","        path = './data/ADNI-Mediation-new.csv'\n","        df = pd.read_csv(path, header=None)\n","        y = df.loc[:, 7].values\n","        age = df.loc[:, 5].values\n","        tr_measures = df.loc[:, 12:79].values\n","        ct_measures = df.loc[:, 80:].values\n","        all_data = np.concatenate((np.expand_dims(age, axis=1), tr_measures), axis=1)\n","        all_data = np.concatenate((all_data, ct_measures), axis=1)\n","        x_idx = np.argwhere(np.isnan(all_data))[:, 0]\n","        y_idx = np.argwhere(np.isnan(y))[:, 0]\n","        idx = np.concatenate([x_idx, y_idx])\n","        idx = np.unique(idx)\n","        idx_diff = np.arange(0, idx.shape[0])\n","        remove_idx = idx - idx_diff\n","        for i in remove_idx:\n","            all_data = np.delete(all_data, i, axis=0)\n","            y = np.delete(y, i, axis=0)\n","\n","        all_data = np.delete(all_data, i, axis=0)\n","        y = np.delete(y, i, axis=0)\n","        x = all_data[:, var_idx]\n","        z = np.delete(all_data, var_idx, axis=1).astype(np.float64)\n","        z_dim = z.shape[1]\n","\n","        z = (z - z.min()) / (z.max() - z.min())\n","        x = (x - x.min()) / (x.max() - x.min())\n","        y = (y - y.min()) / (y.max() - y.min())\n","\n","        x = np.expand_dims(x, axis=1).astype(np.float64)\n","        y = np.expand_dims(y, axis=1)\n","        n = y.shape[0]\n","\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    psy_x_all = []\n","    phi_x_all = []\n","    psy_y_all = []\n","    phi_y_all = []\n","    test_samples = b\n","    test_size = int(n/k)\n","\n","    # split the train-test sets to k folds\n","    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n","    epochs = int(n_iter)\n","    for train_idx, test_idx in kf.split(x):\n","        x_train, y_train, z_train = x[train_idx], y[train_idx], z[train_idx]\n","\n","        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, z_train))\n","        # Repeat n epochs\n","        training = dataset.repeat(epochs)\n","        training_dataset = training.shuffle(100).batch(batch_size * 2)\n","        # test-set is the one left\n","        testing_dataset = tf.data.Dataset.from_tensor_slices((x[test_idx], y[test_idx], z[test_idx]))\n","\n","\n","        if z_dim <= 20:\n","            v_dims = int(3)\n","            h_dims = int(50)\n","\n","        else:\n","            v_dims = int(50)\n","            h_dims = int(512)\n","\n","        v_dist = tfp.distributions.Normal(0, scale=tf.sqrt(1.0 / 3.0))\n","        # create instance of G & D\n","        lr = 0.0005\n","        generator_x = WGanGenerator(n, z_dim, h_dims, v_dims, x_dims, batch_size)\n","        generator_y = WGanGenerator(n, z_dim, h_dims, v_dims, y_dims, batch_size)\n","        discriminator_x = WGanDiscriminator(n, z_dim, h_dims, x_dims, batch_size)\n","        discriminator_y = WGanDiscriminator(n, z_dim, h_dims, y_dims, batch_size)\n","\n","        gen_clipping_val = 0.5\n","        gen_clipping_norm = 1.0\n","        w_clipping_val = 0.5\n","        w_clipping_norm = 1.0\n","        scaling_coef = 1.0\n","        sinkhorn_eps = 0.8\n","        sinkhorn_l = 30\n","\n","        gx_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=gen_clipping_norm, clipvalue=gen_clipping_val)\n","        dx_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=w_clipping_norm, clipvalue=w_clipping_val)\n","        gy_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=gen_clipping_norm, clipvalue=gen_clipping_val)\n","        dy_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=w_clipping_norm, clipvalue=w_clipping_val)\n","\n","        @tf.function\n","        def x_update_d(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            fake_x = generator_x.call(gen_inputs)\n","            fake_x_p = generator_x.call(gen_inputs_p)\n","            d_fake = tf.concat([fake_x, real_z], axis=1)\n","            d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","\n","            with tf.GradientTape() as disc_tape:\n","                f_real = discriminator_x.call(d_real)\n","                f_fake = discriminator_x.call(d_fake)\n","                f_real_p = discriminator_x.call(d_real_p)\n","                f_fake_p = discriminator_x.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","\n","                loss1 = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l,\n","                                                f_real_p, f_fake_p)\n","                # disc_loss = - tf.math.minimum(loss1, 1)\n","                disc_loss = - loss1\n","            # update discriminator parameters\n","            d_grads = disc_tape.gradient(disc_loss, discriminator_x.trainable_variables)\n","            dx_optimiser.apply_gradients(zip(d_grads, discriminator_x.trainable_variables))\n","\n","        @tf.function\n","        def x_update_g(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            with tf.GradientTape() as gen_tape:\n","                fake_x = generator_x.call(gen_inputs)\n","                fake_x_p = generator_x.call(gen_inputs_p)\n","                d_fake = tf.concat([fake_x, real_z], axis=1)\n","                d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","                f_real = discriminator_x.call(d_real)\n","                f_fake = discriminator_x.call(d_fake)\n","                f_real_p = discriminator_x.call(d_real_p)\n","                f_fake_p = discriminator_x.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","                gen_loss = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l, f_real_p, f_fake_p)\n","            # update generator parameters\n","            generator_grads = gen_tape.gradient(gen_loss, generator_x.trainable_variables)\n","            gx_optimiser.apply_gradients(zip(generator_grads, generator_x.trainable_variables))\n","            return gen_loss\n","\n","        @tf.function\n","        def y_update_d(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            fake_x = generator_y.call(gen_inputs)\n","            fake_x_p = generator_y.call(gen_inputs_p)\n","            d_fake = tf.concat([fake_x, real_z], axis=1)\n","            d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","\n","            with tf.GradientTape() as disc_tape:\n","                f_real = discriminator_y.call(d_real)\n","                f_fake = discriminator_y.call(d_fake)\n","                f_real_p = discriminator_y.call(d_real_p)\n","                f_fake_p = discriminator_y.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","\n","                loss1 = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l,\n","                                                f_real_p, f_fake_p)\n","                disc_loss = - loss1\n","            # update discriminator parameters\n","            d_grads = disc_tape.gradient(disc_loss, discriminator_y.trainable_variables)\n","            dy_optimiser.apply_gradients(zip(d_grads, discriminator_y.trainable_variables))\n","\n","        @tf.function\n","        def y_update_g(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            with tf.GradientTape() as gen_tape:\n","                fake_x = generator_y.call(gen_inputs)\n","                fake_x_p = generator_y.call(gen_inputs_p)\n","                d_fake = tf.concat([fake_x, real_z], axis=1)\n","                d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","                f_real = discriminator_y.call(d_real)\n","                f_fake = discriminator_y.call(d_fake)\n","                f_real_p = discriminator_y.call(d_real_p)\n","                f_fake_p = discriminator_y.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","                gen_loss = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l, f_real_p, f_fake_p)\n","            # update generator parameters\n","            generator_grads = gen_tape.gradient(gen_loss, generator_y.trainable_variables)\n","            gy_optimiser.apply_gradients(zip(generator_grads, generator_y.trainable_variables))\n","            return gen_loss\n","\n","        for x_batch, y_batch, z_batch in training_dataset.take(n_iter):\n","            if x_batch.shape[0] != batch_size * 2:\n","                continue\n","\n","            # seperate the batch into two parts to train two gans\n","            x_batch1 = tf.convert_to_tensor(x_batch[:batch_size, ...])\n","            x_batch2 = tf.convert_to_tensor(x_batch[batch_size:, ...])\n","            y_batch1 = tf.convert_to_tensor(y_batch[:batch_size, ...])\n","            y_batch2 = tf.convert_to_tensor(y_batch[batch_size:, ...])\n","            z_batch1 = tf.convert_to_tensor(z_batch[:batch_size, ...])\n","            z_batch2 = tf.convert_to_tensor(z_batch[batch_size:, ...])\n","\n","            noise_v = v_dist.sample([batch_size, v_dims])\n","            noise_v = tf.cast(noise_v, tf.float64)\n","            noise_v_p = v_dist.sample([batch_size, v_dims])\n","            noise_v_p = tf.cast(noise_v_p, tf.float64)\n","            x_update_d(x_batch1, x_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","            loss_x = x_update_g(x_batch1, x_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","            y_update_d(y_batch1, y_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","            loss_y = y_update_g(y_batch1, y_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","\n","            current_iters += 1\n","        tf.keras.backend.clear_session()\n","        psy_x_b = []\n","        phi_x_b = []\n","        psy_y_b = []\n","        phi_y_b = []\n","\n","        x_samples = []\n","        y_samples = []\n","        z_input = []\n","        x_input = []\n","        y_input = []\n","\n","        # the following code generate x_1, ..., x_400 for all B and it takes 61 secs for one test\n","        for test_x, test_y, test_z in testing_dataset:\n","            test_z = tf.reshape(test_z, (1, z_dim))\n","            test_y = tf.reshape(test_y, (1, y_dims))\n","            test_x = tf.reshape(test_x, (1, x_dims))\n","\n","            tiled_z = tf.tile(test_z, [M, 1])\n","            noise_v = v_dist.sample([M, v_dims])\n","            noise_v = tf.cast(noise_v, tf.float64)\n","            g_inputs = tf.concat([tiled_z, noise_v], axis=1)\n","            # generator samples from G and evaluate from D\n","            fake_x = generator_x.call(g_inputs, training=False)\n","            fake_y = generator_y.call(g_inputs, training=False)\n","            x_samples.append(fake_x)\n","            y_samples.append(fake_y)\n","            z_input.append(test_z)\n","            x_input.append(test_x)\n","            y_input.append(test_y)\n","\n","        standardise = True\n","\n","        if standardise:\n","            x_samples = (x_samples - tf.reduce_mean(x_samples)) / tf.math.reduce_std(x_samples)\n","            y_samples = (y_samples - tf.reduce_mean(y_samples)) / tf.math.reduce_std(y_samples)\n","            x_input = (x_input - tf.reduce_mean(x_input)) / tf.math.reduce_std(x_input)\n","            y_input = (y_input - tf.reduce_mean(y_input)) / tf.math.reduce_std(y_input)\n","            z_input = (z_input - tf.reduce_mean(z_input)) / tf.math.reduce_std(z_input)\n","\n","        f1 = CharacteristicFunction(M, x_dims, z_dim, test_size)\n","        f2 = CharacteristicFunction(M, y_dims, z_dim, test_size)\n","        for i in range(test_samples):\n","            phi_x = tf.reduce_mean(f1.call(x_samples, z_input), axis=1)\n","            phi_y = tf.reduce_mean(f2.call(y_samples, z_input), axis=1)\n","            psy_x = tf.squeeze(f1.call(x_input, z_input))\n","            psy_y = tf.squeeze(f2.call(y_input, z_input))\n","\n","            psy_x_b.append(psy_x)\n","            phi_x_b.append(phi_x)\n","            psy_y_b.append(psy_y)\n","            phi_y_b.append(phi_y)\n","            f1.update()\n","            f2.update()\n","\n","        psy_x_all.append(psy_x_b)\n","        phi_x_all.append(phi_x_b)\n","        psy_y_all.append(psy_y_b)\n","        phi_y_all.append(phi_y_b)\n","\n","    # reshape\n","    psy_x_all = tf.reshape(psy_x_all, [k, test_samples, test_size])\n","    psy_y_all = tf.reshape(psy_y_all, [k, test_samples, test_size])\n","    phi_x_all = tf.reshape(phi_x_all, [k, test_samples, test_size])\n","    phi_y_all = tf.reshape(phi_y_all, [k, test_samples, test_size])\n","\n","    t_b = 0.0\n","    std_b = 0.0\n","    for n in range(k):\n","        t, std = t_and_sigma(psy_x_all[n], psy_y_all[n], phi_x_all[n], phi_y_all[n])\n","        t_b += t\n","        std_b += std\n","    t_b = t_b / tf.cast(k, tf.float64)\n","    std_b = std_b / tf.cast(k, tf.float64)\n","\n","    psy_x_all = tf.transpose(psy_x_all, (1, 0, 2))\n","    psy_y_all = tf.transpose(psy_y_all, (1, 0, 2))\n","    phi_x_all = tf.transpose(phi_x_all, (1, 0, 2))\n","    phi_y_all = tf.transpose(phi_y_all, (1, 0, 2))\n","\n","    psy_x_all = tf.reshape(psy_x_all, [test_samples, test_size*k])\n","    psy_y_all = tf.reshape(psy_y_all, [test_samples, test_size*k])\n","    phi_x_all = tf.reshape(phi_x_all, [test_samples, test_size*k])\n","    phi_y_all = tf.reshape(phi_y_all, [test_samples, test_size*k])\n","\n","    stat, critical_vals = test_statistics(psy_x_all, psy_y_all, phi_x_all, phi_y_all, t_b, std_b, j)\n","    comparison = [c > stat or c == stat for c in critical_vals]\n","    comparison = np.reshape(comparison, (-1,))\n","    p_value = np.sum(comparison.astype(np.float32)) / j\n","\n","    # del x_update_d, x_update_g, y_update_d, y_update_g\n","    # gc.collect()\n","    return p_value\n","\n","saved_file = \"{}-{}{}-{}-{}\".format(model, datetime.now().strftime(\"%h\"), datetime.now().strftime(\"%d\"),\n","                                    datetime.now().strftime(\"%H\"), datetime.now().strftime(\"%M\"))\n","log_dir = \"./trained/{}/log\".format(saved_file)\n","base_path = './trained/{}/'.format(saved_file)\n","train_writer = tf.summary.create_file_writer(logdir=log_dir)\n","\n","alpha = 0.1\n","alpha1 = 0.05\n","\n","tf.random.set_seed(set_seed)\n","np.random.seed(set_seed)\n","torch.manual_seed(set_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(set_seed)\n","\n","if test == 'power':\n","    for z_dim in z_dims_scheme:\n","        p_values = np.array([])\n","        p_values1 = []\n","        p_values5 = []\n","        test_count = 0\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","            p_value = 0.0\n","            p_value1 = 0.0\n","            p_value5 = 0.0\n","            if model == 'dgcit':\n","                p_value = dgcit(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                            n_iter=n_iters, train_writer=train_writer, current_iters=test_count * n_test,\n","                            nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                            k=k_value, b=b_value, j=j_value)\n","            else:\n","                raise ValueError('Test does not exist.')\n","\n","            test_count += 1\n","            print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1  for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            print('The p-value is {}'.format(p_value))\n","            print('Power: {} for z dimension {} with significance level {}'.format(final_result,z_dim, alpha))\n","            print('Power: {} for z dimension {} with significance level {}'.format(final_result1,z_dim, alpha1))\n","            gc.collect()\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}