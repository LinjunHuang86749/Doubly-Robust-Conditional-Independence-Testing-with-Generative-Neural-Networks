{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p6fkoQPNyLf0"},"outputs":[],"source":["param = {\n","  \"model\": 'gcit', # ['gcit']\n","  \"sample_size\": 2000, # number of samples\n","  \"batch_size\": 64,\n","  \"z_dim\": 250, # [50,100,150,200,250]\n","  \"dx\": 1,\n","  \"dy\": 1,\n","  \"test\": 'power', # ['type1error', 'power']\n","  \"n_test\": 500, ###original 500\n","  \"n_iters\": 3000, #original 1000\n","  \"eps_std\": 0.5,\n","  \"dist_z\": 'gaussian',#### choices=['gaussian', 'laplace']\n","  \"alpha_x\": 0.15, ## only used under alternative [0.15, 0.30, 0.45, 0.60, 0.75]\n","  \"m_value\": 100,\n","  \"k_value\": 2,\n","  \"b_value\": 30, #original 30\n","  \"j_value\": 1000, #original 1000##bootstrap number\n","  \"alpha\": 0.1,\n","  \"alpha1\": 0.05,\n","  \"set_seed\": 42\n","}\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# Utilites related to Sinkhorn computations and training for TensorFlow 2.0\n","import tensorflow as tf\n","import logging\n","import tensorflow_probability as tfp\n","from sklearn.metrics.pairwise import rbf_kernel\n","from scipy.stats import rankdata, ks_2samp, wilcoxon\n","from sklearn.model_selection import KFold\n","from datetime import datetime\n","import decimal\n","import gc # Garbage Collector\n","from tqdm import tqdm\n","\n","\n","\n","logging.getLogger('tensorflow').disabled = True\n","tf.keras.backend.set_floatx('float32')\n","\n","\n","'''\n","This code reproduces the real data experiments using the CCLE data.\n","Preprocessing steps follow the code of W. Tansey at https://github.com/tansey/hrt.\n","And the following code is obtained from https://github.com/alexisbellot/GCIT.\n","'''\n","\n","\n","def load_ccle(drug_target='PLX4720', feature_type='both', normalize=False):\n","    '''\n","    :param drug target: specific drug we w ant to analyse\n","    :param normalize: normalize data\n","    :return: genetic features (mutations) as a 2d array for each cancer cell and corresponding drug response measured with Amax\n","    '''\n","    if feature_type in ['expression', 'both']:\n","        # Load gene expression\n","        expression = pd.read_csv('./ccle_data/expression.txt', delimiter='\\t', header=2, index_col=1).iloc[:, 1:]\n","        expression.columns = [c.split(' (ACH')[0] for c in expression.columns]\n","        features = expression\n","    if feature_type in ['mutation', 'both']:\n","        # Load gene mutation\n","        mutations = pd.read_csv('./ccle_data/mutation.txt', delimiter='\\t', header=2, index_col=1).iloc[:,1:]\n","        mutations = mutations.iloc[[c.endswith('_MUT') for c in mutations.index]]\n","        features = mutations\n","    if feature_type == 'both':\n","        # get cells having both expression and mutation data\n","        both_cells = set(expression.columns) & set(mutations.columns)\n","        z = {}\n","        for c in both_cells:\n","            exp = expression[c].values\n","            if len(exp.shape) > 1:\n","                exp = exp[:, 0]\n","            z[c] = np.concatenate([exp, mutations[c].values])\n","        both_df = pd.DataFrame(z, index=[c for c in expression.index] + [c for c in mutations.index])\n","        features = both_df\n","\n","    print('Genetic features dimension = {} on {} cancer cells'.format(features.shape[0], features.shape[1]))\n","\n","    # Get per-drug X and y regression targets\n","    response = pd.read_csv('./ccle_data/response.csv', header=0, index_col=[0, 2])\n","\n","    # names of cell lines, there are 504\n","    cells = response.index.levels[0]\n","    # names of drugs, there are 24\n","    drugs = response.index.levels[1]\n","\n","    X_drugs = [[] for _ in drugs]\n","    y_drugs = [[] for _ in drugs]\n","\n","    # subset data to include only cells, mutations and response associated with chosen drug\n","    for j, drug in enumerate(drugs):\n","            if drug_target is not None and drug != drug_target:\n","                continue # return to beginning of the loop\n","            for i, cell in enumerate(cells):\n","                if cell not in features.columns or (cell, drug) not in response.index:\n","                    continue\n","                # all j empty except index that corresponds to target drug\n","                # for this j we iteratively append all the mutations on every cell\n","                X_drugs[j].append(features[cell].values) # store genetic features (mutations and expression) that appear in cells\n","                y_drugs[j].append(response.loc[(cell, drug), 'Amax']) # store response of the drug\n","            print('{}: Cell number = {}'.format(drug, len(y_drugs[j])))\n","\n","    # convert to np array\n","    X_drugs = [np.array(x_i) for x_i in X_drugs]\n","    y_drugs = [np.array(y_i) for y_i in y_drugs]\n","\n","    if normalize:\n","        X_drugs = [(x_i if (len(x_i) == 0) else (x_i - x_i.min(axis=0, keepdims=True)) /\n","                                                (x_i.max(axis=0, keepdims=True) - x_i.min(axis=0, keepdims=True)))\n","                   for x_i in X_drugs]\n","        y_drugs = [(y_i if (len(y_i) == 0 or y_i.std() == 0) else (y_i - y_i.min(axis=0, keepdims=True)) /\n","                                                (y_i.max(axis=0, keepdims=True) - y_i.min(axis=0, keepdims=True)))\n","                   for y_i in y_drugs]\n","\n","    '''\n","    if normalize:\n","        X_drugs = [(x_i if (len(x_i) == 0) else (x_i - x_i.mean(axis=0, keepdims=True)) /\n","        x_i.std(axis=0).clip(1e-6)) for x_i in X_drugs]\n","        y_drugs = [(y_i if (len(y_i) == 0 or y_i.std() == 0) else (y_i - y_i.mean()) / y_i.std()) for y_i in y_drugs]\n","    '''\n","    drug_idx = drugs.get_loc(drug_target)\n","    # 2d array for features and 1d array for response\n","    X_drug, y_drug = X_drugs[drug_idx], y_drugs[drug_idx]\n","\n","    return X_drug, y_drug, features\n","\n","\n","# X_drug, y_drug, features = load_ccle(feature_type='mutation')\n","\n","\n","def ccle_feature_filter(X, y, threshold=0.1):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param threshold: correlation threshold\n","    :return: logical array with False for all features that do not have at least pearson correlation at threshold with y\n","    and correlations for all variables\n","    '''\n","    corrs = np.array([np.abs(np.corrcoef(x, y)[0, 1]) if x.std() > 0 else 0 for x in X.T])\n","    selected = corrs >= threshold # True/False\n","    print(selected.sum(), selected.shape, corrs)\n","    return selected, corrs\n","\n","# ccle_selected, corrs = ccle_feature_filter(X_drug, y_drug, threshold=0.1)\n","\n","# features.index[ccle_selected]\n","# stats.describe(corrs[ccle_selected])\n","\n","\n","def fit_elastic_net_ccle(X, y, nfolds=3):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param nfolds: number of folds for hyperparameter tuning\n","    :return: fitted elastic net model\n","    '''\n","    from sklearn.linear_model import ElasticNetCV\n","    # The parameter l1_ratio corresponds to alpha in the glmnet R package\n","    # while alpha corresponds to the lambda parameter in glmnet\n","    # enet = ElasticNetCV(l1_ratio=np.linspace(0.2, 1.0, 10),\n","    #                     alphas=np.exp(np.linspace(-6, 5, 250)),\n","    #                     cv=nfolds)\n","    enet = ElasticNetCV(l1_ratio=0.2, # It always chooses l1_ratio=0.2\n","                        alphas=np.exp(np.linspace(-6, 5, 250)),\n","                        cv=nfolds)\n","    print('Fitting via CV')\n","    enet.fit(X, y)\n","    alpha, l1_ratio = enet.alpha_, enet.l1_ratio_\n","    print('Chose values: alpha={}, l1_ratio={}'.format(alpha, l1_ratio))\n","    return enet\n","\n","# elastic_model = fit_elastic_net_ccle(X_drug[:,ccle_selected], y_drug)\n","\n","\n","def fit_random_forest_ccle(X, y):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param nfolds: number of folds for hyperparameter tuning\n","    :return: fitted elastic net model\n","    '''\n","    from sklearn.ensemble import RandomForestRegressor\n","\n","    rf = RandomForestRegressor()\n","\n","    rf.fit(X,y)\n","\n","    return rf\n","\n","# rf_model = fit_random_forest_ccle(X_drug[:,ccle_selected], y_drug)\n","\n","\n","def plot_ccle_predictions(model, X, y):\n","    from sklearn.metrics import r2_score\n","    plt.close()\n","    y_hat = model.predict(X)\n","    plt.scatter(y_hat, y, color='blue')\n","    plt.plot([min(y.min(), y_hat.min()), max(y.max(), y_hat.max())],\n","             [min(y.min(), y_hat.min()),max(y.max(), y_hat.max())], color='red', lw=3)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Truth')\n","    plt.title(' ($r^2$={:.4f})'.format( r2_score(y, y_hat)))\n","    plt.tight_layout()\n","\n","# plot_ccle_predictions(elastic_model,X_drug[:,ccle_selected],y_drug)\n","\n","\n","def print_top_features(model):\n","    # model_weights = np.mean([m.coef_ for m in model.models], axis=0)\n","    if model == rf_model:\n","        model_weights = model.feature_importances_\n","    else:\n","        model_weights = model.coef_\n","\n","    ccle_features = features[ccle_selected]\n","\n","    print('Top by fit:')\n","    for idx, top in enumerate(np.argsort(np.abs(model_weights))[::-1]):\n","        print('{}. {}: {:.4f}'.format(idx+1, ccle_features.index[top], model_weights[top]))\n","\n","# print_top_features(rf_model)\n","# print_top_features(elastic_model)\n","\n","\n","def run_test_ccle(X, Y):\n","    pval = []\n","    for x_index in range(X.shape[1]):\n","        z = np.delete(X, x_index, axis=1)\n","        x = X[:, x_index]\n","        x = x.reshape((len(x), 1))\n","        Y = Y.reshape((len(Y), 1))\n","        # now run test\n","        pval.append(GCIT(x, Y, z))\n","\n","    ccle_features = features[ccle_selected]\n","\n","    print('Top by fit:')\n","    for idx, top in enumerate(np.argsort(np.abs(pval))):\n","        print('{}. {}: {:.4f}'.format(idx+1, ccle_features.index[top], pval[top]))\n","\n","# run_test_ccle(X_drug[:,ccle_selected],y_drug)\n","\n","\n","\n","def cost_xy(x, y, scaling_coef):\n","    '''\n","    L2 distance between vectors, using expanding and hence is more memory intensive\n","    :param x: x is tensor of shape [batch_size, x dims]\n","    :param y: y is tensor of shape [batch_size, y dims]\n","    :param scaling_coef: a scaling coefficient for distance between x and y\n","    :return: cost matrix: a matrix of size [batch_size, batch_size] where\n","    '''\n","    x = tf.expand_dims(x, 1)\n","    y = tf.expand_dims(y, 0)\n","    return tf.reduce_sum((x - y)**2, -1) * scaling_coef\n","\n","\n","def benchmark_sinkhorn(x, y, scaling_coef, epsilon=1.0, L=10):\n","    '''\n","    :param x: a tensor of shape [batch_size, sequence length]\n","    :param y: a tensor of shape [batch_size, sequence length]\n","    :param scaling_coef: a scaling coefficient for squared distance between x and y\n","    :param epsilon: (float) entropic regularity constant\n","    :param L: (int) number of iterations\n","    :return: V: (float) value of regularized optimal transport\n","    '''\n","    n_data = x.shape[0]\n","    # Note that batch size of x can be different from batch size of y\n","    m = 1.0 / tf.cast(n_data, tf.float64) * tf.ones(n_data, dtype=tf.float64)\n","    n = 1.0 / tf.cast(n_data, tf.float64) * tf.ones(n_data, dtype=tf.float64)\n","    m = tf.expand_dims(m, axis=1)\n","    n = tf.expand_dims(n, axis=1)\n","\n","    c_xy = cost_xy(x, y, scaling_coef)  # shape: [batch_size, batch_size]\n","\n","    k = tf.exp(-c_xy / epsilon) + 1e-09  # add 1e-09 to prevent numerical issues\n","    k_t = tf.transpose(k)\n","\n","    a = tf.expand_dims(tf.ones(n_data, dtype=tf.float64), axis=1)\n","    b = tf.expand_dims(tf.ones(n_data, dtype=tf.float64), axis=1)\n","\n","    for i in range(L):\n","        b = m / tf.matmul(k_t, a)  # shape: [m,]\n","        a = n / tf.matmul(k, b)  # shape: [m,]\n","\n","    return tf.reduce_sum(a * k * tf.reshape(b, (1, -1)) * c_xy)\n","\n","\n","def benchmark_loss(x, y, scaling_coef, sinkhorn_eps, sinkhorn_l, xp=None, yp=None):\n","    '''\n","    :param x: real data of shape [batch size, sequence length]\n","    :param y: fake data of shape [batch size, sequence length]\n","    :param scaling_coef: a scaling coefficient\n","    :param sinkhorn_eps: Sinkhorn parameter - epsilon\n","    :param sinkhorn_l: Sinkhorn parameter - the number of iterations\n","    :return: final Sinkhorn loss(and several values for monitoring the training process)\n","    '''\n","    if yp is None:\n","        yp = y\n","    if xp is None:\n","        xp = x\n","    x = tf.reshape(x, [x.shape[0], -1])\n","    y = tf.reshape(y, [y.shape[0], -1])\n","    xp = tf.reshape(xp, [xp.shape[0], -1])\n","    yp = tf.reshape(yp, [yp.shape[0], -1])\n","    loss_xy = benchmark_sinkhorn(x, y, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","    loss_xx = benchmark_sinkhorn(x, xp, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","    loss_yy = benchmark_sinkhorn(y, yp, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","\n","    loss = loss_xy - 0.5 * loss_xx - 0.5 * loss_yy\n","\n","    return loss\n","\n","class WGanGenerator(tf.keras.Model):\n","    '''\n","    class for WGAN generator\n","    Args:\n","        inputs, noise and confounding factor [v, z], of shape [batch size, z_dims + v_dims]\n","    return:\n","       fake samples of shape [batch size, x_dims]\n","    '''\n","    def __init__(self, n_samples, z_dims, h_dims, v_dims, x_dims, batch_size):\n","        super(WGanGenerator, self).__init__()\n","        self.n_samples = n_samples\n","        self.hidden_dims = h_dims\n","        self.batch_size = batch_size\n","        self.dz = z_dims\n","        self.dx = x_dims\n","        self.dv = v_dims\n","\n","        self.input_dim = self.dz + self.dv\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, self.hidden_dims]\n","        self.input_shape3 = [self.hidden_dims, self.dx]\n","\n","        self.w1 = self.xavier_var_creator(self.input_shape1)\n","        self.b1 = tf.Variable(tf.zeros(self.input_shape1[1], tf.float64))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator(self.input_shape3)\n","        self.b3 = tf.Variable(tf.zeros(self.input_shape3[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = 1.0 / tf.sqrt(input_shape[0] / 2.0)\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def call(self, inputs, training=None, mask=None):\n","        # inputs are concatenations of z and v\n","        z = tf.reshape(tensor=inputs, shape=[-1, self.input_dim])\n","        h1 = tf.nn.relu(tf.matmul(z, self.w1) + self.b1)\n","        # h2 = tf.nn.relu(tf.matmul(h1, self.w2) + self.b2)\n","        out = tf.math.sigmoid(tf.matmul(h1, self.w3) + self.b3)\n","        return out\n","\n","\n","class WGanDiscriminator(tf.keras.Model):\n","    '''\n","    class for WGAN discriminator\n","    Args:\n","        inputss: real and fake samples of shape [batch size, x_dims]\n","    return:\n","       features f_x of shape [batch size, features]\n","    '''\n","    def __init__(self, n_samples, z_dims, h_dims, v_dims, batch_size):\n","        super(WGanDiscriminator, self).__init__()\n","        self.n_samples = n_samples\n","        self.hidden_dims = h_dims\n","        self.batch_size = batch_size\n","\n","        self.input_dim = z_dims + v_dims\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, self.hidden_dims]\n","        self.input_shape3 = [self.hidden_dims, 1]\n","\n","        self.w1 = self.xavier_var_creator(self.input_shape1)\n","        self.b1 = tf.Variable(tf.zeros(self.input_shape1[1], tf.float64))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator(self.input_shape3)\n","        self.b3 = tf.Variable(tf.zeros(self.input_shape3[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = 1.0 / tf.sqrt(input_shape[0] / 2.0)\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def call(self, inputs, training=None, mask=None):\n","        # inputs are concatenations of z and v\n","        z = tf.reshape(tensor=inputs, shape=[self.batch_size, -1])\n","        z = tf.cast(z, tf.float64)\n","        h1 = tf.nn.relu(tf.matmul(z, self.w1) + self.b1)\n","        # h2 = tf.nn.sigmoid(tf.matmul(h1, self.w2) + self.b2)\n","        # out = tf.nn.sigmoid(tf.matmul(h1, self.w3) + self.b3)\n","        out = tf.matmul(h1, self.w3) + self.b3\n","        return out\n","\n","\n","class MINEDiscriminator(tf.keras.layers.Layer):\n","    '''\n","    class for MINE discriminator for benchmark GCIT\n","    '''\n","\n","    def __init__(self, in_dims, output_activation='linear'):\n","        super(MINEDiscriminator, self).__init__()\n","        self.output_activation = output_activation\n","        self.input_dim = in_dims\n","\n","        self.w1a = self.xavier_var_creator()\n","        self.w1b = self.xavier_var_creator()\n","        self.b1 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","        self.w2a = self.xavier_var_creator()\n","        self.w2b = self.xavier_var_creator()\n","        self.b2 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator()\n","        self.b3 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","    def xavier_var_creator(self):\n","        xavier_stddev = 1.0 / tf.sqrt(self.input_dim / 2.0)\n","        init = tf.random.normal(shape=[self.input_dim, ], mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(self.input_dim, ), trainable=True)\n","        return var\n","\n","    def mine_layer(self, x, x_hat, wa, wb, b):\n","        return tf.math.tanh(wa * x + wb * x_hat + b)\n","\n","    def call(self, x, x_hat):\n","        h1 = self.mine_layer(x, x_hat, self.w1a, self.w1b, self.b1)\n","        h2 = self.mine_layer(x, x_hat, self.w2a, self.w2b, self.b2)\n","        out = self.w3 * (h1 + h2) + self.b3\n","        return out, tf.exp(out)\n","\n","\n","class CharacteristicFunction:\n","    '''\n","    class to construct a function that represents the characteristic function\n","    '''\n","\n","    def __init__(self, size, x_dims, z_dims, test_size):\n","        self.n_samples = size\n","        self.hidden_dims = 20\n","        self.test_size = test_size\n","\n","        self.input_dim = z_dims + x_dims\n","        self.z_dims = z_dims\n","        self.x_dims = x_dims\n","        self.input_shape1x = [self.x_dims, self.hidden_dims]\n","        self.input_shape1z = [self.z_dims, self.hidden_dims]\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, 1]\n","\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = tf.sqrt(2.0 / (input_shape[0]))\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def update(self):\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","\n","    def call(self, x, z):\n","        # inputs are concatenations of z and v\n","        x = tf.reshape(tensor=x, shape=[self.test_size, -1, self.x_dims])\n","        z = tf.reshape(tensor=z, shape=[self.test_size, -1, self.z_dims])\n","        # we asssume parameter b for z to be 0\n","        h1 = tf.nn.sigmoid(tf.matmul(x, self.w1x) + self.b1)\n","        out = tf.nn.sigmoid(tf.matmul(h1, self.w2))\n","        return out\n","\n","\n","\n","\n","#\n","# The generate_samples_random function and rdc function were inspired by\n","# GCIT Github repository by Alexis Bellot1,2 Mihaela van der Schaar\n","# source: https://github.com/alexisbellot/GCIT\n","#\n","\n","def generate_samples_random(size=1000, sType='CI', dx=1, dy=1, dz=20, nstd=0.05, alpha_x=0.05,\n","               normalize=True, seed=None, dist_z='gaussian', scaling_z = False):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    if dist_z == 'gaussian':\n","        cov = np.eye(dz)\n","        mu = np.zeros(dz)\n","        Z = np.random.multivariate_normal(mu, cov, num)\n","\n","    elif dist_z == 'laplace':\n","        Z = np.random.laplace(loc=0.0, scale=1.0, size=num*dz)\n","        Z = np.reshape(Z, (num, dz))\n","\n","    Ax = np.random.rand(dz, dx)\n","    for i in range(dx):\n","        Ax[:, i] = Ax[:, i] / np.linalg.norm(Ax[:, i], ord=1)\n","    Ay = np.random.rand(dz, dy)\n","    for i in range(dy):\n","        Ay[:, i] = Ay[:, i] / np.linalg.norm(Ay[:, i], ord=1)\n","\n","    Axy = np.ones((dx, dy)) * alpha_x\n","\n","    if sType == 'CI':\n","        X = np.sin(np.matmul(Z, Ax) + nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","        Y = np.cos(np.matmul(Z, Ay) + nstd * np.random.multivariate_normal(np.zeros(dy), np.eye(dy), num))\n","    elif sType == 'I':\n","        X = np.sin(nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","        Y = np.cos(nstd * np.random.multivariate_normal(np.zeros(dy), np.eye(dy), num))\n","    else:\n","        X = np.sin(np.matmul(Z, Ax) + nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","        Y = np.cos(np.matmul(X, Axy) + np.matmul(Z, Ay) + nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","\n","    if normalize:\n","        Z = (Z - Z.min()) / (Z.max() - Z.min())\n","        X = (X - X.min()) / (X.max() - X.min())\n","        Y = (Y - Y.min()) / (Y.max() - Y.min())\n","    if scaling_z:\n","        Z = Z / Z.max()\n","    return np.array(X), np.array(Y), np.array(Z)\n","\n","\n","\n","\n","def rdc(x, y, f=np.sin, k=20, s=1 / 6., n=1):\n","    \"\"\"\n","    Computes the Randomized Dependence Coefficient\n","    x,y: numpy arrays 1-D or 2-D\n","         If 1-D, size (samples,)\n","         If 2-D, size (samples, variables)\n","    f:   function to use for random projection\n","    k:   number of random projections to use\n","    s:   scale parameter\n","    n:   number of times to compute the RDC and\n","         return the median (for stability)\n","    According to the paper, the coefficient should be relatively insensitive to\n","    the settings of the f, k, and s parameters.\n","\n","    Source: https://github.com/garydoranjr/rdc\n","    \"\"\"\n","    x = tf.reshape(x, shape=(x.shape[0], ))\n","    y = tf.reshape(y, shape=(y.shape[0], ))\n","\n","    if n > 1:\n","        values = []\n","        for i in range(n):\n","            try:\n","                values.append(rdc(x, y, f, k, s, 1))\n","            except np.linalg.linalg.LinAlgError:\n","                pass\n","        return np.median(values)\n","\n","    if len(x.shape) == 1: x = tf.reshape(x, shape=(-1, 1))\n","    if len(y.shape) == 1: y = tf.reshape(y, shape=(-1, 1))\n","\n","    # Copula Transformation\n","    cx = np.column_stack([rankdata(xc, method='ordinal') for xc in np.transpose(x)]) / float(x.shape[0])\n","    cy = np.column_stack([rankdata(yc, method='ordinal') for yc in np.transpose(y)]) / float(y.shape[0])\n","\n","    # Add a vector of ones so that w.x + b is just a dot product\n","    O = np.ones(cx.shape[0])\n","    X = np.column_stack([cx, O])\n","    Y = np.column_stack([cy, O])\n","\n","    # Random linear projections\n","    Rx = (s / X.shape[1]) * np.random.randn(X.shape[1], k)\n","    Ry = (s / Y.shape[1]) * np.random.randn(Y.shape[1], k)\n","    X = np.dot(X, Rx)\n","    Y = np.dot(Y, Ry)\n","\n","    # Apply non-linear function to random projections\n","    fX = f(X)\n","    fY = f(Y)\n","\n","    # Compute full covariance matrix\n","    C = np.cov(np.hstack([fX, fY]).T)\n","\n","    # Due to numerical issues, if k is too large,\n","    # then rank(fX) < k or rank(fY) < k, so we need\n","    # to find the largest k such that the eigenvalues\n","    # (canonical correlations) are real-valued\n","    k0 = k\n","    lb = 1\n","    ub = k\n","    while True:\n","        # Compute canonical correlations\n","        Cxx = C[:k, :k]\n","        Cyy = C[k0:k0 + k, k0:k0 + k]\n","        Cxy = C[:k, k0:k0 + k]\n","        Cyx = C[k0:k0 + k, :k]\n","\n","        eigs = np.linalg.eigvals(np.dot(np.dot(np.linalg.pinv(Cxx), Cxy),\n","                                        np.dot(np.linalg.pinv(Cyy), Cyx)))\n","\n","        # Binary search if k is too large\n","        if not (np.all(np.isreal(eigs)) and\n","                0 <= np.min(eigs) and\n","                np.max(eigs) <= 1):\n","            ub -= 1\n","            k = (ub + lb) // 2\n","            continue\n","        if lb == ub: break\n","        lb = k\n","        if ub == lb + 1:\n","            k = ub\n","        else:\n","            k = (ub + lb) // 2\n","\n","    return np.sqrt(np.max(eigs))\n","\n","\n","def permute(x):\n","    n = x.shape[0]\n","    idx = np.random.permutation(n)\n","    out = x[idx]\n","    return out\n","\n","#\n","# test statistics and GCIT method\n","# Paper link: https://arxiv.org/pdf/1907.04068.pdf\n","#\n","\n","\n","def mmd_squared(X, Y, gamma=1):\n","    X = X.reshape((len(X)), 1)\n","    Y = Y.reshape((len(Y)), 1)\n","\n","    K_XX = rbf_kernel(X, gamma=gamma)\n","    K_YY = rbf_kernel(Y, gamma=gamma)\n","    K_XY = rbf_kernel(X, Y, gamma=gamma)\n","\n","    n = K_XX.shape[0]\n","    m = K_YY.shape[0]\n","\n","    mmd_squared = (np.sum(K_XX)-np.trace(K_XX))/(n*(n-1)) + (np.sum(K_YY)-np.trace(K_YY))/(m*(m-1)) - 2 * np.sum(K_XY) / (m * n)\n","\n","    return mmd_squared\n","\n","\n","def correlation(X, Y):\n","    X = X.reshape((len(X)))\n","    Y = Y.reshape((len(Y)))\n","    return np.abs(np.corrcoef(X, Y)[0, 1])\n","\n","\n","def kolmogorov(X, Y):\n","    X = X.reshape((len(X)))\n","    Y = Y.reshape((len(Y)))\n","    return ks_2samp(X, Y)[0]\n","\n","\n","def wilcox(X, Y):\n","    X = X.reshape((len(X)))\n","    Y = Y.reshape((len(Y)))\n","    return wilcoxon(X, Y)[0]\n","\n","\n","def gcit_sinkhorn(n=1000, z_dim=100, simulation='type1error', statistic=\"rdc\", batch_size=64, nstd=0.5,\n","                  z_dist='gaussian', n_iter=1000, current_iters=0, a_x=0.01, var_idx=1):\n","    # generate samples x, y, z\n","    # arguments: size, sType='CI', dx=1, dy=1, dz=20, nstd=1, fixed_function='linear',\n","    # debug=False, normalize=True, seed=None, dist_z='gaussian'\n","    x_dims = 1\n","    y_dims = 1\n","    if simulation == 'type1error':\n","        # generate samples x, y, z under null hypothesis - x and y are conditional independent\n","        x, y, z = generate_samples_random(size=n, sType='CI', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd,\n","                                          dist_z=z_dist)\n","\n","    elif simulation == 'power':\n","        # generate samples x, y, z under alternative hypothesis - x and y are dependent\n","        x, y, z = generate_samples_random(size=n, sType='dependent', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd,\n","                                          dist_z=z_dist, alpha_x=a_x)\n","\n","    elif simulation == 'ccle':\n","        x_drug, y, features = load_ccle(feature_type='mutation', normalize=False)\n","\n","        ccle_selected, corrs = ccle_feature_filter(x_drug, y, threshold=0.05)\n","\n","        features.index[ccle_selected]\n","\n","        var_names = ['BRAF.MC_MUT', 'BRAF.V600E_MUT', 'HIP1_MUT', 'CDC42BPA_MUT', 'THBS3_MUT', 'DNMT1_MUT', 'PRKD1_MUT',\n","                     'FLT3_MUT', 'PIP5K1A_MUT', 'MAP3K5_MUT']\n","        idx = []\n","\n","        for var in var_names:\n","            idx.append(features.T.columns.get_loc(var))\n","\n","        x = x_drug[:, idx[0]]\n","\n","        z = np.delete(x_drug, (idx[0]), axis=1).astype(np.float64)\n","        z_dim = z.shape[1]\n","\n","        x = np.expand_dims(x, axis=1).astype(np.float64)\n","        y = np.expand_dims(y, axis=1)\n","        n = y.shape[0]\n","\n","    elif simulation == 'brain':\n","        path = './data/ADNI-Mediation-new.csv'\n","        df = pd.read_csv(path, header=None)\n","        y = df.loc[:, 7].values\n","        age = df.loc[:, 5].values\n","        tr_measures = df.loc[:, 12:79].values\n","        ct_measures = df.loc[:, 80:].values\n","        all_data = np.concatenate((np.expand_dims(age, axis=1), tr_measures), axis=1)\n","        all_data = np.concatenate((all_data, ct_measures), axis=1)\n","        x_idx = np.argwhere(np.isnan(all_data))[:, 0]\n","        y_idx = np.argwhere(np.isnan(y))[:, 0]\n","        idx = np.concatenate([x_idx, y_idx])\n","        idx = np.unique(idx)\n","        idx_diff = np.arange(0, idx.shape[0])\n","        remove_idx = idx - idx_diff\n","        for i in remove_idx:\n","            all_data = np.delete(all_data, i, axis=0)\n","            y = np.delete(y, i, axis=0)\n","\n","        all_data = np.delete(all_data, 0, axis=0)\n","        y = np.delete(y, 0, axis=0)\n","        x = all_data[:, var_idx]\n","        z = np.delete(all_data, (var_idx), axis=1).astype(np.float64)\n","        z_dim = z.shape[1]\n","\n","        z = (z - z.min()) / (z.max() - z.min())\n","        x = (x - x.min()) / (x.max() - x.min())\n","        y = (y - y.min()) / (y.max() - y.min())\n","\n","        x = np.expand_dims(x, axis=1).astype(np.float64)\n","        y = np.expand_dims(y, axis=1)\n","        n = y.shape[0]\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    # define training and testing subsets, training for learning the sampler and\n","    # testing for computing test statistic. Set 2/3 and 1/3 as default\n","    x_train, y_train, z_train = x[:int(2 * n / 3), ], y[:int(2 * n / 3), ], z[:int(2 * n / 3), ]\n","    # build data pipline for training set\n","    training_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, z_train))\n","    # Repeat n epochs\n","    epochs = int(n_iter)\n","    training_dataset = training_dataset.repeat(epochs)\n","    batched_training_set = training_dataset.shuffle(1000).batch(batch_size*2)\n","    # build data pipline for test set\n","    x_test, y_test, z_test = x[int(2 * n / 3):, ], y[int(2 * n / 3):, ], z[int(2 * n / 3):, ]\n","\n","    # no. of random and hidden dimensions\n","    if z_dim <= 20:\n","        v_dims = int(3)\n","        h_dims = int(3)\n","\n","    else:\n","        v_dims = 50\n","        h_dims = 512\n","\n","    # v_mean = tf.zeros(v_dim, dtype=tf.float64)\n","    v_dist = tfp.distributions.Normal(0, scale=tf.sqrt(1.0/3.0))\n","\n","    # create instance of G & D\n","    lr = 0.0005\n","    scaling_coef = 1.0\n","    sinkhorn_eps = 0.8\n","    sinkhorn_l = 30\n","    generator = WGanGenerator(n, z_dim, h_dims, v_dims, x_dims, batch_size)\n","    discriminator = WGanDiscriminator(n, z_dim, h_dims, x_dims, batch_size)\n","\n","    gen_clipping_val = 0.5\n","    gen_clipping_norm = 1.0\n","    w_clipping_val = 0.5\n","    w_clipping_norm = 1.0\n","\n","    gen_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=gen_clipping_norm, clipvalue=gen_clipping_val)\n","    disc_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=w_clipping_norm, clipvalue=w_clipping_val)\n","\n","    stat = None\n","\n","    # 2. Choice of statistic\n","    if statistic == \"corr\":\n","        stat = correlation\n","    if statistic == \"mmd\":\n","        stat = mmd_squared\n","    if statistic == \"kolmogorov\":\n","        stat = kolmogorov\n","    if statistic == \"wilcox\":\n","        stat = wilcox\n","    if statistic == \"rdc\":\n","        stat = rdc\n","\n","    @tf.function\n","    def update_d(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","        gen_inputs = tf.concat([real_z, v], axis=1)\n","        gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","        # concatenate real inputs for WGAN discriminator (x, z)\n","        d_real = tf.concat([real_x, real_z], axis=1)\n","        d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","        fake_x = generator.call(gen_inputs)\n","        fake_x_p = generator.call(gen_inputs_p)\n","        d_fake = tf.concat([fake_x, real_z], axis=1)\n","        d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","\n","        with tf.GradientTape() as disc_tape:\n","            f_real = discriminator.call(d_real)\n","            f_fake = discriminator.call(d_fake)\n","            f_real_p = discriminator.call(d_real_p)\n","            f_fake_p = discriminator.call(d_fake_p)\n","            # call compute loss using @tf.function + autograph\n","\n","            loss1 = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l, f_real_p, f_fake_p)\n","            disc_loss = - loss1\n","        # update discriminator parameters\n","        d_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","        disc_optimiser.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n","\n","    @tf.function\n","    def update_g(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","        gen_inputs = tf.concat([real_z, v], axis=1)\n","        gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","        # concatenate real inputs for WGAN discriminator (x, z)\n","        d_real = tf.concat([real_x, real_z], axis=1)\n","        d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","        with tf.GradientTape() as gen_tape:\n","            fake_x = generator.call(gen_inputs)\n","            fake_x_p = generator.call(gen_inputs_p)\n","            d_fake = tf.concat([fake_x, real_z], axis=1)\n","            d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","            f_real = discriminator.call(d_real)\n","            f_fake = discriminator.call(d_fake)\n","            f_real_p = discriminator.call(d_real_p)\n","            f_fake_p = discriminator.call(d_fake_p)\n","            # call compute loss using @tf.function + autograph\n","            gen_loss = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l, f_real_p, f_fake_p)\n","        # update generator parameters\n","        generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","        gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n","        return gen_loss\n","\n","    for x_batch, y_batch, z_batch in batched_training_set.take(n_iter):\n","        # for x_batch2, y_batch2, z_batch2 in batched_training_set1.take(1):\n","        if x_batch.shape[0] != batch_size * 2:\n","            continue\n","        x_batch1 = x_batch[0:batch_size, ...]\n","        x_batch2 = x_batch[batch_size:, ...]\n","        z_batch1 = z_batch[0:batch_size, ...]\n","        z_batch2 = z_batch[batch_size:, ...]\n","\n","        noise_v = v_dist.sample([batch_size, v_dims])\n","        noise_v = tf.cast(noise_v, tf.float64)\n","        noise_v_p = v_dist.sample([batch_size, v_dims])\n","        noise_v_p = tf.cast(noise_v_p, tf.float64)\n","        update_d(x_batch1, x_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","        loss = update_g(x_batch1, x_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","\n","        if tf.abs(loss) < 0.1:\n","            break\n","        current_iters += 1\n","\n","    test_samples = 1000\n","    rho = []\n","    test_size = z_test.shape[0]\n","\n","    for i in range(test_samples):\n","        v = v_dist.sample([test_size, v_dims])\n","        v = tf.cast(v, tf.float64)\n","        g_inputs = tf.concat([z_test, v], axis=1)\n","        # generator samples from G and evaluate from D\n","        fake_data = generator.call(g_inputs, training=False)\n","        rho.append(stat(fake_data, y_test))\n","    tf.keras.backend.clear_session()\n","    rho = tf.stack(rho)\n","    stat_real = stat(x_test, y_test)\n","    # p-value computation as a two-sided test\n","    p_value = min(tf.reduce_sum(tf.cast(rho < stat_real, tf.float32)) / test_samples,\n","                  tf.reduce_sum(tf.cast(rho > stat_real, tf.float32)) / test_samples)\n","    return p_value\n","\n","\n","def run_experiment(params):\n","\n","    model = params['model']\n","    sample_size = params['sample_size']\n","    batch_size = params['batch_size']\n","    z_dim = params['z_dim']\n","    dx = params['dx']\n","    dy = params['dy']\n","    test = params['test']\n","    n_test = params['n_test']\n","    n_iters = params['n_iters']\n","    eps_std = params['eps_std']\n","    dist_z = params['dist_z']\n","    alpha_x = params['alpha_x']\n","    alpha = params['alpha']\n","    alpha1 = params['alpha1']\n","    set_seed = params['set_seed']\n","\n","\n","\n","    tf.random.set_seed(set_seed)\n","    np.random.seed(set_seed)\n","\n","    if test == 'type1error':\n","        p_values = np.array([])\n","        p_values1 = []\n","        p_values5 = []\n","        test_count = 0\n","        for n in tqdm(range(n_test)):\n","            start_time = datetime.now()\n","            p_value = 0.0\n","            p_value1 = 0.0\n","            p_value5 = 0.0\n","\n","            if model == 'gcit':\n","                p_value = gcit_sinkhorn(n=sample_size, z_dim=z_dim, simulation=test, statistic=\"rdc\",\n","                                              batch_size=batch_size, n_iter=n_iters, nstd=eps_std, z_dist=dist_z)\n","            else:\n","                raise ValueError('Test does not exist.')\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            if model == 'gcit':\n","                p_values = np.append(p_values, p_value)\n","                fp = [pval < alpha / 2.0 for pval in p_values]## need to devide by 2\n","                final_result = np.mean(fp)\n","                fp1 = [pval < alpha1 / 2.0 for pval in p_values]## need to devide by 2\n","                final_result1 = np.mean(fp1)\n","            else:\n","                p_values = np.append(p_values, p_value)\n","                fp = [pval < alpha  for pval in p_values]##no need to devide by 2\n","                final_result = np.mean(fp)\n","                fp1 = [pval < alpha1 for pval in p_values]##no need to devide by 2\n","                final_result1 = np.mean(fp1)\n","\n","\n","            # print('The p-value is {}'.format(p_value))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","            gc.collect()\n","\n","    if test == 'power':\n","        p_values = np.array([])\n","        p_values1 = []\n","        p_values5 = []\n","        test_count = 0\n","        for n in tqdm(range(n_test)):\n","            start_time = datetime.now()\n","            p_value = 0.0\n","            p_value1 = 0.0\n","            p_value5 = 0.0\n","\n","            if model == 'gcit':\n","                p_value = gcit_sinkhorn(n=sample_size, z_dim=z_dim, simulation=test, statistic=\"rdc\",\n","                                              batch_size=batch_size, n_iter=n_iters,\n","                                              nstd=eps_std, z_dist=dist_z)\n","            else:\n","                raise ValueError('Test does not exist.')\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            if model == 'gcit':\n","                p_values = np.append(p_values, p_value)\n","                fp = [pval < alpha / 2.0 for pval in p_values]## need to devide by 2\n","                final_result = np.mean(fp)\n","                fp1 = [pval < alpha1 / 2.0 for pval in p_values]## need to devide by 2\n","                final_result1 = np.mean(fp1)\n","            else:\n","                p_values = np.append(p_values, p_value)\n","                fp = [pval < alpha  for pval in p_values]\n","                final_result = np.mean(fp)\n","                fp1 = [pval < alpha1  for pval in p_values]\n","                final_result1 = np.mean(fp1)\n","            # print('The p-value is {}'.format(p_value))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result,z_dim, alpha))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result1,z_dim, alpha1))\n","            gc.collect()\n","\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","    return p_values\n"]},{"cell_type":"code","source":["# @title code to get Emperical Rejection Rate for Figure 2 (a) GCIT\n","\n","param[\"test\"] = \"type1error\"\n","\n","for z_dim in [50, 100, 150, 200, 250]:\n","    param[\"z_dim\"] = z_dim\n","    p_val_list = run_experiment(param)"],"metadata":{"id":"9iYMBv2Kqql8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title code to get Size Adjusted Power for Figure 2 (b) GCIT\n","\n","param[\"test\"] = \"power\"\n","param[\"z_dim\"] = 200\n","\n","param[\"alpha\"] = 0.038 # 5% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\" param[\"z_dim\"] = 200\n","param[\"alpha1\"] = 0.078 # 10% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\" param[\"z_dim\"] = 200\n","\n","\n","for alpha_x in [0.15, 0.30, 0.45, 0.60, 0.75]:\n","    param[\"alpha_x\"] = alpha_x\n","    p_val_list = run_experiment(param)"],"metadata":{"id":"i2kIsWS6qq4g"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}