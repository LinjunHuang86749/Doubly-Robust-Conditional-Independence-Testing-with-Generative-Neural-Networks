{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p6fkoQPNyLf0"},"outputs":[],"source":["param = {\n","  \"model\": 'mgcit',\n","  \"sample_size\": 2000, # number of samples\n","  \"batch_size\": 64,\n","  \"z_dim\": 200, # [50,100,150,200,250,600,1300]\n","  \"dx\": 1,\n","  \"dy\": 1,\n","  \"test\": 'power', # ['type1error', 'power']\n","  \"n_test\": 500, ###original 500\n","  \"n_iters\": 3000, #original 1000\n","  \"eps_std\": 0.5,\n","  \"dist_z\": 'gaussian', #### choices=['gaussian', 'laplace']\n","  \"alpha_x\": 0.75, ##only used under alternative [0.15, 0.30, 0.45, 0.60, 0.75]\n","  \"m_value\": 100,\n","  \"k_value\": 2,\n","  \"b_value\": 30, #original 30\n","  \"j_value\": 1000, #original 1000##bootstrap number\n","  \"alpha\": 0.1,\n","  \"alpha1\": 0.05,\n","  \"set_seed\": 42,\n","}\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# Utilites related to Sinkhorn computations and training for TensorFlow 2.0\n","import tensorflow as tf\n","import logging\n","import tensorflow_probability as tfp\n","from sklearn.metrics.pairwise import rbf_kernel\n","from scipy.stats import rankdata, ks_2samp, wilcoxon\n","from sklearn.model_selection import KFold\n","from datetime import datetime\n","import decimal\n","import torch\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","import gc # Garbage Collector\n","from tqdm import tqdm\n","\n","logging.getLogger('tensorflow').disabled = True\n","tf.keras.backend.set_floatx('float32')\n","\n","\n","'''\n","This code reproduces the real data experiments using the CCLE data.\n","Preprocessing steps follow the code of W. Tansey at https://github.com/tansey/hrt.\n","And the following code is obtained from https://github.com/alexisbellot/GCIT.\n","'''\n","\n","\n","def load_ccle(drug_target='PLX4720', feature_type='both', normalize=False):\n","    '''\n","    :param drug target: specific drug we w ant to analyse\n","    :param normalize: normalize data\n","    :return: genetic features (mutations) as a 2d array for each cancer cell and corresponding drug response measured with Amax\n","    '''\n","    if feature_type in ['expression', 'both']:\n","        # Load gene expression\n","        expression = pd.read_csv('./ccle_data/expression.txt', delimiter='\\t', header=2, index_col=1).iloc[:, 1:]\n","        expression.columns = [c.split(' (ACH')[0] for c in expression.columns]\n","        features = expression\n","    if feature_type in ['mutation', 'both']:\n","        # Load gene mutation\n","        mutations = pd.read_csv('./ccle_data/mutation.txt', delimiter='\\t', header=2, index_col=1).iloc[:,1:]\n","        mutations = mutations.iloc[[c.endswith('_MUT') for c in mutations.index]]\n","        features = mutations\n","    if feature_type == 'both':\n","        # get cells having both expression and mutation data\n","        both_cells = set(expression.columns) & set(mutations.columns)\n","        z = {}\n","        for c in both_cells:\n","            exp = expression[c].values\n","            if len(exp.shape) > 1:\n","                exp = exp[:, 0]\n","            z[c] = np.concatenate([exp, mutations[c].values])\n","        both_df = pd.DataFrame(z, index=[c for c in expression.index] + [c for c in mutations.index])\n","        features = both_df\n","\n","    print('Genetic features dimension = {} on {} cancer cells'.format(features.shape[0], features.shape[1]))\n","\n","    # Get per-drug X and y regression targets\n","    response = pd.read_csv('./ccle_data/response.csv', header=0, index_col=[0, 2])\n","\n","    # names of cell lines, there are 504\n","    cells = response.index.levels[0]\n","    # names of drugs, there are 24\n","    drugs = response.index.levels[1]\n","\n","    X_drugs = [[] for _ in drugs]\n","    y_drugs = [[] for _ in drugs]\n","\n","    # subset data to include only cells, mutations and response associated with chosen drug\n","    for j, drug in enumerate(drugs):\n","            if drug_target is not None and drug != drug_target:\n","                continue # return to beginning of the loop\n","            for i, cell in enumerate(cells):\n","                if cell not in features.columns or (cell, drug) not in response.index:\n","                    continue\n","                # all j empty except index that corresponds to target drug\n","                # for this j we iteratively append all the mutations on every cell\n","                X_drugs[j].append(features[cell].values) # store genetic features (mutations and expression) that appear in cells\n","                y_drugs[j].append(response.loc[(cell, drug), 'Amax']) # store response of the drug\n","            print('{}: Cell number = {}'.format(drug, len(y_drugs[j])))\n","\n","    # convert to np array\n","    X_drugs = [np.array(x_i) for x_i in X_drugs]\n","    y_drugs = [np.array(y_i) for y_i in y_drugs]\n","\n","    if normalize:\n","        X_drugs = [(x_i if (len(x_i) == 0) else (x_i - x_i.min(axis=0, keepdims=True)) /\n","                                                (x_i.max(axis=0, keepdims=True) - x_i.min(axis=0, keepdims=True)))\n","                   for x_i in X_drugs]\n","        y_drugs = [(y_i if (len(y_i) == 0 or y_i.std() == 0) else (y_i - y_i.min(axis=0, keepdims=True)) /\n","                                                (y_i.max(axis=0, keepdims=True) - y_i.min(axis=0, keepdims=True)))\n","                   for y_i in y_drugs]\n","\n","    '''\n","    if normalize:\n","        X_drugs = [(x_i if (len(x_i) == 0) else (x_i - x_i.mean(axis=0, keepdims=True)) /\n","        x_i.std(axis=0).clip(1e-6)) for x_i in X_drugs]\n","        y_drugs = [(y_i if (len(y_i) == 0 or y_i.std() == 0) else (y_i - y_i.mean()) / y_i.std()) for y_i in y_drugs]\n","    '''\n","    drug_idx = drugs.get_loc(drug_target)\n","    # 2d array for features and 1d array for response\n","    X_drug, y_drug = X_drugs[drug_idx], y_drugs[drug_idx]\n","\n","    return X_drug, y_drug, features\n","\n","\n","# X_drug, y_drug, features = load_ccle(feature_type='mutation')\n","\n","\n","def ccle_feature_filter(X, y, threshold=0.1):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param threshold: correlation threshold\n","    :return: logical array with False for all features that do not have at least pearson correlation at threshold with y\n","    and correlations for all variables\n","    '''\n","    corrs = np.array([np.abs(np.corrcoef(x, y)[0, 1]) if x.std() > 0 else 0 for x in X.T])\n","    selected = corrs >= threshold # True/False\n","    print(selected.sum(), selected.shape, corrs)\n","    return selected, corrs\n","\n","# ccle_selected, corrs = ccle_feature_filter(X_drug, y_drug, threshold=0.1)\n","\n","# features.index[ccle_selected]\n","# stats.describe(corrs[ccle_selected])\n","\n","\n","def fit_elastic_net_ccle(X, y, nfolds=3):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param nfolds: number of folds for hyperparameter tuning\n","    :return: fitted elastic net model\n","    '''\n","    from sklearn.linear_model import ElasticNetCV\n","    # The parameter l1_ratio corresponds to alpha in the glmnet R package\n","    # while alpha corresponds to the lambda parameter in glmnet\n","    # enet = ElasticNetCV(l1_ratio=np.linspace(0.2, 1.0, 10),\n","    #                     alphas=np.exp(np.linspace(-6, 5, 250)),\n","    #                     cv=nfolds)\n","    enet = ElasticNetCV(l1_ratio=0.2, # It always chooses l1_ratio=0.2\n","                        alphas=np.exp(np.linspace(-6, 5, 250)),\n","                        cv=nfolds)\n","    print('Fitting via CV')\n","    enet.fit(X, y)\n","    alpha, l1_ratio = enet.alpha_, enet.l1_ratio_\n","    print('Chose values: alpha={}, l1_ratio={}'.format(alpha, l1_ratio))\n","    return enet\n","\n","# elastic_model = fit_elastic_net_ccle(X_drug[:,ccle_selected], y_drug)\n","\n","\n","def fit_random_forest_ccle(X, y):\n","    '''\n","    :param X: features\n","    :param y: response\n","    :param nfolds: number of folds for hyperparameter tuning\n","    :return: fitted elastic net model\n","    '''\n","    from sklearn.ensemble import RandomForestRegressor\n","\n","    rf = RandomForestRegressor()\n","\n","    rf.fit(X,y)\n","\n","    return rf\n","\n","# rf_model = fit_random_forest_ccle(X_drug[:,ccle_selected], y_drug)\n","\n","\n","def plot_ccle_predictions(model, X, y):\n","    from sklearn.metrics import r2_score\n","    plt.close()\n","    y_hat = model.predict(X)\n","    plt.scatter(y_hat, y, color='blue')\n","    plt.plot([min(y.min(), y_hat.min()), max(y.max(), y_hat.max())],\n","             [min(y.min(), y_hat.min()),max(y.max(), y_hat.max())], color='red', lw=3)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Truth')\n","    plt.title(' ($r^2$={:.4f})'.format( r2_score(y, y_hat)))\n","    plt.tight_layout()\n","\n","# plot_ccle_predictions(elastic_model,X_drug[:,ccle_selected],y_drug)\n","\n","\n","def print_top_features(model):\n","    # model_weights = np.mean([m.coef_ for m in model.models], axis=0)\n","    if model == rf_model:\n","        model_weights = model.feature_importances_\n","    else:\n","        model_weights = model.coef_\n","\n","    ccle_features = features[ccle_selected]\n","\n","    print('Top by fit:')\n","    for idx, top in enumerate(np.argsort(np.abs(model_weights))[::-1]):\n","        print('{}. {}: {:.4f}'.format(idx+1, ccle_features.index[top], model_weights[top]))\n","\n","# print_top_features(rf_model)\n","# print_top_features(elastic_model)\n","\n","\n","def run_test_ccle(X, Y):\n","    pval = []\n","    for x_index in range(X.shape[1]):\n","        z = np.delete(X, x_index, axis=1)\n","        x = X[:, x_index]\n","        x = x.reshape((len(x), 1))\n","        Y = Y.reshape((len(Y), 1))\n","        # now run test\n","        pval.append(GCIT(x, Y, z))\n","\n","    ccle_features = features[ccle_selected]\n","\n","    print('Top by fit:')\n","    for idx, top in enumerate(np.argsort(np.abs(pval))):\n","        print('{}. {}: {:.4f}'.format(idx+1, ccle_features.index[top], pval[top]))\n","\n","# run_test_ccle(X_drug[:,ccle_selected],y_drug)\n","\n","\n","def cost_xy(x, y, scaling_coef):\n","    '''\n","    L2 distance between vectors, using expanding and hence is more memory intensive\n","    :param x: x is tensor of shape [batch_size, x dims]\n","    :param y: y is tensor of shape [batch_size, y dims]\n","    :param scaling_coef: a scaling coefficient for distance between x and y\n","    :return: cost matrix: a matrix of size [batch_size, batch_size] where\n","    '''\n","    x = tf.expand_dims(x, 1)\n","    y = tf.expand_dims(y, 0)\n","    return tf.reduce_sum((x - y)**2, -1) * scaling_coef\n","\n","\n","def benchmark_sinkhorn(x, y, scaling_coef, epsilon=1.0, L=10):\n","    '''\n","    :param x: a tensor of shape [batch_size, sequence length]\n","    :param y: a tensor of shape [batch_size, sequence length]\n","    :param scaling_coef: a scaling coefficient for squared distance between x and y\n","    :param epsilon: (float) entropic regularity constant\n","    :param L: (int) number of iterations\n","    :return: V: (float) value of regularized optimal transport\n","    '''\n","    n_data = x.shape[0]\n","    # Note that batch size of x can be different from batch size of y\n","    m = 1.0 / tf.cast(n_data, tf.float64) * tf.ones(n_data, dtype=tf.float64)\n","    n = 1.0 / tf.cast(n_data, tf.float64) * tf.ones(n_data, dtype=tf.float64)\n","    m = tf.expand_dims(m, axis=1)\n","    n = tf.expand_dims(n, axis=1)\n","\n","    c_xy = cost_xy(x, y, scaling_coef)  # shape: [batch_size, batch_size]\n","\n","    k = tf.exp(-c_xy / epsilon) + 1e-09  # add 1e-09 to prevent numerical issues\n","    k_t = tf.transpose(k)\n","\n","    a = tf.expand_dims(tf.ones(n_data, dtype=tf.float64), axis=1)\n","    b = tf.expand_dims(tf.ones(n_data, dtype=tf.float64), axis=1)\n","\n","    for i in range(L):\n","        b = m / tf.matmul(k_t, a)  # shape: [m,]\n","        a = n / tf.matmul(k, b)  # shape: [m,]\n","\n","    return tf.reduce_sum(a * k * tf.reshape(b, (1, -1)) * c_xy)\n","\n","\n","def benchmark_loss(x, y, scaling_coef, sinkhorn_eps, sinkhorn_l, xp=None, yp=None):\n","    '''\n","    :param x: real data of shape [batch size, sequence length]\n","    :param y: fake data of shape [batch size, sequence length]\n","    :param scaling_coef: a scaling coefficient\n","    :param sinkhorn_eps: Sinkhorn parameter - epsilon\n","    :param sinkhorn_l: Sinkhorn parameter - the number of iterations\n","    :return: final Sinkhorn loss(and several values for monitoring the training process)\n","    '''\n","    if yp is None:\n","        yp = y\n","    if xp is None:\n","        xp = x\n","    x = tf.reshape(x, [x.shape[0], -1])\n","    y = tf.reshape(y, [y.shape[0], -1])\n","    xp = tf.reshape(xp, [xp.shape[0], -1])\n","    yp = tf.reshape(yp, [yp.shape[0], -1])\n","    loss_xy = benchmark_sinkhorn(x, y, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","    loss_xx = benchmark_sinkhorn(x, xp, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","    loss_yy = benchmark_sinkhorn(y, yp, scaling_coef, sinkhorn_eps, sinkhorn_l)\n","\n","    loss = loss_xy - 0.5 * loss_xx - 0.5 * loss_yy\n","\n","    return loss\n","\n","class WGanGenerator(tf.keras.Model):\n","    '''\n","    class for WGAN generator\n","    Args:\n","        inputs, noise and confounding factor [v, z], of shape [batch size, z_dims + v_dims]\n","    return:\n","       fake samples of shape [batch size, x_dims]\n","    '''\n","    def __init__(self, n_samples, z_dims, h_dims, v_dims, x_dims, batch_size):\n","        super(WGanGenerator, self).__init__()\n","        self.n_samples = n_samples\n","        self.hidden_dims = h_dims\n","        self.batch_size = batch_size\n","        self.dz = z_dims\n","        self.dx = x_dims\n","        self.dv = v_dims\n","\n","        self.input_dim = self.dz + self.dv\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, self.hidden_dims]\n","        self.input_shape3 = [self.hidden_dims, self.dx]\n","\n","        self.w1 = self.xavier_var_creator(self.input_shape1)\n","        self.b1 = tf.Variable(tf.zeros(self.input_shape1[1], tf.float64))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator(self.input_shape3)\n","        self.b3 = tf.Variable(tf.zeros(self.input_shape3[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = 1.0 / tf.sqrt(input_shape[0] / 2.0)\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def call(self, inputs, training=None, mask=None):\n","        # inputs are concatenations of z and v\n","        z = tf.reshape(tensor=inputs, shape=[-1, self.input_dim])\n","        h1 = tf.nn.relu(tf.matmul(z, self.w1) + self.b1)\n","        # h2 = tf.nn.relu(tf.matmul(h1, self.w2) + self.b2)\n","        out = tf.math.sigmoid(tf.matmul(h1, self.w3) + self.b3)\n","        return out\n","\n","\n","class WGanDiscriminator(tf.keras.Model):\n","    '''\n","    class for WGAN discriminator\n","    Args:\n","        inputss: real and fake samples of shape [batch size, x_dims]\n","    return:\n","       features f_x of shape [batch size, features]\n","    '''\n","    def __init__(self, n_samples, z_dims, h_dims, v_dims, batch_size):\n","        super(WGanDiscriminator, self).__init__()\n","        self.n_samples = n_samples\n","        self.hidden_dims = h_dims\n","        self.batch_size = batch_size\n","\n","        self.input_dim = z_dims + v_dims\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, self.hidden_dims]\n","        self.input_shape3 = [self.hidden_dims, 1]\n","\n","        self.w1 = self.xavier_var_creator(self.input_shape1)\n","        self.b1 = tf.Variable(tf.zeros(self.input_shape1[1], tf.float64))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator(self.input_shape3)\n","        self.b3 = tf.Variable(tf.zeros(self.input_shape3[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = 1.0 / tf.sqrt(input_shape[0] / 2.0)\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def call(self, inputs, training=None, mask=None):\n","        # inputs are concatenations of z and v\n","        z = tf.reshape(tensor=inputs, shape=[self.batch_size, -1])\n","        z = tf.cast(z, tf.float64)\n","        h1 = tf.nn.relu(tf.matmul(z, self.w1) + self.b1)\n","        # h2 = tf.nn.sigmoid(tf.matmul(h1, self.w2) + self.b2)\n","        # out = tf.nn.sigmoid(tf.matmul(h1, self.w3) + self.b3)\n","        out = tf.matmul(h1, self.w3) + self.b3\n","        return out\n","\n","\n","class MINEDiscriminator(tf.keras.layers.Layer):\n","    '''\n","    class for MINE discriminator for benchmark GCIT\n","    '''\n","\n","    def __init__(self, in_dims, output_activation='linear'):\n","        super(MINEDiscriminator, self).__init__()\n","        self.output_activation = output_activation\n","        self.input_dim = in_dims\n","\n","        self.w1a = self.xavier_var_creator()\n","        self.w1b = self.xavier_var_creator()\n","        self.b1 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","        self.w2a = self.xavier_var_creator()\n","        self.w2b = self.xavier_var_creator()\n","        self.b2 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","        self.w3 = self.xavier_var_creator()\n","        self.b3 = tf.Variable(tf.zeros([self.input_dim, ], tf.float64))\n","\n","    def xavier_var_creator(self):\n","        xavier_stddev = 1.0 / tf.sqrt(self.input_dim / 2.0)\n","        init = tf.random.normal(shape=[self.input_dim, ], mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(self.input_dim, ), trainable=True)\n","        return var\n","\n","    def mine_layer(self, x, x_hat, wa, wb, b):\n","        return tf.math.tanh(wa * x + wb * x_hat + b)\n","\n","    def call(self, x, x_hat):\n","        h1 = self.mine_layer(x, x_hat, self.w1a, self.w1b, self.b1)\n","        h2 = self.mine_layer(x, x_hat, self.w2a, self.w2b, self.b2)\n","        out = self.w3 * (h1 + h2) + self.b3\n","        return out, tf.exp(out)\n","\n","\n","class CharacteristicFunction:\n","    '''\n","    class to construct a function that represents the characteristic function\n","    '''\n","\n","    def __init__(self, size, x_dims, z_dims, test_size):\n","        self.n_samples = size\n","        self.hidden_dims = 20\n","        self.test_size = test_size\n","\n","        self.input_dim = z_dims + x_dims\n","        self.z_dims = z_dims\n","        self.x_dims = x_dims\n","        self.input_shape1x = [self.x_dims, self.hidden_dims]\n","        self.input_shape1z = [self.z_dims, self.hidden_dims]\n","        self.input_shape1 = [self.input_dim, self.hidden_dims]\n","        self.input_shape2 = [self.hidden_dims, 1]\n","\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","        self.b2 = tf.Variable(tf.zeros(self.input_shape2[1], tf.float64))\n","\n","    def xavier_var_creator(self, input_shape):\n","        xavier_stddev = tf.sqrt(2.0 / (input_shape[0]))\n","        init = tf.random.normal(shape=input_shape, mean=0.0, stddev=xavier_stddev)\n","        init = tf.cast(init, tf.float64)\n","        var = tf.Variable(init, shape=tf.TensorShape(input_shape), trainable=True)\n","        return var\n","\n","    def update(self):\n","        self.w1x = self.xavier_var_creator(self.input_shape1x)\n","        self.b1 = tf.squeeze(self.xavier_var_creator([self.hidden_dims, 1]))\n","        self.w2 = self.xavier_var_creator(self.input_shape2)\n","\n","    def call(self, x, z):\n","        # inputs are concatenations of z and v\n","        x = tf.reshape(tensor=x, shape=[self.test_size, -1, self.x_dims])\n","        z = tf.reshape(tensor=z, shape=[self.test_size, -1, self.z_dims])\n","        # we asssume parameter b for z to be 0\n","        h1 = tf.nn.sigmoid(tf.matmul(x, self.w1x) + self.b1)\n","        out = tf.nn.sigmoid(tf.matmul(h1, self.w2))\n","        return out\n","\n","\n","#\n","# The generate_samples_random function and rdc function were inspired by\n","# GCIT Github repository by Alexis Bellot1,2 Mihaela van der Schaar\n","# source: https://github.com/alexisbellot/GCIT\n","#\n","\n","def generate_samples_random(size=1000, sType='CI', dx=1, dy=1, dz=20, nstd=0.05, alpha_x=0.05,\n","               normalize=True, seed=None, dist_z='gaussian', scaling_z = False):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    if dist_z == 'gaussian':\n","        cov = np.eye(dz)\n","        mu = np.zeros(dz)\n","        Z = np.random.multivariate_normal(mu, cov, num)\n","\n","    elif dist_z == 'laplace':\n","        Z = np.random.laplace(loc=0.0, scale=1.0, size=num*dz)\n","        Z = np.reshape(Z, (num, dz))\n","\n","    Ax = np.random.rand(dz, dx)\n","    for i in range(dx):\n","        Ax[:, i] = Ax[:, i] / np.linalg.norm(Ax[:, i], ord=1)\n","    Ay = np.random.rand(dz, dy)\n","    for i in range(dy):\n","        Ay[:, i] = Ay[:, i] / np.linalg.norm(Ay[:, i], ord=1)\n","\n","    Axy = np.ones((dx, dy)) * alpha_x\n","\n","    if sType == 'CI':\n","        X = np.sin(np.matmul(Z, Ax) + nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","        Y = np.cos(np.matmul(Z, Ay) + nstd * np.random.multivariate_normal(np.zeros(dy), np.eye(dy), num))\n","    elif sType == 'I':\n","        X = np.sin(nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","        Y = np.cos(nstd * np.random.multivariate_normal(np.zeros(dy), np.eye(dy), num))\n","    else:\n","        X = np.sin(np.matmul(Z, Ax) + nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","        Y = np.cos(np.matmul(X, Axy) + np.matmul(Z, Ay) + nstd * np.random.multivariate_normal(np.zeros(dx), np.eye(dx), num))\n","\n","    if normalize:\n","        Z = (Z - Z.min()) / (Z.max() - Z.min())\n","        X = (X - X.min()) / (X.max() - X.min())\n","        Y = (Y - Y.min()) / (Y.max() - Y.min())\n","    if scaling_z:\n","        Z = Z / Z.max()\n","    return np.array(X), np.array(Y), np.array(Z)\n","\n","\n","#\n","# test statistics\n","#\n","\n","\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u = 1, sigma_v = 1,\n","                       boor_rv_type = \"gaussian\"):\n","\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n,1,1) - torch.swapaxes(z_torch.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    w_mx = torch.exp(-w_mx/sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.abs(y_torch.repeat(1,n) - y_torch.repeat(1,n).T)/sigma_u)\n","    u_mx_2 = torch.mean(torch.exp(-torch.abs(gen_y_all_torch.repeat(n,1,1) - y_torch.repeat(1, n).reshape(n,n,1))/sigma_u), dim = 2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n,1,1)\n","\n","    temp_mx = gen_y_all_torch_rep[:,:,0].T\n","    sum_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n,n,1))/sigma_u), dim = 2)\n","\n","    v_mx_1 = torch.exp(-torch.abs(x_torch.repeat(1,n) - x_torch.repeat(1,n).T)/sigma_v)\n","    v_mx_2 = torch.mean(torch.exp(-torch.abs(gen_x_all_torch.repeat(n,1,1) - x_torch.repeat(1, n).reshape(n,n,1))/sigma_v), dim = 2)\n","    v_mx_3 = v_mx_2.T\n","\n","    gen_x_all_torch_rep = gen_x_all_torch.repeat(n,1,1)\n","\n","    temp2_mx = gen_x_all_torch_rep[:,:,0].T\n","    sum2_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n,n,1))/sigma_v), dim = 2)\n","\n","    for i in range(1, M):\n","      temp_mx = gen_y_all_torch_rep[:,:,i].T\n","      temp_add_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n,n,1))/sigma_u), dim = 2)\n","      sum_mx = sum_mx + temp_add_mx\n","\n","      temp2_mx = gen_x_all_torch_rep[:,:,i].T\n","      temp2_add_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n,n,1))/sigma_v), dim = 2)\n","      sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1/M*sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1/M*sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx *w_mx * (1-torch.eye(n).to(device))\n","\n","    stat = 1/(n-1) * torch.sum(FF_mx).item()\n","\n","    boottemp = np.array([])\n","    if boor_rv_type == \"rademacher\":\n","      eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","      eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","      random_mx = torch.matmul(eboot[:,bb].reshape(-1,1), eboot[:,bb].reshape(-1,1).T)\n","      bootmatrix = FF_mx * random_mx\n","      stat_boot = 1/(n-1) * torch.sum(bootmatrix).item()\n","      boottemp = np.append(boottemp, stat_boot)\n","\n","    return stat, boottemp\n","\n","def mgcit(n=500, z_dim=100, simulation='type1error', batch_size=64, n_iter=1000, train_writer=None,\n","          current_iters=0, nstd=1.0, z_dist='gaussian', x_dims=1, y_dims=1, a_x=0.05, M=500, k=2,\n","          var_idx=1, b=30, j=1000):\n","    # generate samples x, y, z\n","    # arguments: size, sType='CI', dx=1, dy=1, dz=20, nstd=1, fixed_function='linear',\n","    # debug=False, normalize=True, seed=None, dist_z='gaussian'\n","    if simulation == 'type1error':\n","        # generate samples x, y, z under null hypothesis - x and y are conditional independent\n","        x, y, z = generate_samples_random(size=n, sType='CI', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd, alpha_x=a_x,\n","                            dist_z=z_dist, scaling_z=False, normalize=True)\n","\n","    elif simulation == 'power':\n","        # generate samples x, y, z under alternative hypothesis - x and y are dependent\n","        x, y, z = generate_samples_random(size=n, sType='dependent', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd,\n","                            alpha_x=a_x, dist_z=z_dist, scaling_z=False, normalize=True)\n","\n","    elif simulation == 'ccle':\n","        x_drug, y, features = load_ccle(feature_type='mutation', normalize=False)\n","\n","        ccle_selected, corrs = ccle_feature_filter(x_drug, y, threshold=0.05)\n","\n","        features.index[ccle_selected]\n","\n","        var_names = ['BRAF.MC_MUT', 'BRAF.V600E_MUT', 'HIP1_MUT', 'CDC42BPA_MUT', 'THBS3_MUT', 'DNMT1_MUT', 'PRKD1_MUT',\n","                     'FLT3_MUT', 'PIP5K1A_MUT', 'MAP3K5_MUT']\n","        idx = []\n","\n","        for var in var_names:\n","            idx.append(features.T.columns.get_loc(var))\n","\n","        x = x_drug[:, idx[5]]\n","        z = np.delete(x_drug, (idx[5]), axis=1).astype(np.float64)\n","        z_dim = z.shape[1]\n","\n","        x = np.expand_dims(x, axis=1).astype(np.float64)\n","        y = np.expand_dims(y, axis=1)\n","        n = y.shape[0]\n","    elif simulation == 'brain':\n","        path = './data/ADNI-Mediation-new.csv'\n","        df = pd.read_csv(path, header=None)\n","        y = df.loc[:, 7].values\n","        age = df.loc[:, 5].values\n","        tr_measures = df.loc[:, 12:79].values\n","        ct_measures = df.loc[:, 80:].values\n","        all_data = np.concatenate((np.expand_dims(age, axis=1), tr_measures), axis=1)\n","        all_data = np.concatenate((all_data, ct_measures), axis=1)\n","        x_idx = np.argwhere(np.isnan(all_data))[:, 0]\n","        y_idx = np.argwhere(np.isnan(y))[:, 0]\n","        idx = np.concatenate([x_idx, y_idx])\n","        idx = np.unique(idx)\n","        idx_diff = np.arange(0, idx.shape[0])\n","        remove_idx = idx - idx_diff\n","        for i in remove_idx:\n","            all_data = np.delete(all_data, i, axis=0)\n","            y = np.delete(y, i, axis=0)\n","\n","        all_data = np.delete(all_data, i, axis=0)\n","        y = np.delete(y, i, axis=0)\n","        x = all_data[:, var_idx]\n","        z = np.delete(all_data, var_idx, axis=1).astype(np.float64)\n","        z_dim = z.shape[1]\n","\n","        z = (z - z.min()) / (z.max() - z.min())\n","        x = (x - x.min()) / (x.max() - x.min())\n","        y = (y - y.min()) / (y.max() - y.min())\n","\n","        x = np.expand_dims(x, axis=1).astype(np.float64)\n","        y = np.expand_dims(y, axis=1)\n","        n = y.shape[0]\n","\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    # w_mx = np.linalg.norm(np.tile(z,(n,1,1)) - np.swapaxes(np.tile(z,(n,1,1)), 0, 1), ord = 1, axis = 2)\n","    # sigma_w = np.median(w_mx).item()\n","\n","    # u_mx = np.abs(np.tile(y,(1, n)) - np.tile(y,(1, n)).T)\n","    # sigma_u = np.median(u_mx).item()\n","\n","    # v_mx = np.abs(np.tile(x,(1, n)) - np.tile(x,(1, n)).T) )\n","    # sigma_v = np.median(v_mx).item()\n","\n","    test_size = int(n/k)\n","    stat_all = torch.zeros(k, 1)\n","    boot_temp_all = torch.zeros(k, j)\n","\n","    cur_k = 0\n","\n","    # split the train-test sets to k folds\n","    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n","    epochs = int(n_iter)\n","\n","    for train_idx, test_idx in kf.split(x):\n","        x_train, y_train, z_train = x[train_idx], y[train_idx], z[train_idx]\n","\n","        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, z_train))\n","        # Repeat n epochs\n","        training = dataset.repeat(epochs)\n","        training_dataset = training.shuffle(100).batch(batch_size * 2)\n","        # test-set is the one left\n","        testing_dataset = tf.data.Dataset.from_tensor_slices((x[test_idx], y[test_idx], z[test_idx]))\n","\n","        if z_dim <= 20:\n","            v_dims = int(3)\n","            h_dims = int(3)\n","\n","        else:\n","            v_dims = int(50)\n","            h_dims = int(512)\n","\n","        v_dist = tfp.distributions.Normal(0, scale=tf.sqrt(1.0 / 3.0))\n","        # create instance of G & D\n","        lr = 0.0005\n","        generator_x = WGanGenerator(n, z_dim, h_dims, v_dims, x_dims, batch_size)\n","        generator_y = WGanGenerator(n, z_dim, h_dims, v_dims, y_dims, batch_size)\n","        discriminator_x = WGanDiscriminator(n, z_dim, h_dims, x_dims, batch_size)\n","        discriminator_y = WGanDiscriminator(n, z_dim, h_dims, y_dims, batch_size)\n","\n","        gen_clipping_val = 0.5\n","        gen_clipping_norm = 1.0\n","        w_clipping_val = 0.5\n","        w_clipping_norm = 1.0\n","        scaling_coef = 1.0\n","        sinkhorn_eps = 0.8\n","        sinkhorn_l = 30\n","\n","        gx_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=gen_clipping_norm, clipvalue=gen_clipping_val)\n","        dx_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=w_clipping_norm, clipvalue=w_clipping_val)\n","        gy_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=gen_clipping_norm, clipvalue=gen_clipping_val)\n","        dy_optimiser = tf.keras.optimizers.Adam(lr, beta_1=0.5, clipnorm=w_clipping_norm, clipvalue=w_clipping_val)\n","\n","        @tf.function\n","        def x_update_d(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            fake_x = generator_x.call(gen_inputs)\n","            fake_x_p = generator_x.call(gen_inputs_p)\n","            d_fake = tf.concat([fake_x, real_z], axis=1)\n","            d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","\n","            with tf.GradientTape() as disc_tape:\n","                f_real = discriminator_x.call(d_real)\n","                f_fake = discriminator_x.call(d_fake)\n","                f_real_p = discriminator_x.call(d_real_p)\n","                f_fake_p = discriminator_x.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","\n","                loss1 = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l,\n","                                                f_real_p, f_fake_p)\n","                # disc_loss = - tf.math.minimum(loss1, 1)\n","                disc_loss = - loss1\n","            # update discriminator parameters\n","            d_grads = disc_tape.gradient(disc_loss, discriminator_x.trainable_variables)\n","            dx_optimiser.apply_gradients(zip(d_grads, discriminator_x.trainable_variables))\n","\n","        @tf.function\n","        def x_update_g(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            with tf.GradientTape() as gen_tape:\n","                fake_x = generator_x.call(gen_inputs)\n","                fake_x_p = generator_x.call(gen_inputs_p)\n","                d_fake = tf.concat([fake_x, real_z], axis=1)\n","                d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","                f_real = discriminator_x.call(d_real)\n","                f_fake = discriminator_x.call(d_fake)\n","                f_real_p = discriminator_x.call(d_real_p)\n","                f_fake_p = discriminator_x.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","                gen_loss = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l, f_real_p, f_fake_p)\n","            # update generator parameters\n","            generator_grads = gen_tape.gradient(gen_loss, generator_x.trainable_variables)\n","            gx_optimiser.apply_gradients(zip(generator_grads, generator_x.trainable_variables))\n","            return gen_loss\n","\n","        @tf.function\n","        def y_update_d(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            fake_x = generator_y.call(gen_inputs)\n","            fake_x_p = generator_y.call(gen_inputs_p)\n","            d_fake = tf.concat([fake_x, real_z], axis=1)\n","            d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","\n","            with tf.GradientTape() as disc_tape:\n","                f_real = discriminator_y.call(d_real)\n","                f_fake = discriminator_y.call(d_fake)\n","                f_real_p = discriminator_y.call(d_real_p)\n","                f_fake_p = discriminator_y.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","\n","                loss1 = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l,\n","                                                f_real_p, f_fake_p)\n","                disc_loss = - loss1\n","            # update discriminator parameters\n","            d_grads = disc_tape.gradient(disc_loss, discriminator_y.trainable_variables)\n","            dy_optimiser.apply_gradients(zip(d_grads, discriminator_y.trainable_variables))\n","\n","        @tf.function\n","        def y_update_g(real_x, real_x_p, real_z, real_z_p, v, v_p):\n","            gen_inputs = tf.concat([real_z, v], axis=1)\n","            gen_inputs_p = tf.concat([real_z_p, v_p], axis=1)\n","            # concatenate real inputs for WGAN discriminator (x, z)\n","            d_real = tf.concat([real_x, real_z], axis=1)\n","            d_real_p = tf.concat([real_x_p, real_z_p], axis=1)\n","            with tf.GradientTape() as gen_tape:\n","                fake_x = generator_y.call(gen_inputs)\n","                fake_x_p = generator_y.call(gen_inputs_p)\n","                d_fake = tf.concat([fake_x, real_z], axis=1)\n","                d_fake_p = tf.concat([fake_x_p, real_z_p], axis=1)\n","                f_real = discriminator_y.call(d_real)\n","                f_fake = discriminator_y.call(d_fake)\n","                f_real_p = discriminator_y.call(d_real_p)\n","                f_fake_p = discriminator_y.call(d_fake_p)\n","                # call compute loss using @tf.function + autograph\n","                gen_loss = benchmark_loss(f_real, f_fake, scaling_coef, sinkhorn_eps, sinkhorn_l, f_real_p, f_fake_p)\n","            # update generator parameters\n","            generator_grads = gen_tape.gradient(gen_loss, generator_y.trainable_variables)\n","            gy_optimiser.apply_gradients(zip(generator_grads, generator_y.trainable_variables))\n","            return gen_loss\n","\n","        for x_batch, y_batch, z_batch in training_dataset.take(n_iter):\n","\n","            if x_batch.shape[0] != batch_size * 2:\n","                continue\n","\n","            # seperate the batch into two parts to train two gans\n","            x_batch1 = tf.convert_to_tensor(x_batch[:batch_size, ...])\n","            x_batch2 = tf.convert_to_tensor(x_batch[batch_size:, ...])\n","            y_batch1 = tf.convert_to_tensor(y_batch[:batch_size, ...])\n","            y_batch2 = tf.convert_to_tensor(y_batch[batch_size:, ...])\n","            z_batch1 = tf.convert_to_tensor(z_batch[:batch_size, ...])\n","            z_batch2 = tf.convert_to_tensor(z_batch[batch_size:, ...])\n","\n","            noise_v = v_dist.sample([batch_size, v_dims])\n","            noise_v = tf.cast(noise_v, tf.float64)\n","            noise_v_p = v_dist.sample([batch_size, v_dims])\n","            noise_v_p = tf.cast(noise_v_p, tf.float64)\n","            x_update_d(x_batch1, x_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","            loss_x = x_update_g(x_batch1, x_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","            y_update_d(y_batch1, y_batch2, z_batch1, z_batch2, noise_v, noise_v_p,)\n","            loss_y = y_update_g(y_batch1, y_batch2, z_batch1, z_batch2, noise_v, noise_v_p)\n","\n","            current_iters += 1\n","        tf.keras.backend.clear_session()\n","        gen_x_all = torch.zeros(test_size, M)\n","        gen_y_all = torch.zeros(test_size, M)\n","        z_all = torch.zeros(test_size, z_dim)\n","        x_all = torch.zeros(test_size, x_dims)\n","        y_all = torch.zeros(test_size, y_dims)\n","\n","        cur_itr = 0\n","        # the following code generate x_1, ..., x_400 for all B and it takes 61 secs for one test\n","        for test_x, test_y, test_z in testing_dataset:\n","            test_z = tf.reshape(test_z, (1, z_dim))\n","            test_y = tf.reshape(test_y, (1, y_dims))\n","            test_x = tf.reshape(test_x, (1, x_dims))\n","\n","            tiled_z = tf.tile(test_z, [M, 1])\n","            noise_v = v_dist.sample([M, v_dims])\n","            noise_v = tf.cast(noise_v, tf.float64)\n","            g_inputs = tf.concat([tiled_z, noise_v], axis=1)\n","            # generator samples from G and evaluate from D\n","            fake_x = generator_x.call(g_inputs, training=False)\n","            fake_y = generator_y.call(g_inputs, training=False)\n","            gen_x_all[cur_itr,:] = torch.from_numpy(fake_x.numpy()).reshape(-1)\n","            gen_y_all[cur_itr,:] = torch.from_numpy(fake_y.numpy()).reshape(-1)\n","            x_all[cur_itr,:] = torch.from_numpy(test_x.numpy())\n","            y_all[cur_itr,:] = torch.from_numpy(test_y.numpy())\n","            z_all[cur_itr,:] = torch.from_numpy(test_z.numpy())\n","            cur_itr = cur_itr + 1\n","        standardise = True\n","\n","        if standardise:\n","            gen_x_all = (gen_x_all - torch.mean(gen_x_all, dim=0, keepdim=True)) / torch.std(gen_x_all, dim=0, keepdim=True)\n","            gen_y_all = (gen_y_all - torch.mean(gen_y_all, dim=0, keepdim=True)) / torch.std(gen_y_all, dim=0, keepdim=True)\n","            x_all = (x_all - torch.mean(x_all, dim=0, keepdim=True)) / torch.std(x_all, dim=0, keepdim=True)\n","            y_all = (y_all - torch.mean(y_all, dim=0, keepdim=True)) / torch.std(y_all, dim=0, keepdim=True)\n","            z_all = (z_all - torch.mean(z_all, dim=0, keepdim=True)) / torch.std(z_all, dim=0, keepdim=True)\n","\n","        w_mx = torch.linalg.vector_norm(z_all.repeat(test_size,1,1) - torch.swapaxes(z_all.repeat(test_size,1,1), 0, 1), ord = 1, dim = 2)\n","        sigma_w = torch.median(w_mx).item()\n","\n","        u_mx = torch.abs(y_all.repeat(1, test_size) - y_all.repeat(1, test_size).T)\n","        sigma_u = torch.median(u_mx).item()\n","\n","        v_mx = torch.abs(x_all.repeat(1, test_size) - x_all.repeat(1, test_size).T)\n","        sigma_v = torch.median(v_mx).item()\n","\n","        cur_stat, cur_boot_temp = get_p_value_stat_1(boot_num = j, M = M, n = test_size, gen_x_all_torch = gen_x_all.to(device), gen_y_all_torch = gen_y_all.to(device), x_torch = x_all.to(device), y_torch = y_all.to(device), z_torch = z_all.to(device), sigma_w = sigma_w, sigma_u = sigma_u, sigma_v = sigma_v,boor_rv_type = \"gaussian\")\n","        stat_all[cur_k,:] = cur_stat\n","        boot_temp_all[cur_k,:] = torch.from_numpy(cur_boot_temp)\n","        cur_k = cur_k + 1\n","    torch.cuda.empty_cache()\n","    return np.mean(torch.mean(boot_temp_all, dim = 0).numpy() > torch.mean(stat_all).item() )\n","\n","\n","\n","def run_experiment(params):\n","\n","    model = params['model']\n","    sample_size = params['sample_size']\n","    batch_size = params['batch_size']\n","    z_dim = params['z_dim']\n","    dx = params['dx']\n","    dy = params['dy']\n","    test = params['test']\n","    n_test = params['n_test']\n","    n_iters = params['n_iters']\n","    eps_std = params['eps_std']\n","    dist_z = params['dist_z']\n","    alpha_x = params['alpha_x']\n","    m_value = params['m_value']\n","    k_value = params['k_value']\n","    b_value = params['b_value']\n","    j_value = params['j_value']\n","    alpha = params['alpha']\n","    alpha1 = params['alpha1']\n","    set_seed = params['set_seed']\n","\n","\n","    saved_file = \"{}-{}{}-{}-{}\".format(model, datetime.now().strftime(\"%h\"), datetime.now().strftime(\"%d\"),\n","                      datetime.now().strftime(\"%H\"), datetime.now().strftime(\"%M\"))\n","    log_dir = \"./trained/{}/log\".format(saved_file)\n","    base_path = './trained/{}/'.format(saved_file)\n","    train_writer = tf.summary.create_file_writer(logdir=log_dir)\n","\n","\n","\n","    tf.random.set_seed(set_seed)\n","    np.random.seed(set_seed)\n","    torch.manual_seed(set_seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(set_seed)\n","\n","    if test == 'type1error':\n","        p_values = np.array([])\n","        p_values1 = []\n","        p_values5 = []\n","        test_count = 0\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","            p_value = 0.0\n","            p_value1 = 0.0\n","            p_value5 = 0.0\n","\n","            if model == 'mgcit':\n","                p_value = mgcit(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                            n_iter=n_iters, train_writer=train_writer, current_iters=test_count * n_test,\n","                            nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                            k=k_value, b=b_value, j=j_value)\n","            else:\n","                raise ValueError('Test does not exist.')\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]##no need to devide by 2\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]##no need to devide by 2\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The p-value is {}'.format(p_value))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","            gc.collect()\n","\n","    if test == 'power':\n","        p_values = np.array([])\n","        p_values1 = []\n","        p_values5 = []\n","        test_count = 0\n","        for n in range(n_test):\n","            start_time = datetime.now()\n","            p_value = 0.0\n","            p_value1 = 0.0\n","            p_value5 = 0.0\n","\n","            if model == 'mgcit':\n","                p_value = mgcit(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                            n_iter=n_iters, train_writer=train_writer, current_iters=test_count * n_test,\n","                            nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                            k=k_value, b=b_value, j=j_value)\n","            else:\n","                raise ValueError('Test does not exist.')\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1  for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The p-value is {}'.format(p_value))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result,z_dim, alpha))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result1,z_dim, alpha1))\n","            gc.collect()\n","\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result,z_dim, alpha))\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result1,z_dim, alpha1))\n","\n","    return p_values"]},{"cell_type":"code","source":["# @title code to get Emperical Rejection Rate for Figure 8 (a) (b) GANS\n","\n","param[\"test\"] = \"type1error\"\n","\n","for z_dim in [50, 100, 150, 200, 250]:\n","    param[\"z_dim\"] = z_dim\n","    p_val_list = run_experiment(param)"],"metadata":{"id":"AatQKpkQBApb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title code to get Size Adjusted Power for Figure 8 (c) (d) GANS\n","\n","param[\"test\"] = \"power\"\n","param[\"z_dim\"] = 200\n","\n","param[\"alpha\"] = 0.043 # 5% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\" param[\"z_dim\"] = 200\n","param[\"alpha1\"] = 0.084 # 10% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\" param[\"z_dim\"] = 200\n","\n","\n","for alpha_x in [0.15, 0.30, 0.45, 0.60, 0.75]:\n","    param[\"alpha_x\"] = alpha_x\n","    p_val_list = run_experiment(param)"],"metadata":{"id":"M-kpNP-oBDwP"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}