{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"96NggURkyHFt"},"outputs":[],"source":["\n","param = {\n","  \"test\": \"power\", # ['type1error', 'power']\n","  \"sample_size\": 400, # [200, 400, 600, 800, 1000]\n","  \"batch_size\": 64, # [32, 64, 128, 256]\n","  \"z_dim\": 1, # [5, 50, 250]\n","  \"dx\": 1,\n","  \"dy\": 1,\n","  \"n_test\": 1000, # [200, 2000] number of trails to run\n","  \"epochs_num\": 50, # [1000, 1500] number of epoch\n","  \"eps_std\" : 1.0, # epsilon std\n","  \"dist_z\" : 'gaussian', # ['laplace', 'gaussian']\n","  \"alpha_x\": 0.20, # only used under alternative [0.05, 0.10, 0.15, 0.20, 0.25]\n","  \"m_value\": 100, # [100, 200]\n","  \"k_value\": 8, # [2, 4, 6, 8] fold number\n","  \"j_value\": 1000, # [1000, 2000]\n","  \"noise_dimension\": 5, # [5, 10, 20]\n","  \"noise_dimension_type\": \"normal\",  # [\"normal\", \"unif\", \"Cauchy\"]\n","  \"noise_dimension_var\": 1, # [1, 4, 9, 16] the variance of the input noise if it is normal\n","  \"hidden_layer_size\": 1024, # [64, 128, 256, 512, 1024]\n","  \"normal_ini\": False, # [True, False]\n","  \"preprocess\": 'scale', # ['normalize', 'scale', 'None']\n","  \"G_lr\": 7e-5, # [5e-6, 1e-5, 2e-5] generator lr 1e-3\n","  \"alpha\": 0.1, # significance level 1\n","  \"alpha1\": 0.05, # significance level 2\n","  \"set_seeds\": 0,\n","  \"using_orcale\": False,\n","  \"lambda_1\": 1, # loss with Laplace kernel\n","  \"lambda_2\": 1,  # loss with Gaussian kernel\n","  \"using_Gen\": '1',  # ['1', '2'], types of generator \"1\" is fully connect, \"2\" is non fully\n","  \"boor_rv_type\":  'gaussian', # ['rademacher', 'gaussian']\n","  \"wgt_decay\": 1e-5, # weight decay for adam optimizer L2 regularization parameter\n","  \"lambda_3\": 1e-4, # L1 regularization parameter for rest of the layers\n","  \"lambda_4\": 2e-5, # L1 regularization parameter for the first layer\n","  \"drop_out_p\": 0.2, #  probability of an element to be zeroed. Default: 0.5, best 0.2\n","  \"is_sparse\": True, # [True, False], using sparse Ax, Ay\n","  \"sparse_ratio\": 0.05 # the ratio of non-zero elements in Ax, Ay\n","}\n","\n","\n","import torch\n","import torch.distributions as TD\n","from torch.utils.data import Dataset, DataLoader\n","from zmq import device\n","import torch.optim as optim\n","import numpy as np\n","from datetime import datetime\n","import functools\n","from tqdm import tqdm\n","\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","def generate_samples_random(size=1000, sType='H0', dx=1, dy=1, dz=1, nstd=1.0, alpha_x=0.05,\n","               preprocess=\"None\", dist_z='gaussian'):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","\n","    num = size\n","\n","    error_generator_x = TD.MultivariateNormal(\n","        torch.zeros(dx), 1 * torch.eye(dx))\n","\n","    error_generator_y = TD.MultivariateNormal(\n","        torch.zeros(dy), 1 * torch.eye(dy))\n","\n","    if dist_z == 'gaussian':\n","        z_generator = TD.MultivariateNormal( torch.zeros(dz), 1 * torch.eye(dz))\n","        Z = z_generator.sample((num,))\n","\n","    elif dist_z == 'laplace':\n","        z_generator = TD.MultivariateNormal( torch.zeros(dz), 1 * torch.eye(dz))\n","        Z = z_generator.sample((num,))\n","\n","    m = TD.Bernoulli(torch.tensor([alpha_x]))\n","\n","    Axy = torch.ones((dx, dy)) * alpha_x\n","\n","    if sType == 'H0':\n","        error_x = error_generator_x.sample((num,))\n","        error_y = error_generator_y.sample((num,))\n","        X = Z + nstd * error_x\n","        Y = Z + nstd * error_y\n","    else:\n","        error_x = error_generator_x.sample((num,))\n","        error_y = error_generator_y.sample((num,))\n","        delta = m.sample((num,))\n","        X = Z + (1 - delta) * nstd * error_x + delta * nstd * error_y\n","        Y = Z + nstd * error_y\n","\n","\n","    return X, Y, Z\n","\n","def generate_samples_from_fixed_Z_random(Z, size=1000, sType='H0', dx=1, dy=1, dz=1, nstd=1.0, alpha_x=0.05,\n","                     normalize=True, seed=None, dist_z='gaussian'):\n","    '''\n","    Generate CI,I or NI post-nonlinear samples for fixed Z\n","    1. Z is independent Gaussian or Laplace\n","    2. X = f1(<a,Z> + b + noise) and Y = f2(<c,Z> + d + noise) in case of CI\n","    Arguments:\n","        size : number of samples\n","        sType: CI, I, or NI\n","        dx: Dimension of X\n","        dy: Dimension of Y\n","        dz: Dimension of Z\n","        nstd: noise standard deviation\n","        we set f1 to be sin function and f2 to be cos function.\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    num = size\n","\n","    error_generator_x = TD.MultivariateNormal(\n","        torch.zeros(dx), 1 * torch.eye(dx))\n","\n","    error_generator_y = TD.MultivariateNormal(\n","        torch.zeros(dy), 1 * torch.eye(dy))\n","\n","    Axy = torch.ones((dx, dy)) * alpha_x\n","\n","    if sType == 'H0':\n","        error_x = error_generator_x.sample((num,))\n","        error_y = error_generator_y.sample((num,))\n","        X = Z + nstd * error_x\n","        Y = Z + nstd * error_y\n","    else:\n","        error_x = error_generator_x.sample((num,))\n","        error_y = error_generator_y.sample((num,))\n","        delta = m.sample((num,))\n","        X = Z + (1 - delta) * nstd * error_x + delta * nstd * error_y\n","        Y = Z + nstd * error_y\n","\n","    return X, Y\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u = 1, sigma_v = 1,\n","                       boor_rv_type = \"gaussian\"):\n","    \"\"\"\n","    Compute the p-value\n","\n","    Input:\n","    - boot_num: Integer giving the number of bootstrap samples.\n","    - M: Integer giving the number of training samples per batch.\n","    - n: Integer giving the number of training samples.\n","    - gen_x_all_torch: PyTorch Tensor (batch_size, M) of generated data of X.\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data of Y.\n","    - x_torch: PyTorch Tensor (batch_size) of training input X.\n","    - y_torch: PyTorch Tensor (batch_size) of training input Y.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z\n","    - sigma_w: Float of the bandwith of the Laplace kernel.\n","    - sigma_u: Float of the bandwith of the Laplace kernel.\n","    - sigma_v: Float of the bandwith of the Laplace kernel.\n","    - boor_rv_type: \"rademacher\" or \"gaussian\", specifying the reference distribution.\n","\n","    Output:\n","    - p_value: Float giving the p-value.\n","    \"\"\"\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n,1,1) - torch.swapaxes(z_torch.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    w_mx = torch.exp(-w_mx/sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.abs(y_torch.repeat(1,n) - y_torch.repeat(1,n).T)/sigma_u)\n","    u_mx_2 = torch.mean(torch.exp(-torch.abs(gen_y_all_torch.repeat(n,1,1) - y_torch.repeat(1, n).reshape(n,n,1))/sigma_u), dim = 2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n,1,1)\n","\n","    temp_mx = gen_y_all_torch_rep[:,:,0].T\n","    sum_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n,n,1))/sigma_u), dim = 2)\n","\n","    v_mx_1 = torch.exp(-torch.abs(x_torch.repeat(1,n) - x_torch.repeat(1,n).T)/sigma_v)\n","    v_mx_2 = torch.mean(torch.exp(-torch.abs(gen_x_all_torch.repeat(n,1,1) - x_torch.repeat(1, n).reshape(n,n,1))/sigma_v), dim = 2)\n","    v_mx_3 = v_mx_2.T\n","\n","    gen_x_all_torch_rep = gen_x_all_torch.repeat(n,1,1)\n","\n","    temp2_mx = gen_x_all_torch_rep[:,:,0].T\n","    sum2_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n,n,1))/sigma_v), dim = 2)\n","\n","    for i in range(1, M):\n","      temp_mx = gen_y_all_torch_rep[:,:,i].T\n","      temp_add_mx = torch.mean(torch.exp(-torch.abs(gen_y_all_torch_rep - temp_mx.reshape(n,n,1))/sigma_u), dim = 2)\n","      sum_mx = sum_mx + temp_add_mx\n","\n","      temp2_mx = gen_x_all_torch_rep[:,:,i].T\n","      temp2_add_mx = torch.mean(torch.exp(-torch.abs(gen_x_all_torch_rep - temp2_mx.reshape(n,n,1))/sigma_v), dim = 2)\n","      sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1/M*sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1/M*sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx *w_mx * (1-torch.eye(n).to(device))\n","\n","    stat = 1/(n-1) * torch.sum(FF_mx).item()\n","\n","    boottemp = np.array([])\n","    if boor_rv_type == \"rademacher\":\n","      eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","      eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","      random_mx = torch.matmul(eboot[:,bb].reshape(-1,1), eboot[:,bb].reshape(-1,1).T)\n","      bootmatrix = FF_mx * random_mx\n","      stat_boot = 1/(n-1) * torch.sum(bootmatrix).item()\n","      boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","class DatasetSelect(Dataset):\n","    \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - Z: PyTorch Tensor of shape (N, output_dimension) giving the training data of Z.\n","    \"\"\"\n","\n","\n","    def __init__(self, X, Y, Z):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index]\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","  \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - batch_size: Integer giving the batch size.\n","  \"\"\"\n","\n","  def __init__(self, X, Y, Z, batch_size):\n","    self.X_real = X\n","    self.Y_real = Y\n","    self.Z_real = Z\n","    self.batch_size = batch_size\n","    self.sample_size = X.shape[0]\n","\n","  def __len__(self):\n","    return self.sample_size\n","\n","  def __getitem__(self, index):\n","    return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[(self.batch_size+index) % self.sample_size]\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN_ver2(torch.utils.data.Dataset):\n","  \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - batch_size: Integer giving the batch size.\n","  \"\"\"\n","\n","  def __init__(self, Y, Z, batch_size):\n","    self.Y_real = Y\n","    self.Z_real = Z\n","    self.batch_size = batch_size\n","    self.sample_size = Z.shape[0]\n","\n","  def __len__(self):\n","    return self.sample_size\n","\n","  def __getitem__(self, index):\n","    return self.Y_real[index], self.Z_real[index]\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var = 1):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","##### GAN architecture #####\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input Z.\n","    - output_dimension: Integer giving the dimension of output X or Y.\n","    - noise_dimension: Integer giving the dimension of random noise.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","    - drop_out_p: Float giving the dropout probability.\n","    - drop_input: Boolean specifying whether to add dropout to the input layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the generator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                 drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","      else:\n","        if self.drop_input:\n","            x = self.drop_out0(x)\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","      return x\n","\n","class NonFullyConnected_1(torch.nn.Module):\n","\n","  def __init__(self, size_in, size_out, m, bias = True):\n","    super(NonFullyConnected_1, self).__init__()\n","    self.linear = torch.nn.Linear(m*size_in, m*size_out, bias = bias).to(device)\n","    self.mask = functools.reduce(torch.block_diag,[torch.ones(size_out, size_in) for i in range(m)]).to(device)\n","\n","  def forward(self, x):\n","\n","    self.linear.weight.data *= self.mask\n","    return self.linear(x)\n","\n","class Generator_2(torch.nn.Module):\n","    def __init__(\n","            self,\n","            input_dimension,\n","            output_dimension,\n","            noise_dimension,\n","            hidden_layer_size,\n","            BN_type,\n","            ReLU_coef,\n","            hidden_layer_depth = 1,\n","            ntargets_k=5):\n","        super(Generator_2, self).__init__()\n","        self.input_dimension = input_dimension + noise_dimension\n","        self.output_dimension = output_dimension\n","        self.ntargets_k = ntargets_k\n","        self.hidden_layer_sizes = [hidden_layer_size] * hidden_layer_depth\n","        self.BN_type = BN_type\n","        self.leakyrelu = torch.nn.LeakyReLU(ReLU_coef)\n","        self.linear_layers_from_input = torch.nn.Linear(self.input_dimension, ntargets_k*self.hidden_layer_sizes[0])\n","\n","        self.linear_layers_between = torch.nn.ModuleList([\n","            NonFullyConnected_1(self.hidden_layer_sizes[0],self.hidden_layer_sizes[0], ntargets_k)\n","            for i in range(len(self.hidden_layer_sizes))\n","        ])\n","        # self.linear8 = torch.nn.Linear(self.hidden_layer_sizes[0]*ntargets_k, self.hidden_layer_sizes[0]*ntargets_k)\n","        # self.linear8.weight = torch.nn.Parameter(torch.eye(self.hidden_layer_sizes[0]*ntargets_k), requires_grad=False)\n","        self.linear8 = torch.nn.Linear(self.hidden_layer_sizes[0] * ntargets_k, self.output_dimension)\n","        if BN_type:\n","            self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","    def forward(self, input):\n","        if self.BN_type:\n","            output = self.linear_layers_from_input(input)\n","            output = self.leakyrelu(self.BN1(output))\n","            for linear_layers_between in self.linear_layers_between:\n","                output = linear_layers_between(output)\n","                output = self.leakyrelu(self.BN1(output))\n","        else:\n","            output = self.linear_layers_from_input(input)\n","            output = self.leakyrelu(output)\n","            for linear_layers_between in self.linear_layers_between:\n","                output = linear_layers_between(output)\n","                output = self.leakyrelu(output)\n","\n","        return self.linear8(output) # torch.mean(self.linear8(output)).reshape(1)\n","\n","##### Training procedures #####\n","\n","\n","def find_loss(Y, hat_Y, Z, sigma_z, sigma_y):\n","    \"\"\"\n","    Compute the MMD loss via Laplace kernel.\n","\n","    Inputs:\n","    - y_torch: PyTorch Tensor (batch_size) of training input. (X or Y)\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z.\n","    - sigma_w: Float of the bandwith of the kernel.\n","    - sigma_u: Float of the bandwith of the kernel.\n","    - M: Number of training samples per batch.\n","\n","    Outputs:\n","    - loss: PyTorch Tensor containing the MMD loss.\n","    \"\"\"\n","\n","    n = Z.shape[0]\n","    mx_1_1 = torch.exp(-torch.abs(Y.repeat(1,n) - Y.repeat(1,n).T)/sigma_y)\n","    mx_1_2 = torch.linalg.vector_norm(Z.repeat(n,1,1) - torch.swapaxes(Z.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    # sigma = torch.median(mx_1_2)\n","    mx_1_2 = torch.exp(-mx_1_2/sigma_z)\n","    mx_1 = mx_1_1 * mx_1_2\n","\n","    mx_2_1 = torch.exp(-torch.abs(Y.repeat(1,n) - hat_Y.repeat(1,n).T)/sigma_y)\n","    mx_2 = mx_2_1 * mx_1_2\n","\n","    mx_3 = mx_2.T\n","\n","    mx_4_1 = torch.exp(-torch.abs(hat_Y.repeat(1,n) - hat_Y.repeat(1,n).T)/sigma_y)\n","    mx_4 = mx_4_1 * mx_1_2\n","\n","    FF_mx = (mx_1 - mx_2 - mx_3 + mx_4)\n","    loss =  1/(n**2) * torch.sum(FF_mx)\n","    return loss\n","\n","def find_loss_2(Y, hat_Y, Z, sigma_z, sigma_y):\n","    \"\"\"\n","    Compute the MMD loss via Gaussian kernel.\n","\n","    Inputs:\n","    - y_torch: PyTorch Tensor (batch_size) of training input. (X or Y)\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z.\n","    - sigma_w: Float of the bandwith of the kernel.\n","    - sigma_u: Float of the bandwith of the kernel.\n","    - M: Number of training samples per batch.\n","\n","    Outputs:\n","    - loss: PyTorch Tensor containing the MMD loss.\n","    \"\"\"\n","\n","    n = Z.shape[0]\n","    mx_1_1 = torch.exp(-(Y.repeat(1,n) - Y.repeat(1,n).T) ** 2/sigma_y)\n","    mx_1_2 = torch.linalg.vector_norm(Z.repeat(n,1,1) - torch.swapaxes(Z.repeat(n,1,1), 0, 1), ord = 2, dim = 2)\n","    # sigma = torch.median(mx_1_2)\n","    mx_1_2 = torch.exp(-(mx_1_2 ** 2)/sigma_z)\n","    mx_1 = mx_1_1 * mx_1_2\n","\n","    mx_2_1 = torch.exp(-(Y.repeat(1,n) - hat_Y.repeat(1,n).T) ** 2/sigma_y)\n","    mx_2 = mx_2_1 * mx_1_2\n","\n","    mx_3 = mx_2.T\n","\n","    mx_4_1 = torch.exp(-(hat_Y.repeat(1,n) - hat_Y.repeat(1,n).T) ** 2/sigma_y)\n","    mx_4 = mx_4_1 * mx_1_2\n","\n","    FF_mx = (mx_1 - mx_2 - mx_3 + mx_4)\n","    loss =  1/(n**2) * torch.sum(FF_mx)\n","    return loss\n","\n","def train_ver3(X, Y, Z, X_test, Y_test, Z_test, M,\n","      noise_dimension, noise_type, G_lr, hidden_layer_size,\n","      DataLoader, BN_type, ReLU_coef,\n","      epochs_num=10,  sigma_z = 1, sigma_x = 1, sigma_y = 1,\n","      normal_ini = False,\n","      lambda_1 = 1, lambda_2 = 1, using_Gen = '1', wgt_decay = 0,\n","      lambda_3 = 0, drop_out_p = 0.2, noise_dimension_var = 1,\n","      lambda_4 = 0):\n","    \"\"\"\n","    Train loop for GAN.\n","\n","    Inputs:\n","    - X: PyTorch Tensor (sample_size, dimension_X) of training input.\n","    - Y: PyTorch Tensor (sample_size, dimension_Y) of training input.\n","    - Z: PyTorch Tensor (sample_size, dimension_Z) of training input.\n","    - X_test: PyTorch Tensor (sample_size, dimension_X) of test input.\n","    - Y_test: PyTorch Tensor (sample_size, dimension_Y) of test input.\n","    - Z_test: PyTorch Tensor (sample_size, dimension_Z) of test input.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","    - G_lr: Float giving the learning rate of the generator.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - DataLoader: DataLoader object used to generate training batches.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Float giving the coefficient of the Leaky ReLU layer.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - sigma_z: Float of the bandwith of the kernel.\n","    - sigma_x: Float of the bandwith of the kernel.\n","    - sigma_y: Float of the bandwith of the kernel.\n","    - normal_ini: Boolean specifying whether to initialize the generator with normal initialization.\n","    - lambda_1: Float giving the coefficient of the MMD loss using Laplace kernel.\n","    - lambda_2: Float giving the coefficient of the MMD loss using Gaussian kernel. (not using)\n","    - using_Gen: '1' or '2' specifying whether to use the first or second generator.(not using)\n","    - wgt_decay: Float giving the weight decay. (L2 regularization)\n","    - lambda_3: Scalar giving the coefficient of the L1 regularization.\n","    - drop_out_p: Float giving the dropout probability.\n","    - M_train: Number of training samples per batch used in the Laplace or Gaussian kernel.\n","\n","    Outputs:\n","    - G_zy: PyTorch Net giving the trained generator.\n","    - G_zx: PyTorch Net giving the trained generator.\n","    \"\"\"\n","\n","    input_dimension = Z.shape[1]\n","    output_dimension_y = Y.shape[1]\n","    output_dimension_x = X.shape[1]\n","\n","    if using_Gen == '1':\n","\n","        G_zy = Generator(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p).to(device)\n","        G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","        G_zx = Generator(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p).to(device)\n","        G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    elif using_Gen == '2':\n","        G_zy = Generator_2(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type,\n","                         ReLU_coef).to(device)\n","        G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","        G_zx = Generator_2(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type,\n","                         ReLU_coef).to(device)\n","        G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","\n","\n","    iter_count = 0\n","    G_zy = G_zy.train()\n","    G_zx = G_zx.train()\n","\n","    if normal_ini:\n","        for p in G_zy.parameters():\n","            p.data = torch.randn(\n","                p.shape, device=device,\n","                dtype=torch.float32) / np.sqrt(float(hidden_layer_size * 2))\n","\n","        for p in G_zx.parameters():\n","            p.data = torch.randn(\n","                p.shape, device=device,\n","                dtype=torch.float32) / np.sqrt(float(hidden_layer_size * 2))\n","\n","    for epoch in range(epochs_num):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zy = G_zy.train()\n","        G_zx = G_zx.train()\n","        for X_real, Y_real, Z_real, Z_fake in DataLoader:\n","            X_real = X_real.to(device)\n","            Y_real = Y_real.to(device)\n","            Z_real = Z_real.to(device)\n","            Z_fake = Z_fake.to(device)\n","\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real.shape[0], noise_dimension, noise_type, input_var = noise_dimension_var).to(device)\n","            Y_fake = G_zy(torch.cat((Z_real,Noise_fake),dim=1)).to(device)\n","\n","            Noise_fake = sample_noise(Z_real.shape[0], noise_dimension, noise_type, input_var = noise_dimension_var).to(device)\n","            X_fake = G_zx(torch.cat((Z_real,Noise_fake),dim=1)).to(device)\n","\n","            # Generator step\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            l1_regularization_first_layer = 0\n","            l1_regularization_rest_layers = 0\n","\n","            for name, param in G_zy.named_parameters():\n","                if \"fc1\" in name:\n","                    l1_regularization_first_layer += torch.linalg.vector_norm(param, ord = 1)\n","                else :\n","                    l1_regularization_rest_layers += torch.linalg.vector_norm(param, ord=1)\n","\n","            g_zy_error = lambda_1 * find_loss(Y_real, Y_fake, Z_real, sigma_z = sigma_z, sigma_y = sigma_y) + \\\n","                         lambda_2 * find_loss_2(Y_real, Y_fake, Z_real, sigma_z = sigma_z, sigma_y = sigma_y) + \\\n","                         lambda_3 * l1_regularization_rest_layers + \\\n","                         lambda_4 * l1_regularization_first_layer\n","            g_zy_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zy.parameters(), max_norm=0.5)\n","            G_zy_solver.step()\n","\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            l1_regularization_first_layer = 0\n","            l1_regularization_rest_layers = 0\n","\n","            for param in G_zx.parameters():\n","                if \"fc1\" in name:\n","                    l1_regularization_first_layer += torch.linalg.vector_norm(param, ord=1)\n","                else:\n","                    l1_regularization_rest_layers += torch.linalg.vector_norm(param, ord=1)\n","\n","            g_zx_error = lambda_1 * find_loss(X_real, X_fake, Z_real, sigma_z = sigma_z, sigma_y = sigma_x) + \\\n","                         lambda_2 * find_loss_2(X_real, X_fake, Z_real, sigma_z = sigma_z, sigma_y = sigma_x) + \\\n","                         lambda_3 * l1_regularization_rest_layers + \\\n","                         lambda_4 * l1_regularization_first_layer\n","\n","            g_zx_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zx.parameters(), max_norm=0.5)\n","            G_zx_solver.step()\n","\n","            iter_count += 1\n","            batch_count += 1\n","\n","    return G_zy, G_zx\n","\n","def mGAN(n=500, z_dim=1, simulation='type1error', batch_size=64, epochs_num=1000,\n","         nstd=1.0, z_dist='gaussian', x_dims=1, y_dims=1, a_x=0.05, M=500, k=2, boot_num=1000,\n","         noise_dimension = 10, hidden_layer_size = 512, normal_ini = False, preprocess = 'normalize',\n","         G_lr = 1e-5, using_orcale = False, lambda_1 = 1, lambda_2 = 1, using_Gen = '1',\n","         boor_rv_type = \"gaussian\", wgt_decay = 0, lambda_3 = 1, drop_out_p = 0.2, noise_dimension_var = 1,\n","         noise_dimension_type = \"normal\", lambda_4 = 1):\n","    \"\"\"\n","    Compute the test statistics\n","\n","    Inputs:\n","    - Ax: Torch Tensor of shape (sample_size, dimension_X) giving the matrix to generate training data of X.\n","    - Ay: Torch Tensor of shape (sample_size, dimension_Y) giving the matrix to generate training data of Y.\n","    - n: Integer giving the number of samples to generate.\n","    - z_dim: Integer giving the dimension of Z.\n","    - simulation: 'type1error' or 'power'.\n","    - batch_size: Integer giving the batch size.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - nstd: Float. standard deviation of the noise in the simulated data.\n","    - z_dist: 'gaussian' or 'uniform'.\n","    - x_dims: Integer giving the dimension of X.\n","    - y_dims: Integer giving the dimension of Y.\n","    - a_x: Float using in the alternative case. alpha_x.\n","    - M: Integer giving the number of training samples per batch used in the Laplace or Gaussian kernel.\n","    - k: Integer giving the number of cross-validation folds.\n","    - boot_num: Integer of the number of wild bootstrap when computing p-value.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - normal_ini: Boolean specifying whether to initialize the generator with normal initialization.\n","    - G_lr: Float giving the learning rate of the generator.\n","    - using_orcale: 'True' or 'False' specifying whether to use the orcale method.\n","    - lambda_1: Float giving the coefficient of the MMD loss using Laplace kernel.\n","    - lambda_2: Float giving the coefficient of the MMD loss using Gaussian kernel.\n","    - using_Gen: '1' or '2' specifying whether to use the first or second generator.\n","    - boor_rv_type: 'rademacher', 'gaussian'. type of the bootstrap random variable.\n","    - wgt_decay: Float giving the weight decay. (L2 regularization)\n","    - lambda_3: Scalar giving the coefficient of the L1 regularization.\n","    - drop_out_p: Float giving the dropout probability\n","    - exp_num: not using\n","    - M_train: Number of training samples per batch used in the Laplace or Gaussian kernel.\n","\n","    Outputs:\n","    - p-value: Float of computed p-value.\n","\n","    \"\"\"\n","    if simulation == 'type1error':\n","        # generate samples x, y, z under null hypothesis - x and y are conditional independent\n","        sim_x, sim_y, sim_z = generate_samples_random(size=n, sType='H0', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd, alpha_x=a_x,\n","                                  dist_z=z_dist, preprocess = preprocess)\n","\n","    elif simulation == 'power':\n","        # generate samples x, y, z under alternative hypothesis - x and y are dependent\n","        sim_x, sim_y, sim_z = generate_samples_random(size=n, sType='H1', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd,\n","                                  alpha_x=a_x, dist_z=z_dist, preprocess = preprocess)\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    x, y, z = sim_x, sim_y, sim_z\n","    w_mx = torch.linalg.vector_norm(z.repeat(n,1,1) - torch.swapaxes(z.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_w = torch.median(w_mx).item()\n","\n","    u_mx = torch.exp(-torch.abs(y.repeat(1, n) - y.repeat(1, n).T) )\n","    sigma_u = torch.median(u_mx).item()\n","\n","    v_mx = torch.exp(-torch.abs(x.repeat(1, n) - x.repeat(1, n).T) )\n","    sigma_v = torch.median(v_mx).item()\n","\n","    test_size = int(n/k)\n","    stat_all = torch.zeros(k, 1)\n","    boot_temp_all = torch.zeros(k, boot_num)\n","    cur_k = 0\n","\n","\n","    for k_fold in range(k):\n","        k_fold_start = int(n/k * k_fold)\n","        k_fold_end = int(n/k * (k_fold+1))\n","        X_test, Y_test, Z_test = x[k_fold_start:k_fold_end], y[k_fold_start:k_fold_end], z[k_fold_start:k_fold_end]\n","        X_train, Y_train, Z_train = torch.cat((x[0:k_fold_start], x[k_fold_end:])), torch.cat((y[0:k_fold_start], y[k_fold_end:])), torch.cat((z[0:k_fold_start], z[k_fold_end:]))\n","\n","        if (k == 1):\n","            X_train, Y_train, Z_train = X_test, Y_test, Z_test\n","\n","        train_xyz = DatasetSelect_GAN(X_train, Y_train, Z_train, batch_size)\n","        DataLoader_xyz = torch.utils.data.DataLoader(train_xyz, batch_size=batch_size, shuffle=True)\n","        if not using_orcale:\n","            G_zy, G_zx = train_ver3(X = X_train, Y = Y_train, Z = Z_train, M = M,\n","                    X_test = X_test, Y_test = Y_test, Z_test = Z_test,\n","                    noise_dimension = noise_dimension, noise_type = noise_dimension_type,\n","                    G_lr = G_lr, hidden_layer_size = hidden_layer_size,\n","                    DataLoader = DataLoader_xyz, BN_type = False, ReLU_coef = 0.1,\n","                    epochs_num=epochs_num, sigma_z = sigma_w, sigma_x = sigma_v, sigma_y = sigma_u,\n","                    normal_ini = normal_ini, lambda_1 = lambda_1, lambda_2 = lambda_2,\n","                    using_Gen = using_Gen, wgt_decay = wgt_decay, lambda_3 = lambda_3,\n","                    drop_out_p = drop_out_p, noise_dimension_var = noise_dimension_var,\n","                    lambda_4 = lambda_4)\n","\n","\n","        dataset_test = DatasetSelect(X_test, Y_test, Z_test)\n","        dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True)\n","\n","        gen_x_all = torch.zeros(test_size, M)\n","        gen_y_all = torch.zeros(test_size, M)\n","        z_all = torch.zeros(test_size, z_dim)\n","        x_all = torch.zeros(test_size, x_dims)\n","        y_all = torch.zeros(test_size, y_dims)\n","\n","        cur_itr = 0\n","        G_zx = G_zx.eval()\n","        G_zy = G_zy.eval()\n","        for i, (x_test, y_test, z_test) in enumerate(dataloader_test):\n","            z_test_temp = z_test.repeat(M,1)\n","            if not using_orcale:\n","                z_test_temp = z_test_temp.to(device)\n","                Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, \"normal\").to(device)\n","                with torch.no_grad():\n","                    fake_x = G_zx(torch.cat((z_test_temp, Noise_fake),dim=1)).reshape(1, -1)\n","\n","                Noise_fake = sample_noise(z_test_temp.size()[0], noise_dimension, \"normal\").to(device)\n","                with torch.no_grad():\n","                    fake_y = G_zy(torch.cat((z_test_temp, Noise_fake),dim=1)).reshape(1, -1)\n","            elif using_orcale:\n","                if simulation == 'type1error':\n","                    fake_x, fake_y = generate_samples_from_fixed_Z_random(z_test_temp, size=M, sType='H0', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd, alpha_x=a_x,\n","                                      dist_z=z_dist)\n","                elif simulation == 'power':\n","                    fake_x, fake_y = generate_samples_from_fixed_Z_random(z_test_temp, size=M, sType='H1', dx=x_dims, dy=y_dims, dz=z_dim, nstd=nstd, alpha_x=a_x,\n","                                      dist_z=z_dist)\n","\n","            gen_x_all[cur_itr,:] = fake_x.detach().reshape(-1)\n","            gen_y_all[cur_itr,:] = fake_y.detach().reshape(-1)\n","            x_all[cur_itr,:] = x_test\n","            y_all[cur_itr,:] = y_test\n","            z_all[cur_itr,:] = z_test\n","            cur_itr = cur_itr + 1\n","        cur_stat, cur_boot_temp = get_p_value_stat_1(boot_num, M, test_size, gen_x_all.to(device), gen_y_all.to(device),\n","                                x_all.to(device), y_all.to(device), z_all.to(device), sigma_w, sigma_u, sigma_v,\n","                                boor_rv_type)\n","        stat_all[cur_k,:] = cur_stat\n","        boot_temp_all[cur_k,:] = torch.from_numpy(cur_boot_temp)\n","        cur_k = cur_k + 1\n","\n","        if using_orcale:\n","            gen_x_mean = torch.mean(gen_x_all, dim=1).reshape(-1, 1)\n","            gen_y_mean = torch.mean(gen_y_all, dim=1).reshape(-1, 1)\n","\n","            mse_x = torch.mean((gen_x_mean - x_all) ** 2).item()\n","            mse_y = torch.mean((gen_y_mean - y_all) ** 2).item()\n","\n","            print(f'Test MSE x [{mse_x}], MSE y [{mse_y}]')\n","\n","    return np.mean(torch.mean(boot_temp_all, dim = 0).numpy() > torch.mean(stat_all).item() )\n","\n","def run_experiment(params):\n","    test = params[\"test\"]\n","    sample_size = params[\"sample_size\"]\n","    batch_size = params[\"batch_size\"]\n","    z_dim = params[\"z_dim\"]\n","    dx = params[\"dx\"]\n","    dy = params[\"dy\"]\n","    n_test = params[\"n_test\"]\n","    epochs_num = params[\"epochs_num\"]\n","    eps_std = params[\"eps_std\"]\n","    dist_z = params[\"dist_z\"]\n","    alpha_x = params[\"alpha_x\"]\n","    m_value = params[\"m_value\"]\n","    k_value = params[\"k_value\"]\n","    j_value = params[\"j_value\"]\n","    noise_dimension = params[\"noise_dimension\"]\n","    noise_dimension_type = params[\"noise_dimension_type\"]\n","    noise_dimension_var = params[\"noise_dimension_var\"]\n","    hidden_layer_size = params[\"hidden_layer_size\"]\n","    normal_ini = params[\"normal_ini\"]\n","    preprocess = params[\"preprocess\"]\n","    G_lr = params[\"G_lr\"]\n","    alpha = params[\"alpha\"]\n","    alpha1 = params[\"alpha1\"]\n","    set_seeds = params[\"set_seeds\"]\n","    using_orcale = params[\"using_orcale\"]\n","    lambda_1 = params[\"lambda_1\"]\n","    lambda_2 = params[\"lambda_2\"]\n","    using_Gen = params[\"using_Gen\"]\n","    boor_rv_type = params[\"boor_rv_type\"]\n","    wgt_decay = params[\"wgt_decay\"]\n","    lambda_3 = params[\"lambda_3\"]\n","    lambda_4 = params[\"lambda_4\"]\n","    drop_out_p = params[\"drop_out_p\"]\n","    is_sparse = params[\"is_sparse\"]\n","    sparse_ratio = params[\"sparse_ratio\"]\n","\n","    np.random.seed(set_seeds)\n","    torch.manual_seed(set_seeds)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(set_seeds)\n","\n","    p_values = np.array([])\n","    test_count = 0\n","    if test == 'type1error':\n","        for n in tqdm(range(n_test)):\n","            start_time = datetime.now()\n","\n","            p_value = mGAN(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                    epochs_num=epochs_num,\n","                    nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                    k=k_value, boot_num=j_value,\n","                    noise_dimension = noise_dimension, hidden_layer_size = hidden_layer_size, normal_ini = normal_ini,\n","                    preprocess = preprocess, G_lr = G_lr, using_orcale = using_orcale,\n","                    lambda_1 = lambda_1, lambda_2 = lambda_2, using_Gen = using_Gen, boor_rv_type = boor_rv_type,\n","                    wgt_decay = wgt_decay, lambda_3 = lambda_3, drop_out_p = drop_out_p,\n","                    noise_dimension_var = noise_dimension_var, noise_dimension_type = noise_dimension_type,\n","                    lambda_4 = lambda_4)\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The stat is {}'.format(p_value))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","    if test == 'power':\n","        for n in tqdm(range(n_test)):\n","            start_time = datetime.now()\n","\n","            p_value = mGAN(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                    epochs_num=epochs_num,\n","                    nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                    k=k_value, boot_num=j_value,\n","                    noise_dimension = noise_dimension, hidden_layer_size = hidden_layer_size, normal_ini = normal_ini,\n","                    preprocess = preprocess, G_lr = G_lr, using_orcale = using_orcale,\n","                    lambda_1 = lambda_1, lambda_2 = lambda_2, using_Gen = using_Gen, boor_rv_type = boor_rv_type,\n","                    wgt_decay = wgt_decay, lambda_3 = lambda_3, drop_out_p = drop_out_p,\n","                    noise_dimension_var = noise_dimension_var, noise_dimension_type = noise_dimension_type,\n","                    lambda_4 = lambda_4)\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The stat is {}'.format(p_value))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","    return p_values\n","\n","\n","# p_val_list = run_experiment(param)\n","# p_val_list"]},{"cell_type":"code","source":["# @title code to get Emperical Rejection Rate for Figure 1 (a)\n","\n","param[\"test\"] = \"type1error\"\n","\n","for k_value in [2, 4, 8]:\n","    for sample_size in [200, 400, 600, 800, 1000]:\n","        param[\"k_value\"] = k_value\n","        param[\"sample_size\"] = sample_size\n","        p_val_list = run_experiment(param)\n","\n"],"metadata":{"id":"WY8BgUFECayz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title code to get Size Adjusted Power for Figure 1 (b)\n","\n","# For fold number J=2\n","\n","param[\"test\"] = \"power\"\n","param[\"sample_size\"] = 400\n","param[\"k_value\"] = 2\n","\n","param[\"alpha\"] = 0.042 # 5% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\", param[\"k_value\"] = 2, param[\"sample_size\"] = 400\n","param[\"alpha1\"] = 0.095 # 10% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\", param[\"k_value\"] = 2, param[\"sample_size\"] = 400\n","\n","for alpha_x in [0.05, 0.10, 0.15, 0.20, 0.25]:\n","    param[\"alpha_x\"] = alpha_x\n","    p_val_list = run_experiment(param)\n","\n","# For fold number J=4\n","\n","param[\"test\"] = \"power\"\n","param[\"sample_size\"] = 400\n","param[\"k_value\"] = 4\n","\n","param[\"alpha\"] = 0.039 # 5% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\", param[\"k_value\"] = 4, param[\"sample_size\"] = 400\n","param[\"alpha1\"] = 0.087 # 10% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\", param[\"k_value\"] = 4, param[\"sample_size\"] = 400\n","\n","for alpha_x in [0.05, 0.10, 0.15, 0.20, 0.25]:\n","    param[\"alpha_x\"] = alpha_x\n","    p_val_list = run_experiment(param)\n","\n","# For fold number J=8\n","\n","param[\"test\"] = \"power\"\n","param[\"sample_size\"] = 400\n","param[\"k_value\"] = 8\n","\n","param[\"alpha\"] = 0.0578 # 5% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\", param[\"k_value\"] = 8, param[\"sample_size\"] = 400\n","param[\"alpha1\"] = 0.122 # 10% quantile of the p_val_list from previous block when param[\"test\"] = \"type1error\", param[\"k_value\"] = 8, param[\"sample_size\"] = 400\n","\n","for alpha_x in [0.05, 0.10, 0.15, 0.20, 0.25]:\n","    param[\"alpha_x\"] = alpha_x\n","    p_val_list = run_experiment(param)\n"],"metadata":{"id":"utBGRMt9E9DC"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}