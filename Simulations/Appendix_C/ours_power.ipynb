{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"96NggURkyHFt"},"outputs":[],"source":["param = {\n","  \"test\": \"power\", # ['type1error', 'power']\n","  \"sample_size\": 600, # for power, sample_size = 600\n","  \"batch_size\": 128, # [32, 64, 128, 256]\n","  \"z_dim\": 2, # [5, 50, 250]\n","  \"dx\": 2,\n","  \"dy\": 2,\n","  \"n_test\": 1000, # [200, 2000]\n","  \"epochs_num\": 400, # [1000, 1500]\n","  \"eps_std\" : 0.5,\n","  \"dist_z\" : 'gaussian', # ['laplace', 'gaussian']\n","  \"alpha_x\": 0.05, # only used under alternative [0.05, 0.10, 0.15, 0.20, 0.25]\n","  \"m_value\": 100, # [100, 200]\n","  \"k_value\": 2, # [1, 2, 4]\n","  \"j_value\": 1000, # [1000, 2000]\n","  \"noise_dimension\": 50, # [5, 10, 20]\n","  \"hidden_layer_size\": 50, # [64, 128, 256, 512, 1024]\n","  \"normal_ini\": False, # [True, False]\n","  \"preprocess\": 'normalize', # ['normalize',  'scale_Z', 'None' ]\n","  \"G_lr\": 5e-4, # [5e-6, 1e-5, 2e-5ï¼Œ 5e-5]\n","  \"alpha\": 0.1,\n","  \"alpha1\": 0.05,\n","  \"set_seeds\": 0,\n","  \"using_orcale\": False,\n","  \"lambda_1\": 1, # loss with Laplace kernel\n","  \"lambda_2\": 0,  # loss with Gaussian kernel\n","  \"using_Gen\": '1',  # ['1', '2'], types of generator \"1\" is fully connect, \"2\" is non fully\n","  \"boor_rv_type\":  'gaussian', # ['rademacher', 'gaussian']\n","  \"wgt_decay\": 1e-5, # weight decay for adam optimizer L2 regularization parameter\n","  \"lambda_3\": 1e-5, # L1 regularization parameter\n","  \"drop_out_p\": 0.1, #  probability of an element to be zeroed. Default: 0.5, best 0.2\n","  \"M_train\": 10\n","}\n","\n","\n","import torch\n","import torch.distributions as TD\n","from torch.utils.data import Dataset, DataLoader\n","from zmq import device\n","import torch.optim as optim\n","import numpy as np\n","from datetime import datetime\n","import functools\n","from tqdm import tqdm\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","def get_xzy_randn_nl(n_points, ground_truth='H0', dim=2, device='cuda:0', p = 0.0, **ignored):\n","    '''\n","    Generate CI,I  samples\n","    1. Z is independent Gaussian or Laplace (in the following function is y).\n","    2. X = Z + noise (in the following function is x)\n","    and Y = Z + noise (in the following function is z) in case of CI\n","    Arguments:\n","        n_points : number of samples\n","        ground_truth: 'H0'or 'H1'\n","        dim: dimension of X, Y, Z\n","        device: device to run on\n","        p: probability of H1. only used in H1\n","        ignored: ignored\n","    Output:\n","        Samples X, Y, Z\n","    '''\n","    y = torch.randn(n_points, dim, device=device) / np.sqrt(dim)\n","    m = TD.Bernoulli(torch.tensor([p]))\n","    delta = m.sample((n_points,)).to(device)\n","\n","    noise_z = 0.1 * torch.randn(n_points, dim, device=device) / np.sqrt(dim)\n","    z = y + noise_z\n","    x = y.clone()\n","\n","    if ground_truth == 'H1':\n","        x = x + (1 - delta) * (0.1* torch.randn_like(x) / np.sqrt(dim)) + delta * noise_z\n","    elif ground_truth == 'H0':\n","        x = x + (0.1* torch.randn_like(x) / np.sqrt(dim))\n","    else:\n","        raise NotImplementedError(f'{ground_truth} has to be H0 or H1')\n","\n","\n","\n","    return x, z, y\n","\n","def get_p_value_stat_1(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, sigma_w, sigma_u=1,\n","                       sigma_v=1,\n","                       boor_rv_type=\"gaussian\"):\n","  \"\"\"\n","    Compute the p-value\n","\n","    Input:\n","    - boot_num: Integer giving the number of bootstrap samples.\n","    - M: Integer giving the number of training samples per batch.\n","    - n: Integer giving the number of training samples.\n","    - gen_x_all_torch: PyTorch Tensor (batch_size, M) of generated data of X.\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data of Y.\n","    - x_torch: PyTorch Tensor (batch_size) of training input X.\n","    - y_torch: PyTorch Tensor (batch_size) of training input Y.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z\n","    - sigma_w: Float of the bandwith of the Laplace kernel.\n","    - sigma_u: Float of the bandwith of the Laplace kernel.\n","    - sigma_v: Float of the bandwith of the Laplace kernel.\n","    - boor_rv_type: \"rademacher\" or \"gaussian\", specifying the reference distribution.\n","\n","    Output:\n","    - p_value: Float giving the p-value.\n","  \"\"\"\n","\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)**2\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.linalg.vector_norm(y_torch.repeat(n, 1, 1) - torch.swapaxes(y_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)**2 / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, 2) - y_torch.repeat(1, n).reshape(n, n, 1, 2), ord=1, dim=3)**2 / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, 2)\n","\n","    temp_mx = torch.swapaxes(gen_y_all_torch_rep[:, :, 0, :], 0, 1)\n","    sum_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1, 2), ord=1, dim=3)**2 / sigma_u), dim=2)\n","\n","    v_mx_1 = torch.exp(-torch.linalg.vector_norm(x_torch.repeat(n, 1, 1) - torch.swapaxes(x_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)**2 / sigma_v)\n","    v_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_x_all_torch.repeat(n, 1, 1).reshape(n, n, -1, 2) - x_torch.repeat(1, n).reshape(n, n, 1, 2), ord=1, dim=3)**2 / sigma_v), dim=2)\n","    v_mx_3 = v_mx_2.T\n","\n","    gen_x_all_torch_rep = gen_x_all_torch.repeat(n, 1, 1).reshape(n, n, -1, 2)\n","\n","    temp2_mx = torch.swapaxes(gen_x_all_torch_rep[:, :, 0, :], 0, 1)\n","    sum2_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_x_all_torch_rep - temp2_mx.reshape(n, n, 1, 2), ord=1, dim=3)**2 / sigma_v), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = torch.swapaxes(gen_y_all_torch_rep[:, :, i, :], 0, 1)\n","        temp_add_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1, 2), ord=1, dim=3)**2 / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","        temp2_mx = torch.swapaxes(gen_x_all_torch_rep[:, :, i, :], 0, 1)\n","        temp2_add_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_x_all_torch_rep - temp2_mx.reshape(n, n, 1, 2), ord=1, dim=3)**2 / sigma_v), dim=2)\n","        sum2_mx = sum2_mx + temp2_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_4 = 1 / M * sum2_mx\n","    v_mx = v_mx_1 - v_mx_2 - v_mx_3 + v_mx_4\n","\n","    FF_mx = u_mx * v_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    stat = 1 / (n - 1) * torch.sum(FF_mx).item()\n","\n","    # print(\"U_mx:\", u_mx)\n","    # print(\"V_mx:\", v_mx)\n","    # print(\"W_mx:\", w_mx)\n","    boottemp = np.array([])\n","    if boor_rv_type == \"rademacher\":\n","        eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","        eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","        random_mx = torch.matmul(eboot[:, bb].reshape(-1, 1), eboot[:, bb].reshape(-1, 1).T)\n","        bootmatrix = FF_mx * random_mx\n","        stat_boot = 1 / (n - 1) * torch.sum(bootmatrix).item()\n","        boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","class DatasetSelect(Dataset):\n","  \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - Z: PyTorch Tensor of shape (N, output_dimension) giving the training data of Z.\n","  \"\"\"\n","    def __init__(self, X, Y, Z):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index]\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","  \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - batch_size: Integer giving the batch size.\n","  \"\"\"\n","\n","  def __init__(self, X, Y, Z, batch_size):\n","    self.X_real = X\n","    self.Y_real = Y\n","    self.Z_real = Z\n","    self.batch_size = batch_size\n","    self.sample_size = X.shape[0]\n","\n","  def __len__(self):\n","    return self.sample_size\n","\n","  def __getitem__(self, index):\n","    return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[(self.batch_size+index) % self.sample_size]\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN_ver2(torch.utils.data.Dataset):\n","  \"\"\"\n","    Create a DatasetSelect object to generate the DataLoader in the learning process.\n","\n","    Input:\n","    - X: PyTorch Tensor of shape (N, input_dimension) giving the training data of X.\n","    - Y: PyTorch Tensor of shape (N, output_dimension) giving the training data of Y.\n","    - batch_size: Integer giving the batch size.\n","  \"\"\"\n","\n","  def __init__(self, Y, Z, batch_size):\n","    self.Y_real = Y\n","    self.Z_real = Z\n","    self.batch_size = batch_size\n","    self.sample_size = Z.shape[0]\n","\n","  def __len__(self):\n","    return self.sample_size\n","\n","  def __getitem__(self, index):\n","    return self.Y_real[index], self.Z_real[index]\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","    \"\"\"\n","    Generate a PyTorch Tensor of random noise from the specified reference distribution.\n","\n","    Input:\n","    - sample_size: the sample size of noise to generate.\n","    - noise_dimension: the dimension of noise to generate.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","\n","    Output:\n","    - A PyTorch Tensor of shape (sample_size, noise_dimension).\n","    \"\"\"\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","##### GAN architecture #####\n","\n","class Generator(torch.nn.Module):\n","    \"\"\"\n","    Specify the neural network architecture of the Generator.\n","\n","    Here, we consider a FNN with a fully connected hidden layer with a width of 50,\n","    which is followed by a Leaky ReLU activation. The coefficient of Leaky ReLU needs to be\n","    specified. Batch normalization may be added prior to the activation function.\n","    The output layer a fully connected layer without activation.\n","\n","    Inputs:\n","    - input_dimension: Integer giving the dimension of input Z.\n","    - output_dimension: Integer giving the dimension of output X or Y.\n","    - noise_dimension: Integer giving the dimension of random noise.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Scalar giving the coefficient of the Leaky ReLU layer.\n","    - drop_out_p: Float giving the dropout probability.\n","    - drop_input: Boolean specifying whether to add dropout to the input layer.\n","\n","    Returns:\n","    - x: PyTorch Tensor containing the (output_dimension,) output of the generator.\n","    \"\"\"\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                 drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.fc_temp = torch.nn.Linear(input_dimension + noise_dimension, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","\n","      else:\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","        # x = self.sigmoid(x)\n","      return x\n","\n","##### Training procedures #####\n","\n","\n","def find_loss(y_torch, gen_y_all_torch, z_torch, sigma_w, sigma_u, M):\n","    \"\"\"\n","    Compute the MMD loss via Laplace kernel.\n","\n","    Inputs:\n","    - y_torch: PyTorch Tensor (batch_size) of training input. (X or Y)\n","    - gen_y_all_torch: PyTorch Tensor (batch_size, M) of generated data.\n","    - z_torch: PyTorch Tensor (batch_size, dimension_Z) of training input Z.\n","    - sigma_w: Float of the bandwith of the kernel.\n","    - sigma_u: Float of the bandwith of the kernel.\n","    - M: Number of training samples per batch.\n","\n","    Outputs:\n","    - loss: PyTorch Tensor containing the MMD loss.\n","    \"\"\"\n","    n = z_torch.shape[0]\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=2, dim=2)**2\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.linalg.vector_norm(y_torch.repeat(n, 1, 1) - torch.swapaxes(y_torch.repeat(n, 1, 1), 0, 1), ord=2, dim=2)**2 / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, 2) - y_torch.repeat(1, n).reshape(n, n, 1, 2), ord=2, dim=3)**2 / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, 2)\n","\n","    temp_mx = torch.swapaxes(gen_y_all_torch_rep[:, :, 0, :], 0, 1)\n","\n","    sum_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1, 2), ord=2, dim=3)**2 / sigma_u), dim=2)\n","\n","    for i in range(1, M):\n","        temp_mx = torch.swapaxes(gen_y_all_torch_rep[:, :, i, :], 0, 1)\n","        temp_add_mx = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - temp_mx.reshape(n, n, 1, 2), ord=2, dim=3)**2 / sigma_u), dim=2)\n","        sum_mx = sum_mx + temp_add_mx\n","\n","    u_mx_4 = 1 / M * sum_mx\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","\n","    FF_mx = u_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    loss = 1 / (n) * torch.sum(FF_mx)\n","    return loss\n","\n","\n","def train_ver3(X, Y, Z, X_test, Y_test, Z_test, M,\n","      noise_dimension, noise_type, G_lr, hidden_layer_size,\n","      DataLoader, BN_type, ReLU_coef,\n","      epochs_num=10,  sigma_z = 1, sigma_x = 1, sigma_y = 1,\n","      normal_ini = False,\n","      lambda_1 = 1, lambda_2 = 1, using_Gen = '1', wgt_decay = 0,\n","      lambda_3 = 0, drop_out_p = 0.2, M_train = 3):\n","    \"\"\"\n","    Train loop for GAN.\n","\n","    Inputs:\n","    - X: PyTorch Tensor (sample_size, dimension_X) of training input.\n","    - Y: PyTorch Tensor (sample_size, dimension_Y) of training input.\n","    - Z: PyTorch Tensor (sample_size, dimension_Z) of training input.\n","    - X_test: PyTorch Tensor (sample_size, dimension_X) of test input.\n","    - Y_test: PyTorch Tensor (sample_size, dimension_Y) of test input.\n","    - Z_test: PyTorch Tensor (sample_size, dimension_Z) of test input.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - noise_type: \"normal\", \"unif\" or \"Cauchy\", giving the reference distribution.\n","    - G_lr: Float giving the learning rate of the generator.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - DataLoader: DataLoader object used to generate training batches.\n","    - BN_type: 'True' or 'False' specifying whether batch normalization is included.\n","    - ReLU_coef: Float giving the coefficient of the Leaky ReLU layer.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - sigma_z: Float of the bandwith of the kernel.\n","    - sigma_x: Float of the bandwith of the kernel.\n","    - sigma_y: Float of the bandwith of the kernel.\n","    - normal_ini: Boolean specifying whether to initialize the generator with normal initialization.\n","    - lambda_1: Float giving the coefficient of the MMD loss using Laplace kernel.\n","    - lambda_2: Float giving the coefficient of the MMD loss using Gaussian kernel. (not using)\n","    - using_Gen: '1' or '2' specifying whether to use the first or second generator.(not using)\n","    - wgt_decay: Float giving the weight decay. (L2 regularization)\n","    - lambda_3: Scalar giving the coefficient of the L1 regularization.\n","    - drop_out_p: Float giving the dropout probability.\n","    - M_train: Number of training samples per batch used in the Laplace or Gaussian kernel.\n","\n","    Outputs:\n","    - G_zy: PyTorch Net giving the trained generator.\n","    - G_zx: PyTorch Net giving the trained generator.\n","    \"\"\"\n","    input_dimension = Z.shape[1]\n","    output_dimension_y = Y.shape[1]\n","    output_dimension_x = X.shape[1]\n","    M_eval = 50\n","\n","    G_zy = Generator(input_dimension, output_dimension_y, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p).to(device)\n","    G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    G_zx = Generator(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p).to(device)\n","    G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","\n","    iter_count = 0\n","    G_zy = G_zy.train()\n","    G_zx = G_zx.train()\n","\n","    for epoch in range(epochs_num):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zy = G_zy.train()\n","        G_zx = G_zx.train()\n","        for X_real, Y_real, Z_real, Z_fake in DataLoader:\n","            X_real = X_real.to(device)\n","            Y_real = Y_real.to(device)\n","            Z_real = Z_real.to(device)\n","            Z_fake = Z_fake.to(device)\n","\n","            batch_size = Z_real.shape[0]\n","            Z_real_repeat = Z_real.repeat(M_train,1)\n","\n","\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = 1.0/3.0).to(device)\n","            Y_fake = G_zy(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = 1.0/3.0).to(device)\n","            X_fake = G_zx(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","\n","            # Y_fake = Y_fake.reshape(batch_size, M_train, output_dimension_y)\n","            Y_fake = Y_fake.reshape(M_train, batch_size, output_dimension_y)\n","            # X_fake = X_fake.reshape(batch_size, M_train, output_dimension_x)\n","            X_fake = X_fake.reshape(M_train, batch_size, output_dimension_x)\n","\n","            # Generator step\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            # g_zy_error = find_loss(Y_real, Y_fake, Z_real, sigma_z, sigma_y, M_train)\n","            Y_fake = torch.swapaxes(Y_fake, 0, 1)\n","            # g_zy_error = torch.mean(torch.linalg.vector_norm(torch.mean(Y_fake, dim = 1) - Y_real, ord = 2, dim = 1) ** 2)\n","            g_zy_error = find_loss(Y_real, Y_fake, Z_real, sigma_z, sigma_y, M_train)\n","\n","            g_zy_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zy.parameters(), max_norm=0.5)\n","            G_zy_solver.step()\n","\n","            g_zy_error = None\n","            G_zy_solver.zero_grad()\n","\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            X_fake = torch.swapaxes(X_fake, 0, 1)\n","            g_zx_error = find_loss(X_real, X_fake, Z_real, sigma_z, sigma_x, M_train)\n","\n","\n","            g_zx_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zx.parameters(), max_norm=0.5)\n","            G_zx_solver.step()\n","\n","            iter_count += 1\n","            batch_count += 1\n","\n","    return G_zy, G_zx\n","\n","def mGAN(n=500, z_dim=2, simulation='type1error', batch_size=64, epochs_num=1000,\n","    nstd=1.0, z_dist='gaussian', x_dims=2, y_dims=2, a_x=0.05, M=500, k=2, boot_num=1000,\n","    noise_dimension = 1, hidden_layer_size = 512, normal_ini = False, preprocess = 'normalize',\n","    G_lr = 1e-5, using_orcale = False, lambda_1 = 1, lambda_2 = 1, using_Gen = '1',\n","    boor_rv_type = \"gaussian\", wgt_decay = 0, lambda_3 = 1, drop_out_p = 0.2, exp_num = 0, M_train = 3):\n","    \"\"\"\n","    Compute the test statistics\n","\n","    Inputs:\n","    - Ax: Torch Tensor of shape (sample_size, dimension_X) giving the matrix to generate training data of X.\n","    - Ay: Torch Tensor of shape (sample_size, dimension_Y) giving the matrix to generate training data of Y.\n","    - n: Integer giving the number of samples to generate.\n","    - z_dim: Integer giving the dimension of Z.\n","    - simulation: 'type1error' or 'power'.\n","    - batch_size: Integer giving the batch size.\n","    - epochs_num: Number of epochs over the training dataset to use for training.\n","    - nstd: Float. standard deviation of the noise in the simulated data.\n","    - z_dist: 'gaussian' or 'uniform'.\n","    - x_dims: Integer giving the dimension of X.\n","    - y_dims: Integer giving the dimension of Y.\n","    - a_x: Float using in the alternative case. alpha_x.\n","    - M: Integer giving the number of training samples per batch used in the Laplace or Gaussian kernel.\n","    - k: Integer giving the number of cross-validation folds.\n","    - boot_num: Integer of the number of wild bootstrap when computing p-value.\n","    - noise_dimension: Integer giving the dimension of random noise Z.\n","    - hidden_layer_size: Integer giving the size of the hidden layer of the generator.\n","    - normal_ini: Boolean specifying whether to initialize the generator with normal initialization.\n","    - G_lr: Float giving the learning rate of the generator.\n","    - using_orcale: 'True' or 'False' specifying whether to use the orcale method.\n","    - lambda_1: Float giving the coefficient of the MMD loss using Laplace kernel.\n","    - lambda_2: Float giving the coefficient of the MMD loss using Gaussian kernel.\n","    - using_Gen: '1' or '2' specifying whether to use the first or second generator.\n","    - boor_rv_type: 'rademacher', 'gaussian'. type of the bootstrap random variable.\n","    - wgt_decay: Float giving the weight decay. (L2 regularization)\n","    - lambda_3: Scalar giving the coefficient of the L1 regularization.\n","    - drop_out_p: Float giving the dropout probability\n","    - exp_num: not using\n","    - M_train: Number of training samples per batch used in the Laplace or Gaussian kernel.\n","\n","    Outputs:\n","    - p-value: Float of computed p-value.\n","\n","    \"\"\"\n","    if simulation == 'type1error':\n","        sim_x, sim_y, sim_z = get_xzy_randn_nl(n_points = n, ground_truth = 'H0')\n","\n","    elif simulation == 'power':\n","        sim_x, sim_y, sim_z = get_xzy_randn_nl(n_points = n, ground_truth = 'H1', p = a_x)\n","\n","    else:\n","        raise ValueError('Test does not exist.')\n","\n","    x, y, z = sim_x.to(device), sim_y.to(device), sim_z.to(device)\n","\n","    sigma_w_train, sigma_u_train, sigma_v_train =  1.0, 1.0, 1.0\n","\n","    test_size = int(n/k)\n","    stat_all = torch.zeros(k, 1)\n","    boot_temp_all = torch.zeros(k, boot_num)\n","    cur_k = 0\n","\n","    for k_fold in range(k):\n","        k_fold_start = int(n/k * k_fold)\n","        k_fold_end = int(n/k * (k_fold+1))\n","        X_test, Y_test, Z_test = x[k_fold_start:k_fold_end], y[k_fold_start:k_fold_end], z[k_fold_start:k_fold_end]\n","        X_train, Y_train, Z_train = torch.cat((x[0:k_fold_start], x[k_fold_end:])), torch.cat((y[0:k_fold_start], y[k_fold_end:])), torch.cat((z[0:k_fold_start], z[k_fold_end:]))\n","\n","        if (k == 1):\n","            X_train, Y_train, Z_train = X_test, Y_test, Z_test\n","\n","        train_xyz = DatasetSelect_GAN(X_train, Y_train, Z_train, batch_size)\n","        DataLoader_xyz = torch.utils.data.DataLoader(train_xyz, batch_size=batch_size, shuffle=True)\n","\n","        G_zy, G_zx = train_ver3(X = X_train, Y = Y_train, Z = Z_train, M = M,\n","                X_test = X_test, Y_test = Y_test, Z_test = Z_test,\n","                noise_dimension = noise_dimension, noise_type = \"normal\",\n","                G_lr = G_lr, hidden_layer_size = hidden_layer_size,\n","                DataLoader = DataLoader_xyz, BN_type = True, ReLU_coef = 0.2,\n","                epochs_num=epochs_num, sigma_z = sigma_w_train, sigma_x = sigma_v_train, sigma_y = sigma_u_train,\n","                normal_ini = normal_ini, lambda_1 = lambda_1, lambda_2 = lambda_2,\n","                using_Gen = using_Gen, wgt_decay = wgt_decay, lambda_3 = lambda_3,\n","                drop_out_p = drop_out_p, M_train = M_train)\n","\n","        gen_x_all = torch.zeros(test_size, M)\n","        gen_y_all = torch.zeros(test_size, M)\n","        z_all = torch.zeros(test_size, z_dim)\n","        x_all = torch.zeros(test_size, x_dims)\n","        y_all = torch.zeros(test_size, y_dims)\n","\n","        G_zx = G_zx.eval()\n","        G_zy = G_zy.eval()\n","\n","        Z_test_repeat = Z_test.repeat(M,1).to(device)\n","\n","\n","        # Generate fake data\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension, \"normal\", input_var = 1.0/3.0).to(device)\n","        with torch.no_grad():\n","            gen_y_all = G_zy(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device)\n","\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension, \"normal\", input_var = 1.0/3.0).to(device)\n","        with torch.no_grad():\n","            gen_x_all = G_zx(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device)\n","\n","        gen_x_all = gen_x_all.reshape(M, test_size, x_dims).detach().to(device)\n","        gen_y_all = gen_y_all.reshape(M, test_size, y_dims).detach().to(device)\n","\n","        gen_x_all = torch.swapaxes(gen_x_all, 0, 1)\n","        gen_y_all = torch.swapaxes(gen_y_all, 0, 1)\n","\n","        z_all = Z_test.to(device)\n","        x_all = X_test.to(device)\n","        y_all = Y_test.to(device)\n","\n","        sigma_w, sigma_u, sigma_v = 1.0, 1.0, 1.0\n","\n","        standardise = True\n","\n","        if standardise:\n","            gen_x_all = (gen_x_all - torch.mean(gen_x_all, dim=0, keepdim=True)) / torch.std(gen_x_all, dim=0, keepdim=True)\n","            gen_y_all = (gen_y_all - torch.mean(gen_y_all, dim=0, keepdim=True)) / torch.std(gen_y_all, dim=0, keepdim=True)\n","            x_all = (x_all - torch.mean(x_all, dim=0, keepdim=True)) / torch.std(x_all, dim=0, keepdim=True)\n","            y_all = (y_all - torch.mean(y_all, dim=0, keepdim=True)) / torch.std(y_all, dim=0, keepdim=True)\n","            z_all = (z_all - torch.mean(z_all, dim=0, keepdim=True)) / torch.std(z_all, dim=0, keepdim=True)\n","\n","        cur_stat, cur_boot_temp = get_p_value_stat_1(boot_num, M, test_size, gen_x_all.to(device), gen_y_all.to(device),\n","                                x_all.to(device), y_all.to(device), z_all.to(device), sigma_w, sigma_u, sigma_v,\n","                                boor_rv_type)\n","        stat_all[cur_k,:] = cur_stat\n","        boot_temp_all[cur_k,:] = torch.from_numpy(cur_boot_temp)\n","        cur_k = cur_k + 1\n","\n","    return np.mean(torch.mean(boot_temp_all, dim = 0).numpy() > torch.mean(stat_all).item() )\n","\n","def run_experiment(params):\n","    test = params[\"test\"]\n","    sample_size = params[\"sample_size\"]\n","    batch_size = params[\"batch_size\"]\n","    z_dim = params[\"z_dim\"]\n","    dx = params[\"dx\"]\n","    dy = params[\"dy\"]\n","    n_test = params[\"n_test\"]\n","    epochs_num = params[\"epochs_num\"]\n","    eps_std = params[\"eps_std\"]\n","    dist_z = params[\"dist_z\"]\n","    alpha_x = params[\"alpha_x\"]\n","    m_value = params[\"m_value\"]\n","    k_value = params[\"k_value\"]\n","    j_value = params[\"j_value\"]\n","    noise_dimension = params[\"noise_dimension\"]\n","    hidden_layer_size = params[\"hidden_layer_size\"]\n","    normal_ini = params[\"normal_ini\"]\n","    preprocess = params[\"preprocess\"]\n","    G_lr = params[\"G_lr\"]\n","    alpha = params[\"alpha\"]\n","    alpha1 = params[\"alpha1\"]\n","    set_seeds = params[\"set_seeds\"]\n","    using_orcale = params[\"using_orcale\"]\n","    lambda_1 = params[\"lambda_1\"]\n","    lambda_2 = params[\"lambda_2\"]\n","    using_Gen = params[\"using_Gen\"]\n","    boor_rv_type = params[\"boor_rv_type\"]\n","    wgt_decay = params[\"wgt_decay\"]\n","    lambda_3 = params[\"lambda_3\"]\n","    drop_out_p = params[\"drop_out_p\"]\n","    M_train = params[\"M_train\"]\n","\n","    np.random.seed(set_seeds)\n","    torch.manual_seed(set_seeds)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(set_seeds)\n","\n","    p_values = np.array([])\n","    test_count = 0\n","    if test == 'type1error':\n","        for n in tqdm(range(n_test)):\n","            start_time = datetime.now()\n","\n","            p_value = mGAN(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                    epochs_num=epochs_num,\n","                    nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                    k=k_value, boot_num=j_value,\n","                    noise_dimension = noise_dimension, hidden_layer_size = hidden_layer_size, normal_ini = normal_ini,\n","                    preprocess = preprocess, G_lr = G_lr, using_orcale = using_orcale,\n","                    lambda_1 = lambda_1, lambda_2 = lambda_2, using_Gen = using_Gen, boor_rv_type = boor_rv_type,\n","                    wgt_decay = wgt_decay, lambda_3 = lambda_3, drop_out_p = drop_out_p, exp_num = n + 1,\n","                    M_train = M_train)\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The stat is {}'.format(p_value))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","\n","    if test == 'power':\n","        for n in tqdm(range(n_test)):\n","            start_time = datetime.now()\n","\n","            p_value = mGAN(n=sample_size, z_dim=z_dim, simulation=test, batch_size=batch_size,\n","                    epochs_num=epochs_num,\n","                    nstd=eps_std, z_dist=dist_z, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                    k=k_value, boot_num=j_value,\n","                    noise_dimension = noise_dimension, hidden_layer_size = hidden_layer_size, normal_ini = normal_ini,\n","                    preprocess = preprocess, G_lr = G_lr, using_orcale = using_orcale,\n","                    lambda_1 = lambda_1, lambda_2 = lambda_2, using_Gen = using_Gen, boor_rv_type = boor_rv_type,\n","                    wgt_decay = wgt_decay, lambda_3 = lambda_3, drop_out_p = drop_out_p, exp_num = n + 1,\n","                    M_train = M_train)\n","\n","            test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The stat is {}'.format(p_value))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","    print('Emp Rej Rate: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","    return p_values\n","\n","\n","# p_val_list = run_experiment(param)\n","# p_val_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rOE2AGxMbpeW"},"outputs":[],"source":["# @title code to get Size Adjusted Power for Figure 6 (b) hat T_2\n","\n","\n","param[\"alpha\"] = 0.039  # 5% quantile of the p_val_list from ours_size.ipynb when param[\"test\"] = \"type1error\" param[\"sample_size\"] = 600\n","param[\"alpha1\"] = 0.083 # 10% quantile of the p_val_list from ours_size.ipynb when param[\"test\"] = \"type1error\" param[\"sample_size\"] = 600\n","\n","\n","for alpha_x in [0.05, 0.10, 0.15, 0.20, 0.25]:\n","    param[\"alpha_x\"] = alpha_x\n","    p_val_list = run_experiment(param)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMPi0yWR6YQAnzuMviwW6+N"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}